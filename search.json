[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "Nothing to show here.\nJust do it!"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/trigonometric_functions/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/trigonometric_functions/index.html",
    "title": "Trigonometric functions",
    "section": "",
    "text": "To be learned."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html",
    "title": "ROC and AUC",
    "section": "",
    "text": "A receiver operating characteristic curve (ROC curve) is a graphical plot that illustrates the performance of a binary classifier (two-class prediction) model at varying threshold values.\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold seeting.\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.\nA classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value. Or it can be a discrete class label, indicating one of the classes.\nConsider an experiment from \\(\\mathbf{P}\\) positive instances and \\(\\mathbf{N}\\) negative instances for some condition. The four outcomes\n\nTrue positive (TP): the outcome from a prediction is positve and the actual value is positive.\nFlase positive (FP): the outcome from a prediction is positive but the actual value is negative.\nTrue negative (TN): the outcome from a prediction is negative and the actual value is negative.\nFalse negative (FN): the outcome from a prediction is negative but the actual value is positive.\n\ncan be formulated in a \\(2 \\times 2\\) contingency table or confusion matrix, as follows:\n\n\n\n\n\n\n\n\nTo draw a ROC curve, only TPR and FPR are needed (as functions of some classifier parameter).\nA ROC space is defined by FPR and TPR as \\(x\\) and \\(y\\) axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs).\nThe best possible prediction method would yield a point in the upper left corner or coordinate \\((0, 1)\\) of the ROC space, representing \\(100\\%\\) sensitivity (no false negatives) and \\(100\\%\\) specificity (no false positives). The \\((0, 1)\\) point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners.\nThe diagonal line divides the ROC space into two parts. Points above the diagonal line represent good clasification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\n\n\n\nIn binary classification, the class prediction for each instance is often made based on a continuous random variable \\(\\mathbf{X}\\), which is a “score” computed for the instance. Given a threshold \\(\\mathbf{T}\\), the instance is classified as “positive” if \\(\\mathbf{X} &gt; \\mathbf{T}\\), and “negative” otherwise. \\(\\mathbf{X}\\) follows a probability density \\(f_1(x)\\) if the instance actually belongs to class “positive”, and \\(f_0(x)\\) if otherwise.\nTherefore, the true positive rate is given by \\(\\mathbf{TPR(T)} = \\int_{T}^{\\infty} f_1(x) dx\\) and the false positive rate is given by \\(\\mathbf{FPR(T)} = \\int_{T}^{\\infty} f_0(x) dx\\).\nThe ROC curve plots parametrically \\(\\mathbf{TPR(T)}\\) versus \\(\\mathbf{FPR(T)}\\) with \\(\\mathbf{T}\\) as the varying parameter.\nIn the hypothesis testing perspective, we can consider the power as TPR (the probability of correctly rejecting \\(H_0\\)), and the type I error as FPR (the probability of wrongly rejecting \\(H_0\\)), then the ROC curve is the power as a function of the type I error:\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\n# assume that under both H0 and H1\n# the random variable X is distributed as some normal distributions\nmu0, sigma0 = 10, 2\nmu1, sigma1 = 14, 1\n\ndist0 = Normal(mu0, sigma0)\ndist1 = Normal(mu1, sigma1)\n\nalphas = 0:0.01:1\npowers = @. ccdf(dist1, quantile(dist0, 1 - alphas))\n\nfig, ax = lines(alphas, powers; color=:red, label=\"ROC curve (power vs. alpha)\")\nvlines!(ax, [0.05]; color=:black, label=\"alpha = 0.05\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Alpha\"\nax.ylabel = \"Power\"\nfig\n\n\n\n\nAs shown above, as the type I error grows up to \\(1\\), the power also increases up to \\(1\\). But we wish to reach a balance point where we have a larger power and an acceptable type I error rate (e.g. \\(0.05\\))."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html#receiver-operating-characteristic-roc",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html#receiver-operating-characteristic-roc",
    "title": "ROC and AUC",
    "section": "",
    "text": "A receiver operating characteristic curve (ROC curve) is a graphical plot that illustrates the performance of a binary classifier (two-class prediction) model at varying threshold values.\nThe ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold seeting.\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from the cost context or the class distribution. ROC analysis is related in a direct and natural way to the cost/benefit analysis of diagnostic decision making.\nA classification model (classifier or diagnosis) is a mapping of instances between certain classes/groups. Because the classifier or diagnosis result can be an arbitrary real value (continuous output), the classifier boundary between classes must be determined by a threshold value. Or it can be a discrete class label, indicating one of the classes.\nConsider an experiment from \\(\\mathbf{P}\\) positive instances and \\(\\mathbf{N}\\) negative instances for some condition. The four outcomes\n\nTrue positive (TP): the outcome from a prediction is positve and the actual value is positive.\nFlase positive (FP): the outcome from a prediction is positive but the actual value is negative.\nTrue negative (TN): the outcome from a prediction is negative and the actual value is negative.\nFalse negative (FN): the outcome from a prediction is negative but the actual value is positive.\n\ncan be formulated in a \\(2 \\times 2\\) contingency table or confusion matrix, as follows:\n\n\n\n\n\n\n\n\nTo draw a ROC curve, only TPR and FPR are needed (as functions of some classifier parameter).\nA ROC space is defined by FPR and TPR as \\(x\\) and \\(y\\) axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs).\nThe best possible prediction method would yield a point in the upper left corner or coordinate \\((0, 1)\\) of the ROC space, representing \\(100\\%\\) sensitivity (no false negatives) and \\(100\\%\\) specificity (no false positives). The \\((0, 1)\\) point is also called a perfect classification. A random guess would give a point along a diagonal line (the so-called line of no-discrimination) from the bottom left to the top right corners.\nThe diagonal line divides the ROC space into two parts. Points above the diagonal line represent good clasification results (better than random); points below the line represent bad results (worse than random). Note that the output of a consistently bad predictor could simply be inverted to obtain a good predictor.\n\n\n\nIn binary classification, the class prediction for each instance is often made based on a continuous random variable \\(\\mathbf{X}\\), which is a “score” computed for the instance. Given a threshold \\(\\mathbf{T}\\), the instance is classified as “positive” if \\(\\mathbf{X} &gt; \\mathbf{T}\\), and “negative” otherwise. \\(\\mathbf{X}\\) follows a probability density \\(f_1(x)\\) if the instance actually belongs to class “positive”, and \\(f_0(x)\\) if otherwise.\nTherefore, the true positive rate is given by \\(\\mathbf{TPR(T)} = \\int_{T}^{\\infty} f_1(x) dx\\) and the false positive rate is given by \\(\\mathbf{FPR(T)} = \\int_{T}^{\\infty} f_0(x) dx\\).\nThe ROC curve plots parametrically \\(\\mathbf{TPR(T)}\\) versus \\(\\mathbf{FPR(T)}\\) with \\(\\mathbf{T}\\) as the varying parameter.\nIn the hypothesis testing perspective, we can consider the power as TPR (the probability of correctly rejecting \\(H_0\\)), and the type I error as FPR (the probability of wrongly rejecting \\(H_0\\)), then the ROC curve is the power as a function of the type I error:\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\n# assume that under both H0 and H1\n# the random variable X is distributed as some normal distributions\nmu0, sigma0 = 10, 2\nmu1, sigma1 = 14, 1\n\ndist0 = Normal(mu0, sigma0)\ndist1 = Normal(mu1, sigma1)\n\nalphas = 0:0.01:1\npowers = @. ccdf(dist1, quantile(dist0, 1 - alphas))\n\nfig, ax = lines(alphas, powers; color=:red, label=\"ROC curve (power vs. alpha)\")\nvlines!(ax, [0.05]; color=:black, label=\"alpha = 0.05\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Alpha\"\nax.ylabel = \"Power\"\nfig\n\n\n\n\nAs shown above, as the type I error grows up to \\(1\\), the power also increases up to \\(1\\). But we wish to reach a balance point where we have a larger power and an acceptable type I error rate (e.g. \\(0.05\\))."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html#area-under-the-curve-auc",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/roc_and_auc/index.html#area-under-the-curve-auc",
    "title": "ROC and AUC",
    "section": "2 Area under the curve (AUC)",
    "text": "2 Area under the curve (AUC)\nIn addition to those evaluation metrics mentioned in the above table, another evaluation metric, called AUC (area under the ROC curve), defined as\n\\[\n\\begin{align}\nTPR(T)&: T \\to y(x) \\\\\nFPR(T)&: T \\to x \\\\\nA &= \\int_{x = 0}^{1} TPR(FPR^{-1}(x)) dx \\\\\n&= \\int_{\\infty}^{-\\infty} TPR(T) \\cdot FPR'(T) dT\n\\end{align}\n\\]\ncan be used to summarize sensitivity and specificity, but it does not inform regarding precision and negative predictive value.\nIn fact, AUC is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming “positive” ranks higher than “negative”). In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_misc/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_misc/index.html",
    "title": "Probability and statistics (misc)",
    "section": "",
    "text": "Probability is the branch of mathematics that deals with events whose individual outcomes are unpredictable, but whose outcomes on average are predictable.\nExperiments can be divided into two types:\n\nDeterministic: whose individual outcomes are predictable;\nNondeterministic (random): whose individual outcomes are unpredictable.\n\n\n\nNext, all experiments we’ll deal with are repeatable (it can be performed repeatedly any number of times) and random (any single performance of the experiment is unpredictable).\nIn this section, we’ll deal with experiments having a finite number of possible outcomes. We denote the number of possible outcomes by \\(n\\), and number them from \\(1\\) to \\(n\\).\n\n\nIn a random and repeatable experiment, if we denote \\(S_j\\) by the number of instances among the first \\(N\\) experiments where the \\(j\\)th outcome was observed to occur, then the frequency \\(\\frac{S_j}{N}\\) with which the \\(j\\)th outcome has been observed to occur tends to a limit as \\(N\\) tends to infinity. We call this limit the probability of the \\(j\\)th outcome and denote it by \\(p_j\\):\n\\[\np_j = \\lim\\limits_{N \\to \\infty} \\frac{S_j}{N}\n\\]\nThese probabilities have the following properties:\n\n\\(0 \\leq p_j \\leq 1\\);\n\n\n\n\n\n\n\nNote\n\n\n\nFor \\(\\frac{S_j}{N}\\) lies between \\(0\\) and \\(1\\), and therefore so does its limit \\(p_j\\).\n\n\n\n\\(\\sum_{j=1}^{n} p_j = 1\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWe have\n\\[\nS_1 + S_2 + \\cdots + S_n = N\n\\]\nDividing by \\(N\\), we get\n\\[\n\\frac{S_1}{N} + \\frac{S_2}{N} + \\cdots + \\frac{S_n}{N} = 1\n\\]\nAs \\(N\\) tends to infinity, we have\n\\[\n\\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} \\frac{S_j}{N} = \\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} p_j = 1\n\\]\n\n\n\n\n\nIn fact very often, we are not interested in all the details of the outcome of an experiment, but merely in a particular aspect of it (e.g. throwing a die, we may be interested only in whether the outcome is even or odd).\nAn occurrence such as throwing an even number is called an event, which is defined as the following:\n\nAn event \\(E\\) is defined as any collection of possible outcomes.\n\nNote: we say that an event \\(E\\) occurred whenever any outcome belonging to \\(E\\) occurred.\nThe probability \\(p(E)\\) of an event \\(E\\) is\n\\[\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n\\]\nwhere \\(S(E)\\) is the number of instances among the first \\(N\\) experiments when the event \\(E\\) took place.\n\n\n\n\n\n\nNote\n\n\n\nWe have\n\\[\nS(E) = \\sum_{j\\ \\text{in}\\ E} S_j\n\\]\nDividing by \\(N\\)\n\\[\n\\frac{S(E)}{N} = \\sum_{j\\ \\text{in}\\ E} \\frac{S_j}{N}\n\\]\nAs \\(N\\) tends to infinity, we have\n\\[\np(E) = \\sum_{j\\ \\text{in}\\ E} p_j\n\\]\n\n\n\n\n\nAddition rule for disjoint events\n\nTwo events \\(E_1\\) and \\(E_2\\) are called disjoint if both cannot take place simultaneously (i.e. \\(E_1 \\cap E_2 = \\emptyset\\)).\nThen we have\n\\[\np(E_1 \\cup E_2) = p(E_1) + p(E_2)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j\n\\]\nDisjointness means that an outcome \\(j\\) may belong either to \\(E_1\\) or to \\(E_2\\) but not to both; therefore,\n\\[\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j = \\sum_{j\\ \\text{in}\\ E_1} p_j + \\sum_{j\\ \\text{in}\\ E_2} p_j = p(E_1) + p(E_2)\n\\]\n\n\n\nProduct rule for independent events\n\nTwo events \\(E\\) and \\(F\\) are called independent if the outcome of one cannot influence the other, nor is the outcome of both under the influence of a common cause.\nThen we have\n\\[\np(E \\cap F) = p(E)p(F)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAmong the first \\(N\\) experiments, count the number of times \\(E\\) has occurred (\\(S(E)\\)), F has occurred (\\(S(F)\\)), and \\(E \\cap F\\) has occurred (\\(S(E \\cap F)\\)). Then we have\n\\[\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n\\]\n\\[\np(F) = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N}\n\\]\n\\[\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N}\n\\]\nSuppose that we single out from the sequence of combined experiments the subsequence of those where \\(E\\) occurred. The frequency of occurrence of \\(F\\) in this subsequence is \\(\\frac{S(E \\cap F)}{S(E)}\\). If the two events \\(E\\) and \\(F\\) are truly independent, the frequency with which \\(F\\) occurs in this subsequence should be the same as the frequency with which \\(F\\) occurs in the original sequence, i.e.\n\\[\n\\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N} = p(F)\n\\]\nWe write the frequency of \\(\\frac{S(E \\cap F)}{N}\\) as the product\n\\[\n\\frac{S(E \\cap F)}{N} = \\frac{S(E \\cap F)}{S(E)}\\frac{S(E)}{N}\n\\]\nThen we have\n\\[\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N} = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} \\cdot \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N} = p(F)p(E)\n\\]\n\n\n\n\n\n\n\nNumerical outcome\n\nThe numerical outcome of an experiment means the assignment of a real number \\(x_j\\) to each of the possible outcomes.\nNote that different outcomes may be assigned the same number so we have to re-calculate the probability \\(p(x_j)\\) for each \\(x_j\\) with which \\(x_j\\) occurred.\n\nExpectation\n\nIn a random experiment with \\(n\\) possible outcomes of probability \\(p_j\\) and numerical outcome \\(x_j\\), the average numerical outcome, called the mean of \\(x\\) or expectation of \\(x\\), denoted by \\(\\bar{x}\\) or \\(E(x)\\), is given by the formula\n\\[\n\\bar{x} = E(x) = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAmong the first \\(N\\) experiments, denote by \\(S_j\\) the number of instances with which the \\(j\\)th outcome was observed. Then, the average numerical outcome is\n\\[\n\\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N}\n\\]\nAs \\(N \\to \\infty\\), we have\n\\[\n\\bar{x} = E(x) = \\lim\\limits_{N \\to \\infty} \\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N} = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n\\]\n\n\n\nVariance\n\nNext we are tempted to know such a fact: by how much do the numerical outcomes differ on average from the mean?\nThis is characterized by the variance, the expectation of the square of the difference of the numerical outcome and its mean:\n\\[\nV = \\overline{(x - \\bar{x})^2} = E((x - \\bar{x})^2)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\begin{aligned}\nV & = \\overline{(x - \\bar{x})^2} \\\\\n  & = E((x - \\bar{x})^2) \\\\\n  & = \\sum_{j=1}^{n} p_j (x_j - \\bar{x})^2 \\\\\n  & = p_1x_1^2 + \\cdots + p_nx_n^2 - 2(p_1x_1 + \\cdots + p_nx_n)\\bar{x} + (\\bar{x})^2 \\\\\n  & = \\bar{x^2} - (\\bar{x})^2 \\\\\n  & = E(x^2) - (E(x))^2\n\\end{aligned}\n\\]\n\n\nNote: the square root of the variance \\(\\sqrt{V}\\) is called the standard deviation.\n\n\n\n\nThe binomial distribution\n\nSuppose a random experiment has two possible outcomes \\(A\\) and \\(B\\), with probabilities \\(p\\) and \\(q\\) respectively, where \\(p + q = 1\\).\nSuppose we repeat the experiment \\(N\\) times, and the repeated experiments are independent of each other.\nIf we let \\(k\\) (\\(k = 0, 1, ..., N\\)) denote the number of times with which A occurs, then the probability that A occurs exactly \\(k\\) times is given by the formula\n\\[\nb_k(N) = \\dbinom{N}{k} p^k q^{N-k}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nSince the outcomes of the experiments are independent of each other, the probability of a particular sequence of \\(k\\) \\(A\\)’s and \\(N - k\\) \\(B\\)’s is \\(p^k q^{N-k}\\).\nIn addition, there are exactly \\(\\dbinom{N}{k}\\) arrangements of \\(k\\) \\(A\\)’s and \\(N - k\\) \\(B\\)’s, which are disjoint of each other.\n\n\nIn addition, we have\n\\[\nE(x) = \\sum_{k=0}^{N} kp(x = k) = Np\n\\]\nNote: the binomial theorem is \\((a + b)^N = \\sum_{k=0}^{N} \\dbinom{N}{k} a^k b^{N-k}\\).\n\nThe Poisson distribution\n\nSuppose each week there are a large number of vehicles through a busy intersection and there are on average \\(\\mu\\) accidents.\nSuppose the probability of a vehicle having an accident is independent of the occurrence of previous accidents.\nThen we use a binomial distribution to determine the probability of \\(k\\) accidents in a week:\nSetting \\(p = \\frac{\\mu}{N}\\), then we have\n\\[\n\\begin{aligned}\nb_k(N) & = \\dbinom{N}{k} p^k q^{N-k} \\\\\n       & = \\frac{N(N-1) \\cdots (N-k+1)}{k!} p^k q^{N-k} \\\\\n       & = (1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N}) \\frac{N^kp^k(1-p)^{N-k}}{k!} \\\\\n       & = \\frac{(1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N})}{(1-p)^k} \\frac{\\mu^k}{k!} (1-\\frac{\\mu}{N})^N\n\\end{aligned}\n\\]\nSince \\(e^{-x} = \\lim\\limits_{n \\to \\infty} (1-\\frac{x}{n})^n\\) (\\(e^x = \\lim\\limits_{n \\to \\infty} (1+\\frac{x}{n})^n\\)), then we have\n\\[\n\\lim\\limits_{N \\to \\infty,\\ \\mu = Np} b_k(N) = \\frac{\\mu^k}{k!} e^{-\\mu}\n\\]\nThis gives us an estimate for \\(b_k(N)\\) when \\(N\\) is large, \\(p\\) is small, and \\(Np = \\mu\\).\nThe Poisson distribution is defined as\n\\[\np_k(\\mu) = \\frac{\\mu^k}{k!} e^{-\\mu}\n\\]\nwhere \\(\\mu\\) is a parameter. \\(p_k\\) is the probability of \\(k\\) favorable outcomes, \\(k = 0, 1, ...\\).\nIn addition, the combination of two Poisson processes is again a Poisson process.\nDenote by \\(p_k(\\mu)\\) and \\(p_k(\\nu)\\) the probability of \\(k\\) favorable outcomes in these two processes. We claim that the probability of \\(k\\) favorable outcomes when both experiments are performed is \\(p_k(\\mu + \\nu)\\).\n\n\n\n\n\n\nNote\n\n\n\nThere will be \\(k\\) favorable outcomes for the combined experiment if the first experiment has \\(j\\) (\\(j = 0, 1, ..., k\\)) favorable outcomes and the second experiment has \\(k - j\\).\nIf the experiments are independent, the probability of such a combined outcome is the product of the probabilities \\(p_j(\\mu) p_{k-j}(\\nu)\\).\nSo the probability of the combined experiment to have \\(k\\) favorable outcomes is the sum\n\\[\n\\begin{aligned}\n\\sum_j p_j(\\mu) p_{k-j}(\\nu) & = \\sum_j \\frac{\\mu^j}{j!} e^{-\\mu} \\frac{\\nu^{k-j}}{(k-j)!} e^{-\\nu} \\\\\n                             & = \\frac{1}{k!} e^{-(\\mu + \\nu)} \\sum_j \\frac{k!}{j!(k-j)!} \\mu^j \\nu^{(k-j)} \\\\\n                             & = \\frac{(\\mu + \\nu)^k}{k!} e^{-(\\mu + \\nu)}\n\\end{aligned}\n\\]\nwhich is the Poisson distribution \\(p_k(\\mu + \\nu)\\).\n\n\n\n\n\n\nSuppose we have such an experiment making a physical measurement with an apparatus subject to random disturbances that can be reduced but not totally eliminated. Then every real number is a possible numerical outcome of such an experiment.\nRepeat the experiment as many times as we wish and denote by \\(S(x)\\) the number of instances among the first \\(N\\) performances for which the numerical outcome was less than \\(x\\).Then the frequency \\(\\frac{S(x)}{N}\\) with which this event occurs tends to a limit as \\(N\\) tends to infinity. This limit is the probability that the outcome is less than \\(\\mathbfcal{x}\\), and is denoted by \\(P(x)\\):\n\\[\nP(x) = \\lim\\limits_{N \\to \\infty} \\frac{S(x)}{N}\n\\]\nThe probability \\(P(x)\\) has the following properties:\n\n\\(0 \\leq P(x) \\leq 1\\): \\(0 \\leq S(x) \\leq N \\implies 0 \\leq \\frac{S(x)}{N} \\leq 1 \\implies 0 \\leq P(x) \\leq 1\\).\n\\(P(x)\\) is a nondecreasing function of \\(x\\): \\(S(x)\\) is a nondecreasing function of \\(x\\), so that \\(\\frac{S(x)}{N}\\) is a nondecreasing function of \\(x\\); then so is the limit \\(P(x)\\).\n\\(P(x) \\to 0\\ (x \\to -\\infty)\\).\n\\(P(x) \\to 1\\ (x \\to \\infty)\\).\n\\(P(x)\\) is a continuously differentiable function (denote the derivative of \\(P\\) by \\(p\\)): \\(\\frac{\\mathrm{d}P(x)}{dx} = p(x)\\).\n\nThe function \\(p(x)\\) is called the probability density function.\n\n\n\n\n\n\nAddition rule for disjoint events\n\n\n\nSuppose \\(E\\) and \\(F\\) are two events with probabilities \\(P(E)\\) and \\(P(F)\\) respectively.\nSuppose they are disjoint (i.e. \\(E \\cap F = \\emptyset\\)).\nThen we have\n\\[\nP(E \\cup F) = P(E) + P(F)\n\\]\nLet \\(E: x &lt; a\\), \\(F: a \\leq x &lt; b\\), then we have \\(E \\cup F: x &lt; b\\).\nThen we have\n\\[\nP(E) = P(a),\\ P(E \\cup F) = P(b)\n\\]\nWe conclude that\n\\[\nP(F) = P(b) - P(a)\n\\]\nis the probability of \\(a \\leq x &lt; b\\).\n\n\nAccording to the mean value theorem, for every \\(a\\) and \\(b\\), there is a number \\(c\\) lying between \\(a\\) and \\(b\\) such that\n\\[\nP(b) - P(a) = p(c)(b - a)\n\\]\nAccording to the fundamental theorem of calculus\n\\[\nP(b) - P(a) = \\int_a^b p(x)\\mathrm{d}x\n\\]\nAccording to \\(P(a) \\to 0\\ (a \\to -\\infty)\\)\n\\[\nP(b) = \\int_{-\\infty}^b p(x)\\mathrm{d}x\n\\]\nAccording to \\(P(b) \\to 1\\ (b \\to \\infty)\\)\n\\[\n1 = \\int_{-\\infty}^{\\infty} p(x)\\mathrm{d}x\n\\]\nThis is the continuous analogue of the basic fact that \\(p_1 + p_2 + \\cdots + p_n = 1\\) in discrete probability.\n\n\\(p(x) \\geq 0\\): \\(P(x)\\) is a nondecreasing function of \\(x\\).\nThe expectation is:\n\n\\[\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n\\]\n\n\n\n\n\n\nNote\n\n\n\nImagine the experiment performed as many times as we wish, and denote the sequence of outcomes by\n\\[\na_1, a_2, ..., a_N, ...\n\\]\nDivide the interval \\(I\\) in which all outcomes lie into \\(n\\) subintervals \\(I_1, ..., I_n\\). Denote the endpoints by\n\\[\ne_0 &lt; e_1 &lt; \\cdots &lt; e_n\n\\]\nThe probability of \\(e_{j-1} \\leq x &lt; e_j\\) (i.e. \\(x\\) lies in the interval \\(I_j\\)) is\n\\[\nP_j = P(e_j) - P(e_{j-1}) = p(x_j)(e_j - e_{j-1})\n\\]\nwhere \\(x_j\\) is a point in \\(I_j\\) guaranteed by the mean value theorem, and \\(e_j - e_{j-1}\\) denotes the length of \\(I_j\\).\nWe now simplify the original experiment by recording merely the intervals \\(I_j\\) in which the outcome falls, and calling the numerical outcome in this case \\(x_j\\), the point in \\(I_j\\) appears in the above formula; therefore, the actual outcome falling into \\(I_j\\) differs from \\(x_j\\) by at most \\(e_j - e_{j-1}\\).\nNow consider the sequence of outcomes \\(a_1, a_2, ...\\) of the original experiment. Denote the corresponding outcomes of the simplified experiment by \\(b_1, b_2, ...\\). The simplified experiment has a finite number of outcomes. For such discrete experiments, we have the expectation\n\\[\n\\bar{x}_n = \\lim\\limits_{N \\to \\infty} \\frac{b_1 + \\cdots + b_N}{N}\n\\]\nwhere \\(n\\) is the number of subintervals of \\(I\\).\nIn fact, the expectation \\(\\bar{x}_n\\) of the simplified experiment can also be calculated by formula\n\\[\n\\bar{x}_n = P_1x_1 + \\cdots + P_nx_n\n\\]\nThen we have\n\\[\n\\bar{x}_n = p(x_1)x_1(e_1 - e_0) + \\cdots + p(x_n)x_n(e_n - e_{n-1})\n\\]\nAs \\(n \\to \\infty\\), we have\n\\[\n\\bar{x} = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^n x_ip(x_i)\\Delta x_i = \\int_{e_0}^{e_n} xp(x)\\mathrm{d}x\n\\]\nSo we conclude\n\\[\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n\\]\n\n\n\nThe probability density of a combined experiment of two experiments independent of each other:\n\n\nCase 1: the outcome of the first experiment may be any real number, but the second experiment can have only a finite number of outcomes.\n\nDenote by \\(P(a)\\) the probability of \\(x &lt; a\\). The second experiment has \\(n\\) possible outcomes \\(a_1, ..., a_n\\) with probabilities \\(Q_1, ..., Q_n\\).\nWe define the numerical outcome of the combined experiment to be the sum of the separate numerical outcomes of the two experiments that constitute it.\nWe denote by \\(E(x)\\) the event that the numerical outcome of the combined experiment is less than \\(x\\), and denote its probability by \\(U(x)\\).\nThen we have\n\\[\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe denote by \\(E_j(x)\\) the event that the numerical outcome of the second experiment is \\(a_j\\). The numerical outcome of the combined experiment is then less than \\(x\\) if and only if the numerical outcome of the first experiment is less than \\(x - a_j\\).\nThen we have\n\\[\nE(x) = E_1(x) \\cup \\cdots \\cup E_n(x)\n\\]\nwhere the events \\(E_j(x)\\) are disjoint.\nIt follows from the addition rule for disjoint events that\n\\[\nU(x) = P(E_1(x)) + \\cdots + P(E_n(x))\n\\]\nSince the two experiments are independent, we have \\(P(E_j(x)) = Q_j P(x-a_j)\\).\nSo we have\n\\[\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n\\]\n\n\n\nCase 2: both experiments can have any real number as outcome.\n\nDenote by \\(P(a)\\) and \\(Q(a)\\) the probabilities that the outcome is less than \\(a\\) in each of the two experiments, respectively.\nAssume the outcome of the second experiment always lies in some finite interval \\(I\\). Then we subdivide \\(I\\) into a finite number \\(n\\) of subintervals \\(I_j = [e_{j-1}, e_j)\\). Let \\(Q_j\\) denote the probability of the outcome of the experiment lying in \\(I_j\\).\nSuppose \\(Q(x)\\) is continuously differentiable and denote its derivative by \\(q(x)\\).\nAccording to the mean value theorem, we have\n\\[\nQ_j = Q(e_j) - Q(e_{j-1}) = q(a_j)(e_j - e_{j-1})\n\\]\nwhere \\(a_j\\) is some point in \\(I_j\\).\nWe discretize the second experiment by lumping together all outcomes that lie in \\(I_j\\) and redefine the numerical outcome in that case to be \\(a_j\\).\nThen we have\n\\[\n\\begin{aligned}\nU_n(x) & = q(a_1)P(x-a_1)(e_1-e_0) + \\cdots + q(a_n)P(x-a_n)(e_n-e_{n-1}) \\\\\n       & = \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i\n\\end{aligned}\n\\]\nAs \\(n \\to \\infty\\), we have\n\\[\n\\begin{aligned}\nU(x) & = \\lim\\limits_{n \\to \\infty} U_n(x) \\\\\n     & = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i \\\\\n     & = \\int\\limits_{I} q(a)P(x-a)\\mathrm{d}a\n\\end{aligned}\n\\]\nThen we have\n\\[\nU(x) = \\int_{-\\infty}^{\\infty} q(a)P(x-a)\\mathrm{d}a\n\\]\nFurther, let us suppose \\(P(x)\\) is continuously differentiable, and denote its derivative by \\(p(x)\\).\nThen we have\n\\[\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n\\]\nwhere \\(u(x)\\) is the derivative of \\(U(x)\\).\nIn a word, we have proved the following fact:\n\n\n\n\n\n\nNote\n\n\n\nConsider two independent experiments whose outcomes lie in some finite interval and have probability \\(p\\) and \\(q\\) respectively.\nIn the combined experiment of the two experiments, define the outcome of the combined experiment to be the sum of the outcomes of the individual experiments.\nThen the combined experiment has the probability density:\n\\[\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n\\]\n\n\n\nThe convolution of the functions \\(q\\) and \\(p\\):\n\nThe function \\(u\\) defined by \\(u(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\\) is called the convolution of the functions \\(q\\) and \\(p\\). This relation is denoted by \\(u = q*p\\).\nThe convolution has the following properties:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(q_1\\), \\(q_2\\), and \\(p\\) be continuous functions defined for all real numbers \\(x\\), and assume the functions are \\(0\\) outside a finite interval. Then we have\n\nConvolution is distributive: \\((q_1+q_2)*p = q_1*p + q_2*p\\).\nLet \\(k\\) be any constant. Then \\((kq)*p = k(q*p)\\).\nConvolution is commutative: \\(q*p = p*q\\).\nThe integral of the convolution is the product of the integrals of the factors:\n\n\\[\n\\int_{-\\infty}^{\\infty} u(x) \\mathrm{d}x = \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x \\int_{-\\infty}^{\\infty} q(a) \\mathrm{d}a\n\\]"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_misc/index.html#from-the-book-calculus-with-applications-by-peter-d.-lax",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_misc/index.html#from-the-book-calculus-with-applications-by-peter-d.-lax",
    "title": "Probability and statistics (misc)",
    "section": "",
    "text": "Probability is the branch of mathematics that deals with events whose individual outcomes are unpredictable, but whose outcomes on average are predictable.\nExperiments can be divided into two types:\n\nDeterministic: whose individual outcomes are predictable;\nNondeterministic (random): whose individual outcomes are unpredictable.\n\n\n\nNext, all experiments we’ll deal with are repeatable (it can be performed repeatedly any number of times) and random (any single performance of the experiment is unpredictable).\nIn this section, we’ll deal with experiments having a finite number of possible outcomes. We denote the number of possible outcomes by \\(n\\), and number them from \\(1\\) to \\(n\\).\n\n\nIn a random and repeatable experiment, if we denote \\(S_j\\) by the number of instances among the first \\(N\\) experiments where the \\(j\\)th outcome was observed to occur, then the frequency \\(\\frac{S_j}{N}\\) with which the \\(j\\)th outcome has been observed to occur tends to a limit as \\(N\\) tends to infinity. We call this limit the probability of the \\(j\\)th outcome and denote it by \\(p_j\\):\n\\[\np_j = \\lim\\limits_{N \\to \\infty} \\frac{S_j}{N}\n\\]\nThese probabilities have the following properties:\n\n\\(0 \\leq p_j \\leq 1\\);\n\n\n\n\n\n\n\nNote\n\n\n\nFor \\(\\frac{S_j}{N}\\) lies between \\(0\\) and \\(1\\), and therefore so does its limit \\(p_j\\).\n\n\n\n\\(\\sum_{j=1}^{n} p_j = 1\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWe have\n\\[\nS_1 + S_2 + \\cdots + S_n = N\n\\]\nDividing by \\(N\\), we get\n\\[\n\\frac{S_1}{N} + \\frac{S_2}{N} + \\cdots + \\frac{S_n}{N} = 1\n\\]\nAs \\(N\\) tends to infinity, we have\n\\[\n\\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} \\frac{S_j}{N} = \\lim\\limits_{N \\to \\infty} \\sum_{j=1}^{n} p_j = 1\n\\]\n\n\n\n\n\nIn fact very often, we are not interested in all the details of the outcome of an experiment, but merely in a particular aspect of it (e.g. throwing a die, we may be interested only in whether the outcome is even or odd).\nAn occurrence such as throwing an even number is called an event, which is defined as the following:\n\nAn event \\(E\\) is defined as any collection of possible outcomes.\n\nNote: we say that an event \\(E\\) occurred whenever any outcome belonging to \\(E\\) occurred.\nThe probability \\(p(E)\\) of an event \\(E\\) is\n\\[\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n\\]\nwhere \\(S(E)\\) is the number of instances among the first \\(N\\) experiments when the event \\(E\\) took place.\n\n\n\n\n\n\nNote\n\n\n\nWe have\n\\[\nS(E) = \\sum_{j\\ \\text{in}\\ E} S_j\n\\]\nDividing by \\(N\\)\n\\[\n\\frac{S(E)}{N} = \\sum_{j\\ \\text{in}\\ E} \\frac{S_j}{N}\n\\]\nAs \\(N\\) tends to infinity, we have\n\\[\np(E) = \\sum_{j\\ \\text{in}\\ E} p_j\n\\]\n\n\n\n\n\nAddition rule for disjoint events\n\nTwo events \\(E_1\\) and \\(E_2\\) are called disjoint if both cannot take place simultaneously (i.e. \\(E_1 \\cap E_2 = \\emptyset\\)).\nThen we have\n\\[\np(E_1 \\cup E_2) = p(E_1) + p(E_2)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j\n\\]\nDisjointness means that an outcome \\(j\\) may belong either to \\(E_1\\) or to \\(E_2\\) but not to both; therefore,\n\\[\np(E_1 \\cup E_2) = \\sum_{j\\ \\text{in}\\ E_1\\ \\text{or}\\ E_2} p_j = \\sum_{j\\ \\text{in}\\ E_1} p_j + \\sum_{j\\ \\text{in}\\ E_2} p_j = p(E_1) + p(E_2)\n\\]\n\n\n\nProduct rule for independent events\n\nTwo events \\(E\\) and \\(F\\) are called independent if the outcome of one cannot influence the other, nor is the outcome of both under the influence of a common cause.\nThen we have\n\\[\np(E \\cap F) = p(E)p(F)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAmong the first \\(N\\) experiments, count the number of times \\(E\\) has occurred (\\(S(E)\\)), F has occurred (\\(S(F)\\)), and \\(E \\cap F\\) has occurred (\\(S(E \\cap F)\\)). Then we have\n\\[\np(E) = \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N}\n\\]\n\\[\np(F) = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N}\n\\]\n\\[\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N}\n\\]\nSuppose that we single out from the sequence of combined experiments the subsequence of those where \\(E\\) occurred. The frequency of occurrence of \\(F\\) in this subsequence is \\(\\frac{S(E \\cap F)}{S(E)}\\). If the two events \\(E\\) and \\(F\\) are truly independent, the frequency with which \\(F\\) occurs in this subsequence should be the same as the frequency with which \\(F\\) occurs in the original sequence, i.e.\n\\[\n\\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} = \\lim\\limits_{N \\to \\infty} \\frac{S(F)}{N} = p(F)\n\\]\nWe write the frequency of \\(\\frac{S(E \\cap F)}{N}\\) as the product\n\\[\n\\frac{S(E \\cap F)}{N} = \\frac{S(E \\cap F)}{S(E)}\\frac{S(E)}{N}\n\\]\nThen we have\n\\[\np(E \\cap F) = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{N} = \\lim\\limits_{N \\to \\infty} \\frac{S(E \\cap F)}{S(E)} \\cdot \\lim\\limits_{N \\to \\infty} \\frac{S(E)}{N} = p(F)p(E)\n\\]\n\n\n\n\n\n\n\nNumerical outcome\n\nThe numerical outcome of an experiment means the assignment of a real number \\(x_j\\) to each of the possible outcomes.\nNote that different outcomes may be assigned the same number so we have to re-calculate the probability \\(p(x_j)\\) for each \\(x_j\\) with which \\(x_j\\) occurred.\n\nExpectation\n\nIn a random experiment with \\(n\\) possible outcomes of probability \\(p_j\\) and numerical outcome \\(x_j\\), the average numerical outcome, called the mean of \\(x\\) or expectation of \\(x\\), denoted by \\(\\bar{x}\\) or \\(E(x)\\), is given by the formula\n\\[\n\\bar{x} = E(x) = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAmong the first \\(N\\) experiments, denote by \\(S_j\\) the number of instances with which the \\(j\\)th outcome was observed. Then, the average numerical outcome is\n\\[\n\\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N}\n\\]\nAs \\(N \\to \\infty\\), we have\n\\[\n\\bar{x} = E(x) = \\lim\\limits_{N \\to \\infty} \\frac{S_1x_1 + S_2x_2 + \\cdots + S_nx_n}{N} = p_1x_1 + p_2x_2 + \\cdots + p_nx_n\n\\]\n\n\n\nVariance\n\nNext we are tempted to know such a fact: by how much do the numerical outcomes differ on average from the mean?\nThis is characterized by the variance, the expectation of the square of the difference of the numerical outcome and its mean:\n\\[\nV = \\overline{(x - \\bar{x})^2} = E((x - \\bar{x})^2)\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\n\\begin{aligned}\nV & = \\overline{(x - \\bar{x})^2} \\\\\n  & = E((x - \\bar{x})^2) \\\\\n  & = \\sum_{j=1}^{n} p_j (x_j - \\bar{x})^2 \\\\\n  & = p_1x_1^2 + \\cdots + p_nx_n^2 - 2(p_1x_1 + \\cdots + p_nx_n)\\bar{x} + (\\bar{x})^2 \\\\\n  & = \\bar{x^2} - (\\bar{x})^2 \\\\\n  & = E(x^2) - (E(x))^2\n\\end{aligned}\n\\]\n\n\nNote: the square root of the variance \\(\\sqrt{V}\\) is called the standard deviation.\n\n\n\n\nThe binomial distribution\n\nSuppose a random experiment has two possible outcomes \\(A\\) and \\(B\\), with probabilities \\(p\\) and \\(q\\) respectively, where \\(p + q = 1\\).\nSuppose we repeat the experiment \\(N\\) times, and the repeated experiments are independent of each other.\nIf we let \\(k\\) (\\(k = 0, 1, ..., N\\)) denote the number of times with which A occurs, then the probability that A occurs exactly \\(k\\) times is given by the formula\n\\[\nb_k(N) = \\dbinom{N}{k} p^k q^{N-k}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nSince the outcomes of the experiments are independent of each other, the probability of a particular sequence of \\(k\\) \\(A\\)’s and \\(N - k\\) \\(B\\)’s is \\(p^k q^{N-k}\\).\nIn addition, there are exactly \\(\\dbinom{N}{k}\\) arrangements of \\(k\\) \\(A\\)’s and \\(N - k\\) \\(B\\)’s, which are disjoint of each other.\n\n\nIn addition, we have\n\\[\nE(x) = \\sum_{k=0}^{N} kp(x = k) = Np\n\\]\nNote: the binomial theorem is \\((a + b)^N = \\sum_{k=0}^{N} \\dbinom{N}{k} a^k b^{N-k}\\).\n\nThe Poisson distribution\n\nSuppose each week there are a large number of vehicles through a busy intersection and there are on average \\(\\mu\\) accidents.\nSuppose the probability of a vehicle having an accident is independent of the occurrence of previous accidents.\nThen we use a binomial distribution to determine the probability of \\(k\\) accidents in a week:\nSetting \\(p = \\frac{\\mu}{N}\\), then we have\n\\[\n\\begin{aligned}\nb_k(N) & = \\dbinom{N}{k} p^k q^{N-k} \\\\\n       & = \\frac{N(N-1) \\cdots (N-k+1)}{k!} p^k q^{N-k} \\\\\n       & = (1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N}) \\frac{N^kp^k(1-p)^{N-k}}{k!} \\\\\n       & = \\frac{(1-\\frac{1}{N}) \\cdots (1-\\frac{k-1}{N})}{(1-p)^k} \\frac{\\mu^k}{k!} (1-\\frac{\\mu}{N})^N\n\\end{aligned}\n\\]\nSince \\(e^{-x} = \\lim\\limits_{n \\to \\infty} (1-\\frac{x}{n})^n\\) (\\(e^x = \\lim\\limits_{n \\to \\infty} (1+\\frac{x}{n})^n\\)), then we have\n\\[\n\\lim\\limits_{N \\to \\infty,\\ \\mu = Np} b_k(N) = \\frac{\\mu^k}{k!} e^{-\\mu}\n\\]\nThis gives us an estimate for \\(b_k(N)\\) when \\(N\\) is large, \\(p\\) is small, and \\(Np = \\mu\\).\nThe Poisson distribution is defined as\n\\[\np_k(\\mu) = \\frac{\\mu^k}{k!} e^{-\\mu}\n\\]\nwhere \\(\\mu\\) is a parameter. \\(p_k\\) is the probability of \\(k\\) favorable outcomes, \\(k = 0, 1, ...\\).\nIn addition, the combination of two Poisson processes is again a Poisson process.\nDenote by \\(p_k(\\mu)\\) and \\(p_k(\\nu)\\) the probability of \\(k\\) favorable outcomes in these two processes. We claim that the probability of \\(k\\) favorable outcomes when both experiments are performed is \\(p_k(\\mu + \\nu)\\).\n\n\n\n\n\n\nNote\n\n\n\nThere will be \\(k\\) favorable outcomes for the combined experiment if the first experiment has \\(j\\) (\\(j = 0, 1, ..., k\\)) favorable outcomes and the second experiment has \\(k - j\\).\nIf the experiments are independent, the probability of such a combined outcome is the product of the probabilities \\(p_j(\\mu) p_{k-j}(\\nu)\\).\nSo the probability of the combined experiment to have \\(k\\) favorable outcomes is the sum\n\\[\n\\begin{aligned}\n\\sum_j p_j(\\mu) p_{k-j}(\\nu) & = \\sum_j \\frac{\\mu^j}{j!} e^{-\\mu} \\frac{\\nu^{k-j}}{(k-j)!} e^{-\\nu} \\\\\n                             & = \\frac{1}{k!} e^{-(\\mu + \\nu)} \\sum_j \\frac{k!}{j!(k-j)!} \\mu^j \\nu^{(k-j)} \\\\\n                             & = \\frac{(\\mu + \\nu)^k}{k!} e^{-(\\mu + \\nu)}\n\\end{aligned}\n\\]\nwhich is the Poisson distribution \\(p_k(\\mu + \\nu)\\).\n\n\n\n\n\n\nSuppose we have such an experiment making a physical measurement with an apparatus subject to random disturbances that can be reduced but not totally eliminated. Then every real number is a possible numerical outcome of such an experiment.\nRepeat the experiment as many times as we wish and denote by \\(S(x)\\) the number of instances among the first \\(N\\) performances for which the numerical outcome was less than \\(x\\).Then the frequency \\(\\frac{S(x)}{N}\\) with which this event occurs tends to a limit as \\(N\\) tends to infinity. This limit is the probability that the outcome is less than \\(\\mathbfcal{x}\\), and is denoted by \\(P(x)\\):\n\\[\nP(x) = \\lim\\limits_{N \\to \\infty} \\frac{S(x)}{N}\n\\]\nThe probability \\(P(x)\\) has the following properties:\n\n\\(0 \\leq P(x) \\leq 1\\): \\(0 \\leq S(x) \\leq N \\implies 0 \\leq \\frac{S(x)}{N} \\leq 1 \\implies 0 \\leq P(x) \\leq 1\\).\n\\(P(x)\\) is a nondecreasing function of \\(x\\): \\(S(x)\\) is a nondecreasing function of \\(x\\), so that \\(\\frac{S(x)}{N}\\) is a nondecreasing function of \\(x\\); then so is the limit \\(P(x)\\).\n\\(P(x) \\to 0\\ (x \\to -\\infty)\\).\n\\(P(x) \\to 1\\ (x \\to \\infty)\\).\n\\(P(x)\\) is a continuously differentiable function (denote the derivative of \\(P\\) by \\(p\\)): \\(\\frac{\\mathrm{d}P(x)}{dx} = p(x)\\).\n\nThe function \\(p(x)\\) is called the probability density function.\n\n\n\n\n\n\nAddition rule for disjoint events\n\n\n\nSuppose \\(E\\) and \\(F\\) are two events with probabilities \\(P(E)\\) and \\(P(F)\\) respectively.\nSuppose they are disjoint (i.e. \\(E \\cap F = \\emptyset\\)).\nThen we have\n\\[\nP(E \\cup F) = P(E) + P(F)\n\\]\nLet \\(E: x &lt; a\\), \\(F: a \\leq x &lt; b\\), then we have \\(E \\cup F: x &lt; b\\).\nThen we have\n\\[\nP(E) = P(a),\\ P(E \\cup F) = P(b)\n\\]\nWe conclude that\n\\[\nP(F) = P(b) - P(a)\n\\]\nis the probability of \\(a \\leq x &lt; b\\).\n\n\nAccording to the mean value theorem, for every \\(a\\) and \\(b\\), there is a number \\(c\\) lying between \\(a\\) and \\(b\\) such that\n\\[\nP(b) - P(a) = p(c)(b - a)\n\\]\nAccording to the fundamental theorem of calculus\n\\[\nP(b) - P(a) = \\int_a^b p(x)\\mathrm{d}x\n\\]\nAccording to \\(P(a) \\to 0\\ (a \\to -\\infty)\\)\n\\[\nP(b) = \\int_{-\\infty}^b p(x)\\mathrm{d}x\n\\]\nAccording to \\(P(b) \\to 1\\ (b \\to \\infty)\\)\n\\[\n1 = \\int_{-\\infty}^{\\infty} p(x)\\mathrm{d}x\n\\]\nThis is the continuous analogue of the basic fact that \\(p_1 + p_2 + \\cdots + p_n = 1\\) in discrete probability.\n\n\\(p(x) \\geq 0\\): \\(P(x)\\) is a nondecreasing function of \\(x\\).\nThe expectation is:\n\n\\[\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n\\]\n\n\n\n\n\n\nNote\n\n\n\nImagine the experiment performed as many times as we wish, and denote the sequence of outcomes by\n\\[\na_1, a_2, ..., a_N, ...\n\\]\nDivide the interval \\(I\\) in which all outcomes lie into \\(n\\) subintervals \\(I_1, ..., I_n\\). Denote the endpoints by\n\\[\ne_0 &lt; e_1 &lt; \\cdots &lt; e_n\n\\]\nThe probability of \\(e_{j-1} \\leq x &lt; e_j\\) (i.e. \\(x\\) lies in the interval \\(I_j\\)) is\n\\[\nP_j = P(e_j) - P(e_{j-1}) = p(x_j)(e_j - e_{j-1})\n\\]\nwhere \\(x_j\\) is a point in \\(I_j\\) guaranteed by the mean value theorem, and \\(e_j - e_{j-1}\\) denotes the length of \\(I_j\\).\nWe now simplify the original experiment by recording merely the intervals \\(I_j\\) in which the outcome falls, and calling the numerical outcome in this case \\(x_j\\), the point in \\(I_j\\) appears in the above formula; therefore, the actual outcome falling into \\(I_j\\) differs from \\(x_j\\) by at most \\(e_j - e_{j-1}\\).\nNow consider the sequence of outcomes \\(a_1, a_2, ...\\) of the original experiment. Denote the corresponding outcomes of the simplified experiment by \\(b_1, b_2, ...\\). The simplified experiment has a finite number of outcomes. For such discrete experiments, we have the expectation\n\\[\n\\bar{x}_n = \\lim\\limits_{N \\to \\infty} \\frac{b_1 + \\cdots + b_N}{N}\n\\]\nwhere \\(n\\) is the number of subintervals of \\(I\\).\nIn fact, the expectation \\(\\bar{x}_n\\) of the simplified experiment can also be calculated by formula\n\\[\n\\bar{x}_n = P_1x_1 + \\cdots + P_nx_n\n\\]\nThen we have\n\\[\n\\bar{x}_n = p(x_1)x_1(e_1 - e_0) + \\cdots + p(x_n)x_n(e_n - e_{n-1})\n\\]\nAs \\(n \\to \\infty\\), we have\n\\[\n\\bar{x} = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^n x_ip(x_i)\\Delta x_i = \\int_{e_0}^{e_n} xp(x)\\mathrm{d}x\n\\]\nSo we conclude\n\\[\n\\bar{x} = \\int_{-\\infty}^{\\infty} xp(x)\\mathrm{d}x\n\\]\n\n\n\nThe probability density of a combined experiment of two experiments independent of each other:\n\n\nCase 1: the outcome of the first experiment may be any real number, but the second experiment can have only a finite number of outcomes.\n\nDenote by \\(P(a)\\) the probability of \\(x &lt; a\\). The second experiment has \\(n\\) possible outcomes \\(a_1, ..., a_n\\) with probabilities \\(Q_1, ..., Q_n\\).\nWe define the numerical outcome of the combined experiment to be the sum of the separate numerical outcomes of the two experiments that constitute it.\nWe denote by \\(E(x)\\) the event that the numerical outcome of the combined experiment is less than \\(x\\), and denote its probability by \\(U(x)\\).\nThen we have\n\\[\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe denote by \\(E_j(x)\\) the event that the numerical outcome of the second experiment is \\(a_j\\). The numerical outcome of the combined experiment is then less than \\(x\\) if and only if the numerical outcome of the first experiment is less than \\(x - a_j\\).\nThen we have\n\\[\nE(x) = E_1(x) \\cup \\cdots \\cup E_n(x)\n\\]\nwhere the events \\(E_j(x)\\) are disjoint.\nIt follows from the addition rule for disjoint events that\n\\[\nU(x) = P(E_1(x)) + \\cdots + P(E_n(x))\n\\]\nSince the two experiments are independent, we have \\(P(E_j(x)) = Q_j P(x-a_j)\\).\nSo we have\n\\[\nU(x) = Q_1P(x-a_1) + \\cdots + Q_nP(x-a_n)\n\\]\n\n\n\nCase 2: both experiments can have any real number as outcome.\n\nDenote by \\(P(a)\\) and \\(Q(a)\\) the probabilities that the outcome is less than \\(a\\) in each of the two experiments, respectively.\nAssume the outcome of the second experiment always lies in some finite interval \\(I\\). Then we subdivide \\(I\\) into a finite number \\(n\\) of subintervals \\(I_j = [e_{j-1}, e_j)\\). Let \\(Q_j\\) denote the probability of the outcome of the experiment lying in \\(I_j\\).\nSuppose \\(Q(x)\\) is continuously differentiable and denote its derivative by \\(q(x)\\).\nAccording to the mean value theorem, we have\n\\[\nQ_j = Q(e_j) - Q(e_{j-1}) = q(a_j)(e_j - e_{j-1})\n\\]\nwhere \\(a_j\\) is some point in \\(I_j\\).\nWe discretize the second experiment by lumping together all outcomes that lie in \\(I_j\\) and redefine the numerical outcome in that case to be \\(a_j\\).\nThen we have\n\\[\n\\begin{aligned}\nU_n(x) & = q(a_1)P(x-a_1)(e_1-e_0) + \\cdots + q(a_n)P(x-a_n)(e_n-e_{n-1}) \\\\\n       & = \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i\n\\end{aligned}\n\\]\nAs \\(n \\to \\infty\\), we have\n\\[\n\\begin{aligned}\nU(x) & = \\lim\\limits_{n \\to \\infty} U_n(x) \\\\\n     & = \\lim\\limits_{n \\to \\infty} \\sum_{i=1}^{n} q(a_i)p(x-a_i)\\Delta a_i \\\\\n     & = \\int\\limits_{I} q(a)P(x-a)\\mathrm{d}a\n\\end{aligned}\n\\]\nThen we have\n\\[\nU(x) = \\int_{-\\infty}^{\\infty} q(a)P(x-a)\\mathrm{d}a\n\\]\nFurther, let us suppose \\(P(x)\\) is continuously differentiable, and denote its derivative by \\(p(x)\\).\nThen we have\n\\[\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n\\]\nwhere \\(u(x)\\) is the derivative of \\(U(x)\\).\nIn a word, we have proved the following fact:\n\n\n\n\n\n\nNote\n\n\n\nConsider two independent experiments whose outcomes lie in some finite interval and have probability \\(p\\) and \\(q\\) respectively.\nIn the combined experiment of the two experiments, define the outcome of the combined experiment to be the sum of the outcomes of the individual experiments.\nThen the combined experiment has the probability density:\n\\[\nu(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\n\\]\n\n\n\nThe convolution of the functions \\(q\\) and \\(p\\):\n\nThe function \\(u\\) defined by \\(u(x) = \\int_{-\\infty}^{\\infty} q(a)p(x-a)\\mathrm{d}a\\) is called the convolution of the functions \\(q\\) and \\(p\\). This relation is denoted by \\(u = q*p\\).\nThe convolution has the following properties:\n\n\n\n\n\n\nNote\n\n\n\nLet \\(q_1\\), \\(q_2\\), and \\(p\\) be continuous functions defined for all real numbers \\(x\\), and assume the functions are \\(0\\) outside a finite interval. Then we have\n\nConvolution is distributive: \\((q_1+q_2)*p = q_1*p + q_2*p\\).\nLet \\(k\\) be any constant. Then \\((kq)*p = k(q*p)\\).\nConvolution is commutative: \\(q*p = p*q\\).\nThe integral of the convolution is the product of the integrals of the factors:\n\n\\[\n\\int_{-\\infty}^{\\infty} u(x) \\mathrm{d}x = \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x \\int_{-\\infty}^{\\infty} q(a) \\mathrm{d}a\n\\]"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html",
    "title": "Lorenz curve and Gini index",
    "section": "",
    "text": "In economics, the Lorenz curve is a graphical representation of the distribution of income or of wealth.\nThe curve is a graph showing the proportion of overall income or wealth assumed by the bottom \\(\\mathbf{x\\%}\\) of the people. It is often used to represent income distribution, where it shows for the bottom \\(x\\%\\) of households, what percentage \\(y\\%\\) of the total income they have.\n\n\n\n\n\n\n\nA typical Lorenz curve\n\n\nA perfectly equal income distribution would be in which everyone has the same income. In this case, the bottom \\(N\\%\\) of society would always have \\(N\\%\\) of the income. This can be depicted by the straight line \\(y = x\\), called the “line of perfect equality”.\nBy contrast, a perfectly unequal distribution would be one in which one person has all the income and everyone else has none. In that case, the curve would be at \\(y = 0\\%\\) for all \\(x &lt; 100\\%\\), and \\(y = 100\\%\\) when \\(x = 100\\%\\). This curve is called the “line of perfect inequality”.\n\n\n\nThe Lorenz curve can usually be represented by a function \\(L(F)\\), where \\(F\\), the cumulative portion of the population, is represented by the horizontal axis, and \\(L\\), the cumulative portion of the total wealth or income, is represented by the vertical axis.\nThe curve \\(L\\) need not be a smoothly increasing function of \\(F\\). For wealth distributions there may be oligarchies or people with negative wealth for instance.\n\nDiscrete distribution\n\nFor a discrete distribution of \\(Y\\) given by values \\(y_1, ..., y_n\\) in non-decresing order (\\(y_i \\le y_{i+1}\\)) and their probabilities \\(f(y_i) := Pr(Y = y_i)\\), the Lorenz curve is a continuous piecewise linear function connecting the points \\((F_i, L_i)\\) for \\(i=1\\) to \\(n\\), where \\(F_0 = 0, L_0 = 0\\).\n\\[\n\\begin{align}\nF_i &:= \\sum_{j=1}^{i} f(y_j) \\\\\nS_i &:= \\sum_{j=1}^{i} y_i f(y_i) \\\\\nS &:= \\sum_{i=1}^{n} y_i f(y_i) \\\\\nL_i &:= \\frac{S_i}{S}\n\\end{align}\n\\]\n\nContinuous distribution\n\nFor a continuous distribution with the PDF \\(f\\) and the CDF \\(F\\), the Lorenz curve \\(L\\) is given by\n\\[\nL(F(x)) = \\frac{\\int_{-\\infty}^{x} t f(t) dt}{\\int_{-\\infty}^\\infty t f(t) dt} = \\frac{\\int_{-\\infty}^{x} t f(t) dt}{\\mu}\n\\]\nwhere \\(\\mu\\) denotes the average.\nThe Lorenz curve \\(L(F)\\) may then be plotted as a function parametric in \\(x\\): \\(L(x) \\text{ vs. } F(x)\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html#lorenz-curve",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html#lorenz-curve",
    "title": "Lorenz curve and Gini index",
    "section": "",
    "text": "In economics, the Lorenz curve is a graphical representation of the distribution of income or of wealth.\nThe curve is a graph showing the proportion of overall income or wealth assumed by the bottom \\(\\mathbf{x\\%}\\) of the people. It is often used to represent income distribution, where it shows for the bottom \\(x\\%\\) of households, what percentage \\(y\\%\\) of the total income they have.\n\n\n\n\n\n\n\nA typical Lorenz curve\n\n\nA perfectly equal income distribution would be in which everyone has the same income. In this case, the bottom \\(N\\%\\) of society would always have \\(N\\%\\) of the income. This can be depicted by the straight line \\(y = x\\), called the “line of perfect equality”.\nBy contrast, a perfectly unequal distribution would be one in which one person has all the income and everyone else has none. In that case, the curve would be at \\(y = 0\\%\\) for all \\(x &lt; 100\\%\\), and \\(y = 100\\%\\) when \\(x = 100\\%\\). This curve is called the “line of perfect inequality”.\n\n\n\nThe Lorenz curve can usually be represented by a function \\(L(F)\\), where \\(F\\), the cumulative portion of the population, is represented by the horizontal axis, and \\(L\\), the cumulative portion of the total wealth or income, is represented by the vertical axis.\nThe curve \\(L\\) need not be a smoothly increasing function of \\(F\\). For wealth distributions there may be oligarchies or people with negative wealth for instance.\n\nDiscrete distribution\n\nFor a discrete distribution of \\(Y\\) given by values \\(y_1, ..., y_n\\) in non-decresing order (\\(y_i \\le y_{i+1}\\)) and their probabilities \\(f(y_i) := Pr(Y = y_i)\\), the Lorenz curve is a continuous piecewise linear function connecting the points \\((F_i, L_i)\\) for \\(i=1\\) to \\(n\\), where \\(F_0 = 0, L_0 = 0\\).\n\\[\n\\begin{align}\nF_i &:= \\sum_{j=1}^{i} f(y_j) \\\\\nS_i &:= \\sum_{j=1}^{i} y_i f(y_i) \\\\\nS &:= \\sum_{i=1}^{n} y_i f(y_i) \\\\\nL_i &:= \\frac{S_i}{S}\n\\end{align}\n\\]\n\nContinuous distribution\n\nFor a continuous distribution with the PDF \\(f\\) and the CDF \\(F\\), the Lorenz curve \\(L\\) is given by\n\\[\nL(F(x)) = \\frac{\\int_{-\\infty}^{x} t f(t) dt}{\\int_{-\\infty}^\\infty t f(t) dt} = \\frac{\\int_{-\\infty}^{x} t f(t) dt}{\\mu}\n\\]\nwhere \\(\\mu\\) denotes the average.\nThe Lorenz curve \\(L(F)\\) may then be plotted as a function parametric in \\(x\\): \\(L(x) \\text{ vs. } F(x)\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html#gini-index",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/lorenz_curve_and_gini_index/index.html#gini-index",
    "title": "Lorenz curve and Gini index",
    "section": "2 Gini index",
    "text": "2 Gini index\n\n2.1 Definition\nThe Gini coefficient is the ratio of the area between the line of perfect equality and the observed Lorenz curve to the area between the line of pefect equality and the line of perfect inequality. The higher the coefficient, the more unequal the distribution. In the diagram above, this is given by the ratio \\(\\frac{A}{A+B} = 2A = 1-2B\\) due to the fact that \\(A + B = 0.5\\).\nAssuming non-negative income or wealth for all, the Gini coefficient’s theoretical range is from \\(0\\) (total equality) to \\(1\\) (absolute inequality).\nAn alternative approach is to define the Gini coefficient as half of the relative mean absolute difference, which is equivalent to the definition based on the Lorenz curve. The mean absolute difference is the average absolute difference of all pairs of items of the population, and the relative mean absolute difference is the mean absolute difference divided by the average \\(\\bar{x}\\) to normalize for scale.\nIf \\(x_i\\) is the income or wealth of person \\(i\\), and there are \\(n\\) persons, then the Gini coefficient \\(G\\) is given by\n\\[\nG = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n} |x_i - x_j|}{2n^2\\bar{x}} = \\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n} |x_i - x_j|}{2n \\sum_{i=1}^{n} \\bar{x}}\n\\]\nWhen the wealth or income distribution is given as a continuous PDF \\(p(x)\\), the Gini coefficient is again half of the relative mean absolute difference:\n\\[\nG = \\frac{1}{2\\mu} \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} p(x)p(y) |x-y| dxdy\n\\]\nwhere \\(\\mu = \\int_{-\\infty}^{\\infty} xp(x) dx\\) is the mean of the distribution, and the lower limits of integration may be replaced by zero when all incomes are positive.\n\n\n2.2 Calculation\nIf the values are first placed in ascending order, such that each \\(x\\) has rank \\(i\\), some of the comparisons above can be avoided and the computation can be quicker:\n\\[\n\\begin{align}\nG &= \\frac{2}{n^2\\bar{x}} \\sum_{i=1}^{n} i(x_i - \\bar{x}) \\\\\nG &= \\frac{\\sum_{i=1}^{n} (2i-n-1)x_i}{n \\sum_{i=1}^{n} x_i}\n\\end{align}\n\\]\nwhere \\(x\\) is an observed value, \\(n\\) is the number of values observed and \\(i\\) is the rank of values in ascending order.\nNote that only positive non-zero values are used.\n\nJulia code\n\n\nusing Random, Distributions\n\nRandom.seed!(1234)\n\n@doc raw\"\"\"\n    gini(A::AbstractArray)\n\nCalculate the Gini coefficient of an array of numbers using the formula:\n\n``G = \\frac{\\sum_{i=1}^{n} (2i-n-1)x_i}{n \\sum_{i=1}^{n} x_i}``\n\"\"\"\nfunction gini(A::AbstractArray)\n    A = vec(A)\n\n    # values cannot be negative\n    if minimum(A) &lt; 0\n        A = A .- minimum(A)\n    end\n    # values cannot be 0\n    A = A .+ 1e-7\n\n    A = sort(A)\n    n = length(A)\n    index = collect(1:n)\n\n    sum(@. (2 * index - n - 1) * A) / (n * sum(A))\nend\n\n\nA = zeros(1000)\nA[1] = 1\nprintln(gini(A))\n\nB = ones(1000)\nprintln(gini(B))\n\nC = rand(Normal(0, 10), 1000)\nprintln(gini(C))\n\n0.998900109989001\n-1.0004440719058136e-17\n0.1756229561649242"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html",
    "title": "Binomial distribution approximations",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser. No biggie. You can click here to download the PDF file."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#introduction",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#introduction",
    "title": "Binomial distribution approximations",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser. No biggie. You can click here to download the PDF file."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#normal-approximation",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#normal-approximation",
    "title": "Binomial distribution approximations",
    "section": "2 Normal approximation",
    "text": "2 Normal approximation\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\n# the probability of success\nps = [0.1, 0.3, 0.5, 0.9]\n# the number of experiments\nns = [10, 100, 1000]\nlayouts = [[1, 1], [1, 2], [2, 1], [2, 2]]\ncolors = [:blue, :green, :red]\nnormalDist = Normal(0, 1)\nN = 10^6\n\nfig = Figure(size=(1400, 1200))\nfor i in 1:length(ps)\n    p = ps[i]\n    ax = Axis(fig[layouts[i][1], layouts[i][2]])\n    totalBins = []\n    for j in 1:length(ns)\n        n = ns[j]\n        binomialDist = Binomial(n, p)\n        samples = rand(binomialDist, N)\n        # normal standardization\n        normSamples = @. (samples - n * p) / sqrt(n * p * (1 - p))\n        bins = sort(unique(normSamples))\n        stephist!(ax, normSamples; color=colors[j], normalization=:pdf, bins=bins, label=string(\"n = \", n))\n        totalBins = vcat(totalBins, bins)\n    end\n    xGrid = round(minimum(totalBins), RoundDown; digits=0):0.01:round(maximum(totalBins), RoundUp; digits=0)\n    lines!(ax, xGrid, pdf.(normalDist, xGrid); color=:black, label=\"N(0, 1)\")\n    axislegend(ax)\n    ax.xlabel = string(\"x\\n(p = \", p, \")\")\n    ax.ylabel = \"Density\"\nend\nfig"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#poisson-approximation",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/binomial_distribution_approximatrions/index.html#poisson-approximation",
    "title": "Binomial distribution approximations",
    "section": "3 Poisson approximation",
    "text": "3 Poisson approximation\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\nlambda = 5\nns = [10, 20, 200]\nN = 10^6\ncolors = [:red, :blue, :green]\n\nfig = Figure()\nax = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"Density\")\npoissonDist = Poisson(lambda)\nsamples = rand(poissonDist, N)\nbins = sort(unique(samples))\nstephist!(ax, samples; linewidth=3, linestyle=:dot, normalization=:pdf, bins=bins, color=:black, label=\"λ = $(lambda)\")\nfor i in 1:length(ns)\n    p = lambda / ns[i]\n    binomialDist = Binomial(ns[i], p)\n    samples = rand(binomialDist, N)\n    bins = sort(unique(samples))\n    stephist!(ax, samples; normalization=:pdf, bins=bins, color=colors[i], label=\"n = $(ns[i]), p = $(p)\")\nend\naxislegend(ax)\nfig"
  },
  {
    "objectID": "Blogs/Mathematics/about.html",
    "href": "Blogs/Mathematics/about.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "Here is a collection of blogs related to mathematics, such as calculus, linear algebra, probability and statistics, etc.\nEmail: 2413667864@qq.com"
  },
  {
    "objectID": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html",
    "href": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html",
    "title": "How to run ENCODE bulk RNA-seq pipeline in NeuroBorder Galaxy",
    "section": "",
    "text": "NeuroBorder Galaxy platform is only responsible for performing analyses, not for visualizing results, which can be done by using NeuroBorder Shiny App in part. For some of results, you can visualize them via R, Python, GraphPad Prism, etc. yourself."
  },
  {
    "objectID": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html#introduction",
    "href": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html#introduction",
    "title": "How to run ENCODE bulk RNA-seq pipeline in NeuroBorder Galaxy",
    "section": "",
    "text": "NeuroBorder Galaxy platform is only responsible for performing analyses, not for visualizing results, which can be done by using NeuroBorder Shiny App in part. For some of results, you can visualize them via R, Python, GraphPad Prism, etc. yourself."
  },
  {
    "objectID": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html#steps",
    "href": "Blogs/Galaxy/posts/Galaxy/endcode_rna_seq_pipeline_in_galaxy/index.html#steps",
    "title": "How to run ENCODE bulk RNA-seq pipeline in NeuroBorder Galaxy",
    "section": "2 Steps",
    "text": "2 Steps\n\n2.1 Check MD5 sums of your raw FASTQ data in your computer\nFor Windows users, I recommend using git for windows, which provides md5sum program equivalent to the Linux one. For Linux and Mac users, md5sum program has already been built into the system.\nOnce you have installed it, you can right-click the mouse on your target directory and choose Open Git Bash here. This will open Git Bash terminal and set your target directory as the current working directory. Then you can use ls or cd &lt;target directory&gt; command to list files/directories in the current working directory or change the current working directory to your specified target directory. Change your current working directory using cd into the directory containing MD5 file such as md5.txt or MD5.txt, and then run the following code:\nmd5sum -c md5.txt &gt; md5_check.txt\nNow, once the program has been run successfully, you can view the file md5_check.txt to see whether all files are correct.\n\n\n2.2 Copy your raw FASTQ data to a new directory and rename them\nWithin your Git Bash, running the following code in the directory containing your raw FASTQ data needed to be analyzed:\nfind &lt;raw data directory&gt; -type f | grep -P \".+\\.(fastq|fq)\\.gz$\" | xargs -I {} realpath {} | xargs -I {} cp {} &lt;target directory&gt;\nIn the above code, you should replace &lt;raw data directory&gt; with your directory containing your raw FASTQ data needed to be analyzed (you can use . to denote the current working directory), and replace &lt;target directory&gt; with your new directory into which all raw FASTQ data will be copied.\nAfter this step, rename your raw FASTQ files with the following specifications (This is absolutely necessary!):\nNote: input FASTQ file names must strictly adhere to the following rules otherwise you will get unexpected outputs!\n\nSingle-end: ID_repN[_partN].(fastq|fq).gz\nPaired-end: ID_repN[_partN].R(1|2).(fastq|fq).gz\n\nAmong them:\n\nID is the sample name, which can only contain [a-zA-Z0-9] and does NOT start with [0-9].\nrepN means the Nth biological replicate.\npartN means the Nth technical replicate. All technical replicates with the same ID_repN should be merged into a single FASTQ file before running downstream analyses. This will be done by ENCODE bulk RNA-seq pipeline before performing alignment and quantification.\nN can only contain [0-9].\n\nNote: Even though you have only one biological replicate, rep1 CANNOT be omitted. If you have only one technical replicate, part1 CAN be omitted.\ne.g.\nFor paired-end samples: Brain1_rep1.fastq.R1.gz, Brain1_rep1.fastq.R2.gz, Brain1_rep2_part1.R1.fq.gz, Brain1_rep2_part2.R1.fastq.gz, Brain1_rep2_part1.R2.fq.gz, Brain1_rep2_part2.R2.fastq.gz.\nFor single-end samples: Cortex_rep1.fq.gz, Cortex_rep2_part1.fq.gz, Cortex_rep2_part2.fastq.gz.\nOnce renaming is done, generate MD5 sums again for your newly renamed raw FASTQ files by running the following code in the directory containing your newly renamed FASTQ files:\nmd5sum *.fastq.gz &gt; md5.txt\nThis will generate all MD5 sums of your newly renamed FASTQ files in file md5.txt.\n\n\n2.3 Upload your raw FASTQ files and MD5 sums file to NeuroBorder Galaxy\nAt present, the IP address of NeuroBorder Galaxy is an internal IP address http://172.16.50.209. This means that you cannot visit it outside. In addition, this IP is not permanent, which means it may be changed in the future. If you find it is invalid, please let me know. I will update it as I can.\nOnce you have typed the IP address into your browser, you can upload data and perform various analyses within NeuroBorder Galaxy.\nFor how to use some basic functionalities of NeuroBorder Galaxy, such as uploading data, managing history, etc., you can learn them from the Galaxy Training or I can give you some demos in real time if necessary.\nAt present, all tools used for ENCODE RNA-seq pipeline is under Tools \\(\\to\\) NeuroBorder Tools, in which you can choose the concrete tool for your purpose.\nOnce you have uploaded your raw FASTQ files and MD5 sums file to NeuroBorder Galaxy, they will be shown in the right History panel (You can give each of your histories a meaningful name and delete them when you don’t need them anymore. This is a good behavior for saving storage resource when available storage resource is inadequate.). Next, you should build your raw FASTQ files (note: not including the MD5 sums file) into a dataset list and give it a meaningful name.\nBy now, you can run each of the following step one by one.\n\n\n2.4 Run MD5SUM\nThis step ensures that your raw FASTQ files are not malformed during uploading.\nIn this step, you can check or generate (if MD5 file is not provided) MD5 sums of your files.\n\n\n2.5 Run FASTQC/MULTIQC over raw FASTQ files\nGenerate quality reports for your raw FASTQ files.\n\n\n2.6 Run Trim Galore\nThis step performs quality, adapter, and Ns trimmings of your sequencing reads, discard reads too short, and ensure that you only use sequencing reads with high quality for downstream analyses.\n\n\n2.7 Run FASTQC/MULTIQC over trimmed FASTQ files\nGenerate quality reports for your trimmed FASTQ files.\n\n\n2.8 Run ENCODE Bulk RNA-seq Pipeline\nThis step will run the ENCODE Bulk RNA-seq Pipeline and may take a long time from hours to days depending on your sample sizes.\nPlease make sure you have selected the right species (e.g. mm10 if your data is obtained from mouse) for your data.\nThis step will generate some extremely large files, so you should download them with care. Once all files have been downloaded and unzipped, some files of them need to be uploaded again for performing downstream analyses. These files are mainly quantification files (with suffix such as _anno_rsem.genes.results or _anno_rsem.isoforms.results) generated by RSEM.\nAs mentioned before, all uploaded files still need to be built into a dataset list with a name when feeded into the next tool.\n\n\n2.9 Run Extract RSEM Results\nThis step will extract some expression metrics and store them into some files from RSEM results.\n\n\n2.10 Run Bulk RNA-seq DE with DESeq2\nThis step will perform differential expression analysis over all comparison pairs.\n\n\n2.11 Run GO Enrichment Analysis\nThis step performs GO enrichment analysis over one or more gene sets.\n\n\n2.12 Run GSEA Analysis\nThis step performs GSEA analysis.\n\n\n2.13 Epilogue\nTo date, you have run all basic steps of bulk RNA-seq analysis. You can turn to visualize them or perform more adavnced analysis yourself."
  },
  {
    "objectID": "Blogs/Galaxy/about.html",
    "href": "Blogs/Galaxy/about.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "Here is a collection of blogs related to Galaxy guides.\nEmail: 2413667864@qq.com"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/compile_and_install_r/index.html",
    "href": "Blogs/Computer/posts/Programming/R/compile_and_install_r/index.html",
    "title": "Compile and install R",
    "section": "",
    "text": "# To install GNU softwares\n# You should first learn how to use the command to query its configuration options\n./configure --help\n\n# For detailed compiling and installing parameters\n# You should also learn to consult its README/INSTALL or official doc\ntar -xzvf R-4.3.3.tar.gz\n\ncd R-4.3.3\n\n# Here, we add some options to support the outputs of png, jpeg, tiff, etc.\n# Although some of them are enabled by default\n# To make these options enabled, be sure to install the following libraries first\nsudo apt install -y libjpeg-dev\nsudo apt install -y libpng-dev\nsudo apt install -y libtiff-dev\nsudo apt install -y tcl-dev\nsudo apt install -y tk-dev\nsudo apt install -y libx11-dev\nsudo apt install -y libxml2-dev\nsudo apt install -y libcairo2-dev\nsudo apt install -y gfortran\nsudo apt install -y libreadline-dev\nsudo apt install -y libxt-dev\nsudo apt install -y libcurl4-openssl-dev\nsudo apt install -y default-jdk\nsudo apt install -y libbz2-dev\n\n./configure --prefix=/home/yangrui/softs/R_v4.3.3 --enable-R-shlib --enable-memory-profiling --with-blas --with-lapack --with-libpng --with-jpeglib --with-libtiff --with-x --with-cairo --with-tcltk\n# Configuration on server\n# Some of essential libraries and/or headers may be unavailable in the server\n# You can specify their directories using some environmental variables\n./configure --prefix=/home/yangrui/softs/R_v4.3.3 --enable-R-shlib --enable-memory-profiling --with-blas --with-lapack --with-libpng --with-jpeglib --with-libtiff --with-x --with-cairo --with-tcltk --with-pcre2 LDFLAGS=\"-L/gpfs/home/yangrui/softs/packages/pcre2-10.42/lib\" CPPFLAGS=\"-I/gpfs/home/yangrui/softs/packages/pcre2-10.42/include\"\n\nmake\nmake install\n\n# Create symbolic links to make it callable by RStudio\nsudo ln -s /home/yangrui/softs/R_v4.3.3/bin/R /usr/bin/R\nsudo ln -s /home/yangrui/softs/R_v4.3.3/bin/Rscript /usr/bin/Rscript\n# Install dependencies for building the PDF versions of the R docs\nsudo apt install -y texlive texlive-fonts-extra\n\n# Install dependencies for building the HTML versions of the R docs\nsudo apt install -y texinfo\n\n# Prompt for installing PCRE2\ntar -xzvf pcre2-10.42.tar.gz\ncd pcre2-10.42\n./configure --prefix=/home/yangrui/softs/packages/pcre2-10.42\nmake\nmake install\n# Then, add it to PATH\nexport PATH=$PATH:/home/yangrui/softs/packages/pcre2-10.42/bin"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/compile_and_install_r/index.html#compile-and-intsall-r-4.3.3-on-ubuntu-without-root",
    "href": "Blogs/Computer/posts/Programming/R/compile_and_install_r/index.html#compile-and-intsall-r-4.3.3-on-ubuntu-without-root",
    "title": "Compile and install R",
    "section": "",
    "text": "# To install GNU softwares\n# You should first learn how to use the command to query its configuration options\n./configure --help\n\n# For detailed compiling and installing parameters\n# You should also learn to consult its README/INSTALL or official doc\ntar -xzvf R-4.3.3.tar.gz\n\ncd R-4.3.3\n\n# Here, we add some options to support the outputs of png, jpeg, tiff, etc.\n# Although some of them are enabled by default\n# To make these options enabled, be sure to install the following libraries first\nsudo apt install -y libjpeg-dev\nsudo apt install -y libpng-dev\nsudo apt install -y libtiff-dev\nsudo apt install -y tcl-dev\nsudo apt install -y tk-dev\nsudo apt install -y libx11-dev\nsudo apt install -y libxml2-dev\nsudo apt install -y libcairo2-dev\nsudo apt install -y gfortran\nsudo apt install -y libreadline-dev\nsudo apt install -y libxt-dev\nsudo apt install -y libcurl4-openssl-dev\nsudo apt install -y default-jdk\nsudo apt install -y libbz2-dev\n\n./configure --prefix=/home/yangrui/softs/R_v4.3.3 --enable-R-shlib --enable-memory-profiling --with-blas --with-lapack --with-libpng --with-jpeglib --with-libtiff --with-x --with-cairo --with-tcltk\n# Configuration on server\n# Some of essential libraries and/or headers may be unavailable in the server\n# You can specify their directories using some environmental variables\n./configure --prefix=/home/yangrui/softs/R_v4.3.3 --enable-R-shlib --enable-memory-profiling --with-blas --with-lapack --with-libpng --with-jpeglib --with-libtiff --with-x --with-cairo --with-tcltk --with-pcre2 LDFLAGS=\"-L/gpfs/home/yangrui/softs/packages/pcre2-10.42/lib\" CPPFLAGS=\"-I/gpfs/home/yangrui/softs/packages/pcre2-10.42/include\"\n\nmake\nmake install\n\n# Create symbolic links to make it callable by RStudio\nsudo ln -s /home/yangrui/softs/R_v4.3.3/bin/R /usr/bin/R\nsudo ln -s /home/yangrui/softs/R_v4.3.3/bin/Rscript /usr/bin/Rscript\n# Install dependencies for building the PDF versions of the R docs\nsudo apt install -y texlive texlive-fonts-extra\n\n# Install dependencies for building the HTML versions of the R docs\nsudo apt install -y texinfo\n\n# Prompt for installing PCRE2\ntar -xzvf pcre2-10.42.tar.gz\ncd pcre2-10.42\n./configure --prefix=/home/yangrui/softs/packages/pcre2-10.42\nmake\nmake install\n# Then, add it to PATH\nexport PATH=$PATH:/home/yangrui/softs/packages/pcre2-10.42/bin"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html",
    "title": "Julia resource list",
    "section": "",
    "text": "Julia as a second language by Erik Engheim.\nJulia language guide by Dongfeng Li at https://www.math.pku.edu.cn/teachers/lidf/docs/Julia/html/_book/index.html.\nJulia official documentation at https://docs.julialang.org/en/v1."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html#get-started",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html#get-started",
    "title": "Julia resource list",
    "section": "",
    "text": "Julia as a second language by Erik Engheim.\nJulia language guide by Dongfeng Li at https://www.math.pku.edu.cn/teachers/lidf/docs/Julia/html/_book/index.html.\nJulia official documentation at https://docs.julialang.org/en/v1."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html#julia-for-data-science",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_resource_list/index.html#julia-for-data-science",
    "title": "Julia resource list",
    "section": "2 Julia for data science",
    "text": "2 Julia for data science\n\nJulia language guide by Dongfeng Li at https://www.math.pku.edu.cn/teachers/lidf/docs/Julia/html/_book/index.html.\nJulia data science at https://juliadatascience.io."
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Windows/windows_cmds/index.html",
    "href": "Blogs/Computer/posts/OS/Windows/windows_cmds/index.html",
    "title": "Commonly used Windows commands",
    "section": "",
    "text": "Ctrl + Win + D: create\nCtrl + Win &lt;--/--&gt;: move to the left or right\nCtrl + Win + F4: delete\nWin + Tab: show all desktops and apps in the current desktop\n\n\n\nAlt + Tab: move to the last active app\nAlt Esc: looping through apps\nCtrl + Alt + Tab: show all apps and use &lt;--/--&gt; to move\nAlt + F4: close or show power menu if used in desktop level"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Windows/windows_cmds/index.html#windows-11-shortcut-keys",
    "href": "Blogs/Computer/posts/OS/Windows/windows_cmds/index.html#windows-11-shortcut-keys",
    "title": "Commonly used Windows commands",
    "section": "",
    "text": "Ctrl + Win + D: create\nCtrl + Win &lt;--/--&gt;: move to the left or right\nCtrl + Win + F4: delete\nWin + Tab: show all desktops and apps in the current desktop\n\n\n\nAlt + Tab: move to the last active app\nAlt Esc: looping through apps\nCtrl + Alt + Tab: show all apps and use &lt;--/--&gt; to move\nAlt + F4: close or show power menu if used in desktop level"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html",
    "title": "Commonly used Linux commands",
    "section": "",
    "text": "Comprehensive statistics\n\n\nbtop: CPU, GPU, Mem, Disk (I/O), Net.\nhtop: CPU, Mem.\ntop: CPU, Mem.\n\n\nGPU\n\n\nNVIDIA\n\nnvitop -m full --colorful --gpu-util-thresh 10 80 --mem-util-thresh 10 80\nnvidia-smi\n\n\n\nDisk I/O\n\n\niostat\n\n\nCPU temperatures\n\n\nFrom lm-sensors: sensors\n\n\nNetwork\n\n\nTotal statistics: nload\nStatistics per process: nethogs\n\n\nDisk usage\n\n\nTotal statistics: df -h\nFor given directories/files: du -sh"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#系统资源监控",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#系统资源监控",
    "title": "Commonly used Linux commands",
    "section": "",
    "text": "Comprehensive statistics\n\n\nbtop: CPU, GPU, Mem, Disk (I/O), Net.\nhtop: CPU, Mem.\ntop: CPU, Mem.\n\n\nGPU\n\n\nNVIDIA\n\nnvitop -m full --colorful --gpu-util-thresh 10 80 --mem-util-thresh 10 80\nnvidia-smi\n\n\n\nDisk I/O\n\n\niostat\n\n\nCPU temperatures\n\n\nFrom lm-sensors: sensors\n\n\nNetwork\n\n\nTotal statistics: nload\nStatistics per process: nethogs\n\n\nDisk usage\n\n\nTotal statistics: df -h\nFor given directories/files: du -sh"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#用户管理",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#用户管理",
    "title": "Commonly used Linux commands",
    "section": "2 用户管理",
    "text": "2 用户管理\n# create a normal user\nsudo adduser yangrui\n\n# set the password for an user\nsudo passwd yangrui\n\n# modify your own password\npasswd\n\n# add an user to the sudo group (administrator) without removing it from other groups\nsudo usermod -aG sudo yangrui\n\n# delete an user and its home directory\nsudo deluser --remove-home yangrui\n\n# view an user's info\nid yangrui\n\n# login as a given user\nsu - yangrui"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#内核相关命令",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#内核相关命令",
    "title": "Commonly used Linux commands",
    "section": "3 内核相关命令",
    "text": "3 内核相关命令\n# 查看当前正在使用的内核版本\nuname -r\n\n# 查看所有已安装的内核版本\ndpkg --list | grep linux\ndpkg --get-selections | grep linux\n\n# 固定内核版本\nsudo apt-mark hold linux-image-$(uname -r)\nsudo apt-mark hold linux-headers-$(uname -r)\nsudo apt-mark hold linux-modules-$(uname -r)\nsudo apt-mark hold linux-modules-extra-$(uname -r)\nsudo apt-mark hold linux-generic\nsudo apt-mark hold linux-image-generic\nsudo apt-mark hold linux-headers-generic\nsudo apt-mark hold linux-libc-dev:amd64\n\n# 恢复内核更新\nsudo apt-mark unhold linux-image-$(uname -r)\nsudo apt-mark unhold linux-headers-$(uname -r)\nsudo apt-mark unhold linux-modules-$(uname -r)\nsudo apt-mark unhold linux-modules-extra-$(uname -r)\nsudo apt-mark unhold linux-generic\nsudo apt-mark unhold linux-image-generic\nsudo apt-mark unhold linux-headers-generic\nsudo apt-mark unhold linux-libc-dev:amd64"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件权限与归属",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件权限与归属",
    "title": "Commonly used Linux commands",
    "section": "4 文件权限与归属",
    "text": "4 文件权限与归属\n\n4.1 基本文件权限\n\n基本文件权限的字符及其数字表示\n\n\n\n\n\n\n\n\nBasic file permisions\n\n\n\nr、w 和 x 权限对于文件和目录的区别\n\n\n\n\n\n\n\n\nMeanings of basic file permissions for files and directories\n\n\n\n\n4.2 特殊文件权限\n\nSUID\n\nSUID 是一种对二进制程序进行设置的权限，能够让二进制程序的执行者临时拥有所有者的权限，执行者应首先拥有二进制程序的执行权限。如果一个文件被赋予了 SUID 权限且其所有者拥有执行权限，则其所有者的 x 权限位变为 s，否则变为 S。\n设置/取消 SUID 权限：u+s/u-s。\n\nSGID\n\n\n对二进制程序进行设置时，能够让执行者临时拥有文件所属组的权限，执行者同样应首先拥有二进制程序的执行权限。\n对目录进行设置时，则是让该目录内新创建的文件自动继承该目录的所属组。\n\n如果一个文件或目录被赋予了 SGID 权限且其所属组拥有执行权限，则其所属组的 x 权限位变为 s，否则变为 S。\n设置/取消 SGID 权限：g+s/g-s。\n\nSBIT\n\n当某个目录被设置了 SBIT 权限后，该目录下的文件只可被其所有者删除。\n如果一个目录被赋予了 SBIT 权限且其他用户拥有执行权限，则其他用户的 x 权限位变为 t，否则变为 T。\n设置/取消 SBIT 权限：o+t/o-t。\n对于数字表示法：SUID/SGID/SBIT 分别对应 4/2/1。\n完整的数字表示法是：特殊权限 + 一般权限。例如：7777 是最大权限，其第一位代表特殊权限位。\n\n\n4.3 修改文件权限\n# 对目录进行递归操作加 -R\n# 数字表示法\nchmod 755 test\n\n# 字符表示法\nchmod o+x test\n\n\n4.4 修改文件所属\n# 对目录进行递归操作加 -R\n# 设置文件的所有者和所属组\nchown owner:group test"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件的隐藏属性",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件的隐藏属性",
    "title": "Commonly used Linux commands",
    "section": "5 文件的隐藏属性",
    "text": "5 文件的隐藏属性\n可以使用 chattr 和 lsattr 来设置或查看文件的隐藏属性。"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件访问控制列表acl",
    "href": "Blogs/Computer/posts/OS/Linux/linux_cmds/index.html#文件访问控制列表acl",
    "title": "Commonly used Linux commands",
    "section": "6 文件访问控制列表（ACL）",
    "text": "6 文件访问控制列表（ACL）\n利用 ACL 可以针对特定的用户或用户组设置文件的权限。\n可以利用 setfacl 和 getfacl 来设置或查看文件的 ACL 权限。\n如果一个文件设置了 ACL 权限，则其权限位最后一个 . 就会变成 +。\nsetfacl -mR u:yangrui:rwx /root\n\ngetfacl /root"
  },
  {
    "objectID": "Blogs/Computer/posts/Misc/sticky_notes/index.html",
    "href": "Blogs/Computer/posts/Misc/sticky_notes/index.html",
    "title": "Sticky notes",
    "section": "",
    "text": "Note that the following IP addresses all are internal IP addresses, which means that they can only be accessible within the internal network!"
  },
  {
    "objectID": "Blogs/Computer/posts/Misc/sticky_notes/index.html#introduction",
    "href": "Blogs/Computer/posts/Misc/sticky_notes/index.html#introduction",
    "title": "Sticky notes",
    "section": "",
    "text": "Note that the following IP addresses all are internal IP addresses, which means that they can only be accessible within the internal network!"
  },
  {
    "objectID": "Blogs/Computer/posts/Misc/sticky_notes/index.html#rstudio-server-address",
    "href": "Blogs/Computer/posts/Misc/sticky_notes/index.html#rstudio-server-address",
    "title": "Sticky notes",
    "section": "2 Rstudio server address",
    "text": "2 Rstudio server address\nhttp://172.16.50.95:8787."
  },
  {
    "objectID": "Blogs/Computer/posts/Misc/sticky_notes/index.html#notes-for-deploying-galaxy-with-ansible",
    "href": "Blogs/Computer/posts/Misc/sticky_notes/index.html#notes-for-deploying-galaxy-with-ansible",
    "title": "Sticky notes",
    "section": "3 Notes for deploying Galaxy with Ansible",
    "text": "3 Notes for deploying Galaxy with Ansible\nGalaxy server address: http://172.16.50.95.\nReferences:\n\nGalaxy Installation with Ansible.\nAnsible.\n\nCautions:\n\nBefore deploying Galaxy with Ansible in your server, make sure 2to3 is installed in your server. In Ubuntu, you can run sudo apt install -y 2to3.\nGenerally, after Galaxy has been deployed, it will be managed as a set of systemd services. But due to some unknown reason, Galaxy’s systemd services are not added automatically, you can add it by running sudo galaxyctl update yourself, and then reboot if needed.\nIn production environment, Galaxy uses NGINX as its reverse proxy server, which is also managed by the systemd service. However, sometimes, if the NGINX server is started before the Galaxy server, the NGINX server will fail to start. In this case, you can try to set Restart=always and RestartSec=120 in your NGINX server, this can make sure that the NGINX service will always try to restart after 120 seconds when the previous NGINX service failed to start. In principle, once the Galaxy service has been started, the NGINX can restart successfully after a few times of restarting attempts. Of course, you can restart the NGINX service yourself by running sudo systemctl restart nginx."
  },
  {
    "objectID": "Blogs/Computer/about.html",
    "href": "Blogs/Computer/about.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "Here is a collection of blogs related to computer, such as Linux, Git, R, Julia, etc.\nEmail: 2413667864@qq.com"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html",
    "title": "Identify housekeeping genes",
    "section": "",
    "text": "In brief, housekeeping genes are those with higher expression levels, low variances, and ubiquitous expression profiles across samples and species.\nIn paper “What are housekeeping genes” by Chintan J. Joshi, housekeeping genes are defined as those with the following four properties:\n\nHigher expression stability\nCellular maintenance\nEssentiality\nConservation"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#methods-overview",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#methods-overview",
    "title": "Identify housekeeping genes",
    "section": "",
    "text": "In brief, housekeeping genes are those with higher expression levels, low variances, and ubiquitous expression profiles across samples and species.\nIn paper “What are housekeeping genes” by Chintan J. Joshi, housekeeping genes are defined as those with the following four properties:\n\nHigher expression stability\nCellular maintenance\nEssentiality\nConservation"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-1",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-1",
    "title": "Identify housekeeping genes",
    "section": "2 Method 1",
    "text": "2 Method 1\nFrom “A Comprehensive Mouse Transcriptomic BodyMap across 17 Tissues by RNA-seq” by Bin Li.\nCriteria for identification of housekeeping genes:\n\nHighly expressed in all biological samples (\\(FPKM &gt; 1\\));\nLow variance across tissues: std(log2(FPKM)) &lt; 1;\nNo logarithmic expression value differed from the averaged log2(FPKM) value by a factor of two (i.e. fourfold) or more.\n\nCriteria for identification of reference genes:\n\n\\(FPKM &gt; 50\\) across all biological samples;\nstd(log2(FPKM)) &lt; 0.5 over tissues;\nNo logarithmic expression value differed from the averaged log2(FPKM) value by a factor of one (i.e. twofold) or more."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-2",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-2",
    "title": "Identify housekeeping genes",
    "section": "3 Method 2",
    "text": "3 Method 2\nFrom “Housekeeping protein‑coding genes interrogated with tissue and individual variations” by Kuo‑FengTung.\nGini coefficient of inequality (Gini index):\n\n\\(TPM &gt; 0.05\\);\n\\(\\text{Gini index} &lt; 0.2\\)."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-3",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#method-3",
    "title": "Identify housekeeping genes",
    "section": "4 Method 3",
    "text": "4 Method 3\nFrom “The evolution of gene expression levels in mammalian organs” by David Brawand.\nPipeline used to pick housekeeping genes and normalize expression levels across species:\n\nConvert TPM to \\(log2(TPM+1)\\);\nRetrieve and only keep one-to-one orthologous genes across all species with confidence equal to \\(1\\) from Ensembl BioMart;\nSort orthologs based on TPMs in descending order and represent each gene by its TPM rank in each sample;\nCalculate the standard deviation and median of each ortholog based on its ranks across samples;\nKeep orthologs the medians of which are within \\(0.25 \\times \\text{the number of orthologs} \\sim 0.75 \\times \\text{the number of orthologs}\\) (discarding those orthologs with expression levels extremely high or extremely low across samples);\nRetain the \\(1000\\) orthologs with the lower variances (standard deviations);\nCalculate the medians of those \\(1000\\) orthologs’ TPMs in each sample;\nCalculate the scaling factor of each sample: the scaling factor of sample A = TPM median of sample A / median(TPM median of all samples);\nFor each sample, scaled TPM = TPM / scaling factor.\n\nNote: be aware of the fact that the expression difference of each same/homologous gene among species and the difference among batches are different."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#reference-datasets",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/identify_housekeeping_genes/index.html#reference-datasets",
    "title": "Identify housekeeping genes",
    "section": "5 Reference datasets",
    "text": "5 Reference datasets\nHuman housekeeping genes: https://www.gsea-msigdb.org/gsea/msigdb/cards/HOUNKPE_HOUSEKEEPING_GENES.html.\nMouse housekeeping genes: https://www.gsea-msigdb.org/gsea/msigdb/mouse/geneset/HOUNKPE_HOUSEKEEPING_GENES.html."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html",
    "title": "Bulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva",
    "section": "",
    "text": "Perform batch correction with DESeq2, limma, or sva.\n\nwork_dir &lt;- \"/home/yangrui/mywd/wtt_proj\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#introduction",
    "title": "Bulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva",
    "section": "",
    "text": "Perform batch correction with DESeq2, limma, or sva.\n\nwork_dir &lt;- \"/home/yangrui/mywd/wtt_proj\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#differential-expression-analysis-with-batch-correction-using-deseq2",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#differential-expression-analysis-with-batch-correction-using-deseq2",
    "title": "Bulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva",
    "section": "2 Differential expression analysis with batch correction using DESeq2",
    "text": "2 Differential expression analysis with batch correction using DESeq2\n\nlibrary(tximport)\nlibrary(DESeq2)\nlibrary(tidyverse)\nlibrary(vroom)\n\ninput_dir &lt;- \"transcriptome/rsem_gene_results\"\noutput_dir &lt;- \"transcriptome/merged_degs\"\nsample_file &lt;- \"transcriptome/rsem_gene_results/genes.sample_sheet.merged.tsv\"\nmpt_file &lt;- \"/data/biodatabase/species/mRatBN7/genome/anno/Rattus_norvegicus.mRatBN7.2.111.gff3.gz.gene_id_name_mapping_table.tsv\"\ntpm_file &lt;- \"transcriptome/extracted_rsem_gene_results/gene.TPM.tsv.tsv\"\npadj_th &lt;- 0.05\nlogfc_th &lt;- 1\ntpm_th &lt;- 1\n\ndir.create(output_dir, recursive = FALSE)\n\nfile_df &lt;- tibble(\n    file = list.files(input_dir, pattern = \"_rsem\\\\.genes\\\\.results$\", full.names = TRUE, recursive = FALSE),\n    basename = basename(file)\n)\nsample_df &lt;- vroom(sample_file) %&gt;%\n    inner_join(file_df, by = \"basename\") %&gt;%\n    mutate(\n        condition = factor(condition),\n        batch = factor(batch)\n    )\ntx.rsem &lt;- tximport(\n    setNames(sample_df[[\"file\"]], sample_df[[\"sample\"]]),\n    type = \"rsem\", txIn = FALSE, txOut = FALSE\n)\n\n# filter out genes with lengths &lt;= 0\nnon_zero_length &lt;- apply(tx.rsem$length, 1, function(x) {\n    all(x &gt; 0)\n})\n\ntx.rsem$abundance &lt;- tx.rsem$abundance[non_zero_length, ]\ntx.rsem$counts &lt;- tx.rsem$counts[non_zero_length, ]\ntx.rsem$length &lt;- tx.rsem$length[non_zero_length, ]\n\n# create DESeqDatatSet\nddsTxi &lt;- DESeqDataSetFromTximport(\n    txi = tx.rsem,\n    colData = sample_df,\n    design = ~ batch + condition\n)\n\n# keep rows that have at least N reads in total (this is just a simple filtering process and is optional)\nkeep &lt;- rowSums(counts(ddsTxi)) &gt;= 10\nddsTxi &lt;- ddsTxi[keep, ]\n\ncondition_levels &lt;- levels(sample_df[[\"condition\"]])\nde_ls &lt;- list(raw = list(), na_filter = list(), unique_symbols = list(), only_degs = list())\nfor (ref in condition_levels) {\n    compara_levels &lt;- c(ref, condition_levels[-which(condition_levels == ref)])\n\n    message(paste0(\"\\n\\nLevels: \", paste0(compara_levels, collapse = \", \")))\n    message(\"Baseline: \", ref, \"\\n\")\n\n    # set the reference sample\n    ddsTxi$condition &lt;- factor(ddsTxi$condition, levels = compara_levels)\n    # DE analysis\n    ddsTxi &lt;- DESeq(ddsTxi)\n\n    # get results table\n    res_tabs &lt;- resultsNames(ddsTxi)\n    res_tabs &lt;- res_tabs[str_detect(res_tabs, \"^condition_\")]\n    message(\"\\n\", paste0(res_tabs, collapse = \", \"))\n\n    # save pairwise comparisons\n    for (pair in res_tabs) {\n        res &lt;- results(ddsTxi, name = pair)\n        res &lt;- as.data.frame(res)\n        res[[\"gene_id\"]] &lt;- row.names(res)\n        res[[\"pvalue\"]][res[[\"pvalue\"]] == 0] &lt;- .Machine[[\"double.xmin\"]]\n        res[[\"padj\"]][res[[\"padj\"]] == 0] &lt;- .Machine[[\"double.xmin\"]]\n        de_ls[[\"raw\"]][[gsub(\"^condition_\", \"\", pair)]] &lt;- as_tibble(res)\n    }\n}\n\n# attach gene info\nmpt &lt;- vroom(mpt_file)\nfor (pair in names(de_ls[[\"raw\"]])) {\n    de_ls[[\"raw\"]][[pair]] &lt;- inner_join(mpt, de_ls[[\"raw\"]][[pair]],\n        by = \"gene_id\"\n    )\n}\n\n# filter rows containing NAs\nfor (pair in names(de_ls[[\"raw\"]])) {\n    pairs &lt;- str_split(pair, fixed(\"_vs_\"))[[1]]\n    de_ls[[\"na_filter\"]][[pair]] &lt;- de_ls[[\"raw\"]][[pair]] %&gt;%\n        filter(!(is.na(baseMean) | is.na(log2FoldChange) | is.na(pvalue) | is.na(padj))) %&gt;%\n        mutate(diff_flag = if_else(padj &lt; padj_th,\n            if_else(abs(log2FoldChange) &gt; logfc_th,\n                if_else(log2FoldChange &gt; logfc_th,\n                    paste0(pairs[1], \" Up\"),\n                    paste0(pairs[2], \" Up\")\n                ),\n                \"NO\"\n            ),\n            \"NO\"\n        ))\n}\n\n# filter duplicated gene symbols\ntpm &lt;- vroom(tpm_file)\nfor (pair in names(de_ls[[\"na_filter\"]])) {\n    # filtered by TPMs\n    pairs &lt;- str_split(pair, fixed(\"_vs_\"))[[1]]\n    flag &lt;- rep(FALSE, nrow(tpm))\n    for (s in pairs) {\n        tmp_df &lt;- tpm[, filter(sample_df, condition == s) %&gt;% pull(sample) %&gt;% unique()]\n        if (ncol(tmp_df) == 0) {\n            stop(\"sample columns in TPM are not matched with those used to perform DE analysis\")\n        }\n        flag &lt;- flag | (rowSums(tmp_df &gt; tpm_th) == ncol(tmp_df))\n    }\n    tmp_df &lt;- de_ls[[\"na_filter\"]][[pair]] %&gt;%\n        filter(gene_id %in% tpm[[\"gene_id\"]][flag]) %&gt;%\n        mutate(gene_name = if_else(is.na(gene_name), gene_id, gene_name))\n\n    # filtered by gene versions\n    if (\"gene_version\" %in% names(tmp_df)) {\n        tmp_df &lt;- tmp_df %&gt;%\n            group_by(gene_name) %&gt;%\n            slice_max(gene_version) %&gt;%\n            ungroup()\n    }\n\n    # filtered by baseMean, padj, log2FoldChange\n    # if still duplicated, sample one randomly\n    de_ls[[\"unique_symbols\"]][[pair]] &lt;- tmp_df %&gt;%\n        group_by(gene_name) %&gt;%\n        slice_max(baseMean) %&gt;%\n        slice_min(padj) %&gt;%\n        slice_max(log2FoldChange) %&gt;%\n        slice_sample(n = 1) %&gt;%\n        ungroup()\n}\n\n# keep only DEGs\nfor (pair in names(de_ls[[\"unique_symbols\"]])) {\n    de_ls[[\"only_degs\"]][[pair]] &lt;- de_ls[[\"unique_symbols\"]][[pair]] %&gt;%\n        filter(diff_flag != \"NO\")\n}\n\n# save results to files\nfor (category in names(de_ls)) {\n    dir.create(file.path(output_dir, category), recursive = FALSE)\n    for (pair in names(de_ls[[category]])) {\n        vroom_write(de_ls[[category]][[pair]], file = file.path(file.path(output_dir, category), paste0(pair, \".tsv\")))\n    }\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#batch-correction-using-limma",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#batch-correction-using-limma",
    "title": "Bulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva",
    "section": "3 Batch correction using limma",
    "text": "3 Batch correction using limma\n\nlibrary(limma)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggrepel)\nlibrary(ggprism)\nlibrary(YRUtils)\n\ntpm_file &lt;- \"transcriptome/extracted_rsem_gene_results/gene.TPM.tsv.tsv\"\nsample_file &lt;- \"transcriptome/rsem_gene_results/genes.sample_sheet.merged.tsv\"\noutput_dir &lt;- \"transcriptome/limma_batch_corrected\"\ntpm_th &lt;- 1\n\ndir.create(output_dir, recursive = FALSE)\n\nsample_df &lt;- vroom(sample_file) %&gt;%\n    mutate(\n        condition = factor(condition),\n        batch = factor(batch)\n    )\ntpm &lt;- vroom(tpm_file) %&gt;%\n    select(all_of(c(\"gene_id\", sample_df[[\"sample\"]]))) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"gene_id\"]]) %&gt;%\n    select(-gene_id)\nflag &lt;- rep(FALSE, nrow(tpm))\nfor (g in unique(sample_df[[\"group\"]])) {\n    tmp_df &lt;- tpm[, filter(sample_df, group == g) %&gt;% pull(sample) %&gt;% unique()]\n    if (ncol(tmp_df) == 0) {\n        stop(\"sample columns in TPM are not matched with those used to perform DE analysis\")\n    }\n    flag &lt;- flag | (rowSums(tmp_df &gt; tpm_th) == ncol(tmp_df))\n}\ntpm &lt;- tpm[flag, ]\ntpm &lt;- log(tpm + 1)\n\ntpm_corrected &lt;- removeBatchEffect(tpm, batch = sample_df[[\"batch\"]], group = sample_df[[\"condition\"]])\n\nvroom_write(\n    tpm %&gt;% mutate(gene_id = row.names(.)) %&gt;% as_tibble(),\n    file = file.path(output_dir, \"tpm.th1_log1p.tsv\")\n)\nvroom_write(\n    tpm_corrected %&gt;% as.data.frame() %&gt;% mutate(gene_id = row.names(.)) %&gt;% as_tibble(),\n    file = file.path(output_dir, \"tpm.th1_log1p.batch_corrected.tsv\")\n)\n\n# uncorrected\npca &lt;- PCA(t(tpm), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord &lt;- as.data.frame(pca$ind$coord)\npca_coord$sample &lt;- row.names(pca_coord)\npca_coord &lt;- inner_join(pca_coord, sample_df, by = \"sample\")\npca_eig &lt;- as.data.frame(pca$eig)\n\np &lt;- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 2) +\n    geom_text_repel(aes(label = sample),\n        size = 5, max.overlaps = 10000,\n        min.segment.length = 3,\n        family = \"Arial\"\n    ) +\n    xlab(paste0(\n        \"PC1 (\",\n        round(pca_eig[\"comp 1\", \"percentage of variance\"]),\n        \"%)\"\n    )) +\n    ylab(paste0(\n        \"PC2 (\",\n        round(pca_eig[\"comp 2\", \"percentage of variance\"]),\n        \"%)\"\n    ))\np &lt;- p + geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25)\np &lt;- p + theme_prism(border = TRUE, base_family = \"Arial\", base_size = 16) +\n    theme(legend.title = element_text())\nppreview(p, file = file.path(output_dir, \"pca.uncorrected.pdf\"))\n\n# batch-corrected\npca_corrected &lt;- PCA(t(tpm_corrected), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord_corrected &lt;- as.data.frame(pca_corrected$ind$coord)\npca_coord_corrected$sample &lt;- row.names(pca_coord_corrected)\npca_coord_corrected &lt;- inner_join(pca_coord_corrected, sample_df, by = \"sample\")\npca_eig_corrected &lt;- as.data.frame(pca_corrected$eig)\n\np &lt;- ggplot(pca_coord_corrected, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 2) +\n    geom_text_repel(aes(label = sample),\n        size = 5, max.overlaps = 10000,\n        min.segment.length = 3,\n        family = \"Arial\"\n    ) +\n    xlab(paste0(\n        \"PC1 (\",\n        round(pca_eig_corrected[\"comp 1\", \"percentage of variance\"]),\n        \"%)\"\n    )) +\n    ylab(paste0(\n        \"PC2 (\",\n        round(pca_eig_corrected[\"comp 2\", \"percentage of variance\"]),\n        \"%)\"\n    ))\np &lt;- p + geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25)\np &lt;- p + theme_prism(border = TRUE, base_family = \"Arial\", base_size = 16) +\n    theme(legend.title = element_text())\nppreview(p, file = file.path(output_dir, \"pca.batch_corrected.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#batch-correction-using-sva",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_de_analysis_and_batch_correction_with_deseq2_limma_and_sva/index.html#batch-correction-using-sva",
    "title": "Bulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva",
    "section": "4 Batch correction using sva",
    "text": "4 Batch correction using sva\n\nlibrary(sva)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggrepel)\nlibrary(ggprism)\nlibrary(YRUtils)\n\ntpm_file &lt;- \"transcriptome/extracted_rsem_gene_results/gene.TPM.tsv.tsv\"\nsample_file &lt;- \"transcriptome/rsem_gene_results/genes.sample_sheet.merged.tsv\"\noutput_dir &lt;- \"transcriptome/sva_batch_corrected\"\ntpm_th &lt;- 1\n\ndir.create(output_dir, recursive = FALSE)\n\nsample_df &lt;- vroom(sample_file) %&gt;%\n    mutate(\n        condition = factor(condition),\n        batch = factor(batch)\n    )\ntpm &lt;- vroom(tpm_file) %&gt;%\n    select(all_of(c(\"gene_id\", sample_df[[\"sample\"]]))) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"gene_id\"]]) %&gt;%\n    select(-gene_id)\nflag &lt;- rep(FALSE, nrow(tpm))\nfor (g in unique(sample_df[[\"group\"]])) {\n    tmp_df &lt;- tpm[, filter(sample_df, group == g) %&gt;% pull(sample) %&gt;% unique()]\n    if (ncol(tmp_df) == 0) {\n        stop(\"sample columns in TPM are not matched with those used to perform DE analysis\")\n    }\n    flag &lt;- flag | (rowSums(tmp_df &gt; tpm_th) == ncol(tmp_df))\n}\ntpm &lt;- tpm[flag, ]\ntpm &lt;- log(tpm + 1)\n\nbatch &lt;- sample_df$batch\nmod &lt;- model.matrix(~condition, data = sample_df)\ntpm_corrected &lt;- ComBat(tpm, batch = batch, mod = mod)\n\nvroom_write(\n    tpm %&gt;% mutate(gene_id = row.names(.)) %&gt;% as_tibble(),\n    file = file.path(output_dir, \"tpm.th1_log1p.tsv\")\n)\nvroom_write(\n    tpm_corrected %&gt;% as.data.frame() %&gt;% mutate(gene_id = row.names(.)) %&gt;% as_tibble(),\n    file = file.path(output_dir, \"tpm.th1_log1p.batch_corrected.tsv\")\n)\n\n# uncorrected\npca &lt;- PCA(t(tpm), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord &lt;- as.data.frame(pca$ind$coord)\npca_coord$sample &lt;- row.names(pca_coord)\npca_coord &lt;- inner_join(pca_coord, sample_df, by = \"sample\")\npca_eig &lt;- as.data.frame(pca$eig)\n\np &lt;- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 2) +\n    geom_text_repel(aes(label = sample),\n        size = 5, max.overlaps = 10000,\n        min.segment.length = 3,\n        family = \"Arial\"\n    ) +\n    xlab(paste0(\n        \"PC1 (\",\n        round(pca_eig[\"comp 1\", \"percentage of variance\"]),\n        \"%)\"\n    )) +\n    ylab(paste0(\n        \"PC2 (\",\n        round(pca_eig[\"comp 2\", \"percentage of variance\"]),\n        \"%)\"\n    ))\np &lt;- p + geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25)\np &lt;- p + theme_prism(border = TRUE, base_family = \"Arial\", base_size = 16) +\n    theme(legend.title = element_text())\nppreview(p, file = file.path(output_dir, \"pca.uncorrected.pdf\"))\n\n# batch-corrected\npca_corrected &lt;- PCA(t(tpm_corrected), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord_corrected &lt;- as.data.frame(pca_corrected$ind$coord)\npca_coord_corrected$sample &lt;- row.names(pca_coord_corrected)\npca_coord_corrected &lt;- inner_join(pca_coord_corrected, sample_df, by = \"sample\")\npca_eig_corrected &lt;- as.data.frame(pca_corrected$eig)\n\np &lt;- ggplot(pca_coord_corrected, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 2) +\n    geom_text_repel(aes(label = sample),\n        size = 5, max.overlaps = 10000,\n        min.segment.length = 3,\n        family = \"Arial\"\n    ) +\n    xlab(paste0(\n        \"PC1 (\",\n        round(pca_eig_corrected[\"comp 1\", \"percentage of variance\"]),\n        \"%)\"\n    )) +\n    ylab(paste0(\n        \"PC2 (\",\n        round(pca_eig_corrected[\"comp 2\", \"percentage of variance\"]),\n        \"%)\"\n    ))\np &lt;- p + geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25)\np &lt;- p + theme_prism(border = TRUE, base_family = \"Arial\", base_size = 16) +\n    theme(legend.title = element_text())\nppreview(p, file = file.path(output_dir, \"pca.batch_corrected.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "",
    "text": "A little example for how to use limma to analyze Agilent mRNA microarray data."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#introduction",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "",
    "text": "A little example for how to use limma to analyze Agilent mRNA microarray data."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#download-ferret-mustela-putorius-furo-9669-reference-files",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#download-ferret-mustela-putorius-furo-9669-reference-files",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "2 Download ferret (Mustela putorius furo, 9669) reference files",
    "text": "2 Download ferret (Mustela putorius furo, 9669) reference files\n\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.fna.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gff.gz\nwget -c https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/011/764/305/GCF_011764305.1_ASM1176430v1.1/GCF_011764305.1_ASM1176430v1.1_genomic.gtf.gz"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#extract-transcript-sequences",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#extract-transcript-sequences",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "3 Extract transcript sequences",
    "text": "3 Extract transcript sequences\n\n# -w: write a fasta file with spliced exons for each transcript\ngffread -w GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -g GCF_011764305.1_ASM1176430v1.1_genomic.fna GCF_011764305.1_ASM1176430v1.1_genomic.gff"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#build-blast-database",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#build-blast-database",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "4 Build blast database",
    "text": "4 Build blast database\n\nmakeblastdb -in GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -parse_seqids -taxid 9669 -blastdb_version 5 -title \"GCF_011764305.1_ASM1176430v1.1 Ferret (Mustela putorius furo) spliced transcripts\" -dbtype nucl"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#prepare-mrna-microarray-data",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#prepare-mrna-microarray-data",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "5 Prepare mRNA microarray data",
    "text": "5 Prepare mRNA microarray data\n\nlibrary(GEOquery)\nlibrary(limma)\nlibrary(tidyverse)\nlibrary(vroom)\n\ngse_accession &lt;- \"GSE60687\"\nout_dir &lt;- \"./data/microarray\"\n\ngse &lt;- getGEO(gse_accession, GSEMatrix = T, getGPL = T)\nsample_df &lt;- gse[[paste0(gse_accession, \"_series_matrix.txt.gz\")]]@phenoData@data %&gt;%\n    mutate(\n        sample_id = paste0(\n            str_extract_all(description, \"A(17|19)\", simplify = T), \".\",\n            str_extract_all(description, \"[A-Z]{2,}\", simplify = T), \".\",\n            str_extract_all(description, \"^\\\\d\", simplify = T),\n            str_extract_all(description, \"_2\", simplify = T)\n        ),\n        sample_file = file.path(out_dir, paste0(sample_id, \".txt.gz\")),\n        wget_cmd = paste0(\"wget -c -O \", sample_file, \" \", supplementary_file)\n    ) %&gt;%\n    arrange(sample_id)\n\nsapply(sample_df$wget_cmd, function(x) {\n    system(x)\n})\n\nvroom_write(sample_df, file = file.path(out_dir, \"samples.tsv\"))\n\ngpl &lt;- getGEO(gse[[paste0(gse_accession, \"_series_matrix.txt.gz\")]]@annotation)\ngpl_fna &lt;- filter(gpl@dataTable@table, SPOT_ID != \"CONTROL\") %&gt;%\n    mutate(fna = paste0(\"&gt;\", ID, \":\", COL, \":\", ROW, \":\", NAME, \":\", CONTROL_TYPE, \":\", ACCESSION_STRING, \":\", CHROMOSOMAL_LOCATION, \"\\n\", SEQUENCE)) %&gt;%\n    pull(fna) %&gt;%\n    unique()\n\nvroom_write(gpl@dataTable@table, file = file.path(out_dir, \"gpl.tsv\"))\nvroom_write_lines(gpl_fna, file = file.path(out_dir, \"gpl.fna\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#run-blastn",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#run-blastn",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "6 Run blastn",
    "text": "6 Run blastn\n\n# GPL probe sequences against transcripts\nblastn -task megablast -db ../genome/blastdb/GCF_011764305.1_ASM1176430v1.1_genomic.transcripts.fna -query gpl.fna -outfmt \"6 qseqid sseqid evalue bitscore pident qcovs stitle\" -dust no -max_target_seqs 1 -num_threads 16 -out gpl.blastn.txt"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#attach-gene-symbols-to-gpl-probes",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#attach-gene-symbols-to-gpl-probes",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "7 Attach gene symbols to GPL probes",
    "text": "7 Attach gene symbols to GPL probes\n\nlibrary(rtracklayer)\nlibrary(vroom)\nlibrary(tidyverse)\n\ngff_file &lt;- \"./data/genome/GCF_011764305.1_ASM1176430v1.1_genomic.gff\"\ngpl_blastn_file &lt;- \"./data/microarray/gpl.blastn.txt\"\nout_dir &lt;- \"./data/microarray\"\n\ngff &lt;- as.data.frame(import(gff_file, version = \"3\")) %&gt;%\n    select(all_of(c(\n        \"seqnames\", \"start\", \"end\", \"width\", \"strand\",\n        \"source\", \"type\", \"ID\", \"Dbxref\", \"Name\", \"gbkey\",\n        \"gene\", \"gene_biotype\", \"Parent\", \"transcript_id\"\n    ))) %&gt;%\n    distinct()\n\ngpl_blastn &lt;- vroom(gpl_blastn_file, col_names = c(\"qseqid\", \"sseqid\", \"evalue\", \"bitscore\", \"pident\", \"qcovs\", \"stitle\")) %&gt;%\n    distinct() %&gt;%\n    group_by(qseqid) %&gt;%\n    slice_min(evalue) %&gt;%\n    slice_max(bitscore) %&gt;%\n    slice_max(pident) %&gt;%\n    slice_max(qcovs) %&gt;%\n    ungroup()\n\ntable(duplicated(gpl_blastn$qseqid))\n\ndf &lt;- left_join(gpl_blastn, gff, by = c(\"sseqid\" = \"ID\"))\ndf &lt;- left_join(df, filter(gff, type == \"gene\") %&gt;%\n    select(all_of(c(\"seqnames\", \"gene\", \"gene_biotype\"))) %&gt;%\n    distinct(),\nby = join_by(seqnames, gene),\nsuffix = c(\".GPL_blastn\", \".Parent_gene\")\n)\n\nDbxref_ls &lt;- lapply(df$Dbxref, function(x) {\n    if (length(x) &gt; 0) {\n        y &lt;- strsplit(x, split = \":\", fixed = T) %&gt;%\n            do.call(rbind, .)\n        setNames(y[, 2], y[, 1]) %&gt;%\n            as.list() %&gt;%\n            as.data.frame()\n    } else {\n        data.frame()\n    }\n})\nDbxref_names &lt;- lapply(Dbxref_ls, names) %&gt;%\n    unlist() %&gt;%\n    unique()\nDbxref_df &lt;- lapply(Dbxref_ls, function(x) {\n    if (nrow(x) == 0) {\n        setNames(\n            rep(NA, length(Dbxref_names)),\n            Dbxref_names\n        ) %&gt;%\n            as.list() %&gt;%\n            as.data.frame()\n    } else {\n        x\n    }\n}) %&gt;% do.call(bind_rows, .)\n\nnames(Dbxref_df) &lt;- paste0(\"Dbxref_\", names(Dbxref_df))\n\ndf &lt;- bind_cols(df, Dbxref_df)\n\nvroom_write(df, file = file.path(out_dir, \"gpl.with_gene_symbols.tsv\"), delim = \"\\t\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#differential-analysis-using-limma",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#differential-analysis-using-limma",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "8 Differential analysis using limma",
    "text": "8 Differential analysis using limma\n\nlibrary(limma)\nlibrary(vroom)\nlibrary(tidyverse)\n\nsample_file &lt;- \"./data/microarray/samples.tsv\"\ngpl_file &lt;- \"./data/microarray/gpl.with_gene_symbols.tsv\"\nout_dir &lt;- \"./data/degs\"\n\ndir.create(out_dir)\n\ngpl &lt;- vroom(gpl_file, delim = \"\\t\")\ngpl$ProbeName &lt;- sapply(gpl$qseqid, function(x) {\n    strsplit(x, \":\")[[1]][4]\n})\ngpl &lt;- gpl %&gt;%\n    select(all_of(c(\n        \"ProbeName\", \"Dbxref_GeneID\", \"gene\",\n        \"gene_biotype.GPL_blastn\", \"gene_biotype.Parent_gene\"\n    ))) %&gt;%\n    mutate(GeneBioType = if_else(is.na(gene_biotype.Parent_gene),\n        gene_biotype.GPL_blastn,\n        gene_biotype.Parent_gene\n    )) %&gt;%\n    select(all_of(c(\"ProbeName\", \"Dbxref_GeneID\", \"gene\", \"GeneBioType\"))) %&gt;%\n    distinct()\nnames(gpl) &lt;- c(\"ProbeName\", \"EntrezID\", \"Symbol\", \"GeneBioType\")\n\ntable(duplicated(gpl$ProbeName))\n\nsample_df &lt;- vroom(sample_file)\n\n# read in data\n# here, we are reading in single-channel Agilent (foreground: median signal; background: median signal) intensity data\n# so source = \"agilent\" and green.only = T\n# here, we read in the extra column gIsWellAboveBG, which records whether the intensity of each spot is considered above the background level for that array\nx &lt;- read.maimages(\n    gsub(\n        \"~/mywd/agilent_mrna_expression_microarray_analysis_using_limma\",\n        \".\",\n        gsub(\"\\\\.gz$\", \"\", sample_df$sample_file)\n    ),\n    source = \"agilent\", green.only = T,\n    other.columns = \"gIsWellAboveBG\"\n)\nx_copy &lt;- x\n\n# gene annotation\nx$genes &lt;- left_join(x$genes, gpl, by = \"ProbeName\")\nall(x$genes$ProbeName == x_copy$genes$ProbeName)\n\n# background correction and normalization\n# at this step, we need control probes to be existed in the dataset\ny &lt;- backgroundCorrect(x, method = \"normexp\")\ny &lt;- normalizeBetweenArrays(y, method = \"quantile\")\n\n# gene filtering\n# filter out control probes\nControl &lt;- y$genes$ControlType == 1L\n# filter out probes without Symbol\nNoSymbol &lt;- is.na(y$genes$Symbol)\n# keep probes that express in at least 3 arrays (because there are at least 3 replicates in each array)\nIsExpr &lt;- rowSums(y$other$gIsWellAboveBG &gt; 0) &gt;= 3\n\nyfilt &lt;- y[!Control & !NoSymbol & IsExpr, ]\n\ngenes_colnames &lt;- c(\"EntrezID\", \"Symbol\")\nE_colnames &lt;- colnames(yfilt$E)\nexprMat &lt;- bind_cols(\n    yfilt$genes[, genes_colnames],\n    as.data.frame(yfilt$E)\n)\nexprMat_dedup &lt;- exprMat %&gt;%\n    arrange(Symbol) %&gt;%\n    group_by(EntrezID, Symbol) %&gt;%\n    reframe(across(everything(), mean))\nyfilt$genes &lt;- exprMat_dedup[, genes_colnames]\nyfilt$E &lt;- as.matrix(exprMat_dedup[, E_colnames])\n\n# differential expression\ntreatments &lt;- gsub(\"\\\\.[_0-9]+$\", \"\", sample_df$sample_id)\nlevels &lt;- unique(treatments)\ntreatments &lt;- factor(treatments, levels = levels)\ndesign &lt;- model.matrix(~ 0 + treatments)\ncolnames(design) &lt;- levels\n\nfit &lt;- lmFit(yfilt, design = design)\ncontrast_pairs &lt;- expand.grid(x = levels, y = levels) %&gt;%\n    mutate(\n        flag = if_else(x != y, T, F),\n        pair = paste0(x, \"-\", y)\n    ) %&gt;%\n    filter(flag) %&gt;%\n    pull(pair) %&gt;%\n    unique()\ncontrast_matrix &lt;- makeContrasts(contrasts = contrast_pairs, levels = design)\nfit2 &lt;- contrasts.fit(fit, contrast_matrix)\nfit2 &lt;- eBayes(fit2, trend = T, robust = T)\n\nfor (pair in contrast_pairs) {\n    res &lt;- topTable(fit2, coef = pair, number = Inf, adjust.method = \"BH\", p.value = 1)\n    vroom_write(res, file = file.path(out_dir, paste0(pair, \".tsv\")), delim = \"\\t\")\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#filter-degs-and-plot-volcanos",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#filter-degs-and-plot-volcanos",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "9 Filter DEGs and plot volcanos",
    "text": "9 Filter DEGs and plot volcanos\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\n\nset_fonts()\n\ndegs_dir &lt;- \"./data/degs\"\nout_dir &lt;- \"./data/clean_degs/padj0.05_logfc1\"\nsig_colors &lt;- c(\"#FF4757\", \"#546DE5\", \"#D2DAE2\")\npadj_th &lt;- 0.05\nlogfc_th &lt;- 1\n\ndir.create(out_dir, recursive = T)\n\ndegs_files &lt;- list.files(degs_dir, pattern = \"\\\\.tsv$\", full.names = T, recursive = F)\nfor (degs_file in degs_files) {\n    pair &lt;- strsplit(gsub(\"\\\\.[a-zA-Z0-9]+$\", \"\", basename(degs_file)),\n        split = \"-\", fixed = T\n    )[[1]]\n\n    degs_df &lt;- vroom(degs_file, delim = \"\\t\", col_names = T)\n    clean_degs_df &lt;- degs_df %&gt;%\n        group_by(Symbol) %&gt;%\n        slice_min(adj.P.Val, n = 1) %&gt;%\n        slice_max(abs(logFC), n = 1) %&gt;%\n        slice_sample(n = 1) %&gt;%\n        ungroup() %&gt;%\n        mutate(\n            diff_flag = if_else(adj.P.Val &lt; padj_th,\n                if_else(logFC &gt; logfc_th,\n                    paste0(pair[1], \" Up\"),\n                    if_else(logFC &lt; -logfc_th,\n                        paste0(pair[2], \" Up\"),\n                        \"NO\"\n                    )\n                ),\n                \"NO\"\n            ),\n            diff_flag = factor(diff_flag, levels = c(\n                paste0(pair[1], \" Up\"),\n                paste0(pair[2], \" Up\"),\n                \"NO\"\n            ))\n        )\n\n    vroom_write(clean_degs_df, file = file.path(out_dir, basename(degs_file)))\n\n    if (any(duplicated(clean_degs_df$Symbol))) {\n        stop(\"Duplicated items still existed after filtering for \", degs_file)\n    }\n\n    diff_counts &lt;- count(clean_degs_df, diff_flag) %&gt;%\n        mutate(show_text = paste0(diff_flag, \": \", n))\n    plot_title &lt;- paste0(\n        paste0(pair, collapse = \"_vs_\"), \"\\n\",\n        paste0(diff_counts$show_text, collapse = \"    \")\n    )\n\n    p &lt;- ggplot(clean_degs_df) +\n        geom_point(aes(logFC, -log10(adj.P.Val), color = diff_flag),\n            alpha = 0.5, size = 2\n        ) +\n        geom_vline(\n            xintercept = c(-logfc_th, logfc_th),\n            linewidth = 1, col = \"grey25\", linetype = \"dashed\"\n        ) +\n        geom_hline(\n            yintercept = -log10(padj_th),\n            linewidth = 1, col = \"grey25\", linetype = \"dashed\"\n        ) +\n        scale_color_manual(values = setNames(sig_colors, levels(clean_degs_df$diff_flag))) +\n        labs(\n            title = plot_title,\n            x = \"log2 Fold Change\", y = \"-log10(p.adjust)\",\n            color = paste0(\"p.adjust &lt; \", padj_th, \"\\n|log2(FC)| &gt; \", logfc_th)\n        ) +\n        theme_classic() +\n        theme(\n            plot.title = element_text(hjust = 0.5),\n            axis.title.x = element_text(size = 26),\n            axis.title.y = element_text(size = 26),\n            axis.text.x = element_text(size = 24),\n            axis.text.y = element_text(size = 24),\n            legend.text = element_text(size = 24),\n            legend.title = element_text(size = 26),\n            text = element_text(family = \"Arial\")\n        )\n\n    ppreview(p, file = file.path(\n        out_dir,\n        gsub(\"\\\\.[a-zA-Z0-9]+$\", \".pdf\", basename(degs_file))\n    ))\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#go-analysis",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#go-analysis",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "10 GO analysis",
    "text": "10 GO analysis\n\n10.1 Prepare ferret gene IDs\n\nlibrary(vroom)\nlibrary(tidyverse)\n\ngpl_file &lt;- \"./data/microarray/gpl.with_gene_symbols.tsv\"\nout_dir &lt;- \"./data/go\"\n\ndir.create(out_dir)\n\ngpl &lt;- vroom(gpl_file, delim = \"\\t\")\ngpl$ProbeName &lt;- sapply(gpl$qseqid, function(x) {\n    strsplit(x, \":\")[[1]][4]\n})\ngpl &lt;- gpl %&gt;%\n    select(all_of(c(\n        \"ProbeName\", \"Dbxref_GeneID\", \"gene\",\n        \"gene_biotype.GPL_blastn\", \"gene_biotype.Parent_gene\"\n    ))) %&gt;%\n    mutate(GeneBioType = if_else(is.na(gene_biotype.Parent_gene),\n        gene_biotype.GPL_blastn,\n        gene_biotype.Parent_gene\n    )) %&gt;%\n    select(all_of(c(\"ProbeName\", \"Dbxref_GeneID\", \"gene\", \"GeneBioType\"))) %&gt;%\n    distinct()\nnames(gpl) &lt;- c(\"ProbeName\", \"EntrezID\", \"Symbol\", \"GeneBioType\")\n\nvroom_write_lines(as.character(sort(unique(gpl$EntrezID))),\n    file = file.path(out_dir, \"ferret_entrezids.txt\")\n)\n\n\n\n10.2 Prepare orthologous gene set between mm10 and ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\n\nferret_mm_orthologs_file &lt;- \"./data/go/ferret_mm10.orthologs.tsv\"\nferret_dataset_file &lt;- \"./data/go/ferret_ncbi_dataset.tsv\"\n\nferret_dataset_df &lt;- vroom(ferret_dataset_file) %&gt;%\n    select(all_of(c(\"NCBI GeneID\", \"Symbol\", \"Gene Type\", \"Gene Group Identifier\"))) %&gt;%\n    set_colnames(c(\"ferret_EntrezID\", \"ferret_Symbol\", \"ferret_GeneType\", \"Ortholog_Group_Identifier\")) %&gt;%\n    na.omit() %&gt;%\n    mutate_all(as.character) %&gt;%\n    distinct()\nferret_mm_orthologs_df &lt;- vroom(ferret_mm_orthologs_file) %&gt;%\n    select(all_of(c(\"NCBI GeneID\", \"Symbol\", \"Gene Group Identifier\"))) %&gt;%\n    set_colnames(c(\"mm10_EntrezID\", \"mm10_Symbol\", \"Ortholog_Group_Identifier\")) %&gt;%\n    na.omit() %&gt;%\n    mutate_all(as.character) %&gt;%\n    distinct()\ndf &lt;- inner_join(ferret_dataset_df, ferret_mm_orthologs_df, by = \"Ortholog_Group_Identifier\")\nvroom_write(df, file = file.path(dirname(ferret_mm_orthologs_file), \"ferret_mm10.orthologs.clean.tsv\"))\n\n\n\n10.3 Attach orthologous mm10 gene IDs to DEGs of ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nferret_mm_orthologs_file &lt;- \"./data/go/ferret_mm10.orthologs.clean.tsv\"\ndegs_files &lt;- c(\n    \"./data/clean_degs/padj0.05_logfc1/A17.VZ-A19.VZ.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.ISVZ-A19.ISVZ.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.OSVZ-A19.OSVZ.tsv\"\n)\n\nferret_mm_orthologs_df &lt;- vroom(ferret_mm_orthologs_file) %&gt;%\n    mutate_all(as.character)\nfor (degs_file in degs_files) {\n    vroom(degs_file) %&gt;%\n        select(all_of(c(\n            \"EntrezID\", \"Symbol\",\n            \"logFC\", \"P.Value\", \"adj.P.Val\", \"diff_flag\"\n        ))) %&gt;%\n        mutate(EntrezID = as.character(EntrezID)) %&gt;%\n        inner_join(ferret_mm_orthologs_df, by = c(\"EntrezID\" = \"ferret_EntrezID\")) %&gt;%\n        arrange(desc(logFC)) %&gt;%\n        distinct() %&gt;%\n        filter(ferret_GeneType == \"PROTEIN_CODING\") %&gt;%\n        vroom_write(file = gsub(\"tsv$\", \"with_mm10_IDs.tsv\", degs_file))\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#check-the-differential-expression-concordance-between-samples-of-mink-and-samples-of-ferret",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/agilent_mrna_microarray_analysis_using_limma/index.html#check-the-differential-expression-concordance-between-samples-of-mink-and-samples-of-ferret",
    "title": "Agilent mRNA microarray analysis using limma",
    "section": "11 Check the differential expression concordance between samples of mink and samples of ferret",
    "text": "11 Check the differential expression concordance between samples of mink and samples of ferret\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nmink_degs_file &lt;- \"H:/ubuntu_ssd_backup/projects/mink/proj/rna/degs/res/only_degs/mm10/P2_Gyr_vs_P2_Sul.txt\"\ndegs_files &lt;- c(\n    \"./data/clean_degs/padj0.05_logfc1/A17.VZ-A19.VZ.with_mm10_IDs.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.ISVZ-A19.ISVZ.with_mm10_IDs.tsv\",\n    \"./data/clean_degs/padj0.05_logfc1/A17.OSVZ-A19.OSVZ.with_mm10_IDs.tsv\"\n)\n\nmink_degs &lt;- vroom(mink_degs_file) %&gt;%\n    select(all_of(c(\"final_gene_name\", \"diff_flag\"))) %&gt;%\n    distinct()\ndegs_df &lt;- tibble()\nfor (degs_file in degs_files) {\n    tmp_df &lt;- vroom(degs_file) %&gt;%\n        select(all_of(c(\"mm10_Symbol\", \"logFC\"))) %&gt;%\n        mutate(tissue = gsub(\n            \"A17\\\\.[A-Z]+-A19\\\\.|\\\\.with_mm10_IDs\\\\.tsv$\",\n            \"\", basename(degs_file)\n        )) %&gt;%\n        distinct()\n    degs_df &lt;- bind_rows(degs_df, tmp_df)\n}\ndf &lt;- inner_join(mink_degs, degs_df, by = c(\"final_gene_name\" = \"mm10_Symbol\")) %&gt;%\n    mutate(\n        tissue = factor(tissue, levels = c(\"VZ\", \"ISVZ\", \"OSVZ\")),\n        logFC_sign = if_else(logFC &gt;= 0, \"+\", \"-\")\n    )\n\nggplot(df) +\n    geom_jitter(aes(tissue, logFC, color = diff_flag))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html",
    "title": "Misc R code",
    "section": "",
    "text": "Used to collect various R code snippets."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#introduction",
    "title": "Misc R code",
    "section": "",
    "text": "Used to collect various R code snippets."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#plot-volcano-plot-in-batch",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#plot-volcano-plot-in-batch",
    "title": "Misc R code",
    "section": "2 Plot volcano plot in batch",
    "text": "2 Plot volcano plot in batch\n\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(magrittr)\nlibrary(ggrepel)\nlibrary(ggprism)\nlibrary(YRUtils)\n\ninput_dir &lt;- \"/home/yangrui/mywd/wutintin_transcriptome/input\"\noutput_dir &lt;- \"/home/yangrui/mywd/wutintin_transcriptome/output\"\ntop_n &lt;- 10\ngene_id_column &lt;- \"gene_name\"\ndiff_flag_column &lt;- \"diff_flag\"\nlogfc_column &lt;- \"log2FoldChange\"\npadj_column &lt;- \"padj\"\ncolor_column &lt;- \"diff_flag\"\nlogfc_threshold &lt;- 1\npadj_threshold &lt;- 0.05\n# Up, Down, No\ndiff_flag_colors &lt;- c(\"magenta4\", \"cyan4\", \"grey25\")\ngeom_point_alpha &lt;- 0.5\ngeom_point_size &lt;- 2\ngeom_line_linewidth &lt;- 1\ngeom_line_color &lt;- \"grey25\"\ngeom_line_linetype &lt;- \"dashed\"\ngeom_text_alpha &lt;- 1\nmin_segment_length &lt;- 3\ngeom_text_size &lt;- 5\ntheme_prism_panel_border &lt;- TRUE\nfont_family &lt;- \"Arial\"\ntheme_base_font_size &lt;- 16\n\nfiles &lt;- list.files(input_dir, pattern = \"\\\\.(tsv|txt|csv)$\", full.names = TRUE, recursive = FALSE)\nfor (file in files) {\n    data &lt;- vroom(file) %&gt;%\n        select(all_of(c(gene_id_column, diff_flag_column, logfc_column, padj_column))) %&gt;%\n        set_colnames(c(\"gene_id\", \"diff_flag\", \"logFC\", \"padj\")) %&gt;%\n        distinct()\n    anno_data &lt;- data %&gt;%\n        filter(diff_flag != \"NO\") %&gt;%\n        group_by(diff_flag) %&gt;%\n        slice_max(abs(logFC), n = top_n)\n\n    diff_flag_count_table &lt;- count(data, diff_flag) %&gt;%\n        arrange(diff_flag)\n    plot_title &lt;- paste0(paste0(\n        diff_flag_count_table$diff_flag, \": \",\n        diff_flag_count_table$n\n    ), collapse = \"\\n\")\n\n    p &lt;- ggplot(data, aes(logFC, -log10(padj), color = diff_flag)) +\n        geom_point(alpha = geom_point_alpha, size = geom_point_size) +\n        geom_vline(\n            xintercept = c(-logfc_threshold, logfc_threshold),\n            linewidth = geom_line_linewidth,\n            color = geom_line_color,\n            linetype = geom_line_linetype\n        ) +\n        geom_hline(\n            yintercept = -log10(padj_threshold),\n            linewidth = geom_line_linewidth,\n            color = geom_line_color,\n            linetype = geom_line_linetype\n        ) +\n        geom_label_repel(\n            data = anno_data,\n            mapping = aes(logFC, -log10(padj), color = diff_flag, label = gene_id),\n            max.overlaps = 10000, show.legend = FALSE, alpha = geom_text_alpha,\n            min.segment.length = min_segment_length, size = geom_text_size\n        ) +\n        labs(\n            title = plot_title,\n            x = \"logFC\", y = \"-log10(padj)\",\n            color = paste0(\"padj &lt; \", padj_threshold, \"\\nlogFC &gt; \", logfc_threshold)\n        ) +\n        theme_prism(border = theme_prism_panel_border, base_family = font_family, base_size = theme_base_font_size) +\n        theme(legend.title = element_text()) +\n        scale_color_manual(values = setNames(\n            diff_flag_colors,\n            c(paste0(strsplit(gsub(\"\\\\.(tsv|txt|csv)$\", \"\", basename(file)), \"_vs_\", fixed = TRUE)[[1]], \" Up\"), \"NO\")\n        ))\n\n    ppreview(p, file = file.path(output_dir, gsub(\"\\\\.(tsv|txt|csv)$\", \".pdf\", basename(file))))\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#visualize-scrna-seq-expression-of-a-single-gene-using-dot-plot-and-violin-plot-of-seurat",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#visualize-scrna-seq-expression-of-a-single-gene-using-dot-plot-and-violin-plot-of-seurat",
    "title": "Misc R code",
    "section": "3 Visualize scRNA-Seq expression of a single gene using dot plot and violin plot of Seurat",
    "text": "3 Visualize scRNA-Seq expression of a single gene using dot plot and violin plot of Seurat\n\nlibrary(Seurat)\nlibrary(tidyverse)\nlibrary(YRUtils)\n\nrna_rds_file &lt;- \"/data/users/yangrui/sc_omics_ref_datasets/human/dataset_2/ana/res/rna_to_umap.rds\"\noutput_dir &lt;- \"/home/yangrui/temp\"\ntarget_gene_names &lt;- \"BTN3A2\"\ntarget_cell_types &lt;- c(\"Exc. GluN1\" = \"GluN1\", \"Exc. GluN2\" = \"GluN2\", \"Exc. GluN3\" = \"GluN3\", \"Exc. GluN4\" = \"GluN4\", \"Exc. GluN5\" = \"GluN5\", \"Exc. GluN6\" = \"GluN6\", \"Exc. GluN7\" = \"GluN7\", \"Exc. GluN8\" = \"GluN8\", \"CGE IN \" = \"CGE IN\", \"MGE IN \" = \"MGE IN\", \"Oligo\" = \"OPC/Oligo\", \"MG\" = \"MG\")\n\nrna &lt;- readRDS(rna_rds_file)\necho_vec(sort(unique(rna[[]]$Name)))\n\n# dot plot\np &lt;- DotPlot(rna, features = target_gene_names, idents = target_cell_types) +\n    RotatedAxis() +\n    scale_y_discrete(\n        limits = rev(target_cell_types),\n        labels = rev(names(target_cell_types))\n    ) +\n    labs(x = NULL, y = NULL) +\n    theme(text = element_text(family = \"Arial\"))\n\nppreview(p, file = file.path(output_dir, \"dotplot.pdf\"))\n\n# violin plot\nident_colors &lt;- scales::hue_pal()(length(target_cell_types))\n\np &lt;- VlnPlot(rna, features = target_gene_names, idents = target_cell_types, pt.size = 0, cols = ident_colors)\np &lt;- p + geom_jitter(mapping = aes(color = ident), data = p$data, size = 1) +\n    scale_x_discrete(\n        limits = target_cell_types,\n        labels = names(target_cell_types)\n    ) +\n    scale_y_continuous(\n        expand = expansion(0),\n        limits = c(0, ceiling(max(p$data[[target_gene_names]])))\n    ) +\n    labs(x = NULL, y = NULL, color = \"Cell Type\") +\n    guides(fill = \"none\") +\n    theme(text = element_text(family = \"Arial\"))\n\nppreview(p, file = file.path(output_dir, \"violinplot.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#latex-spaces",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#latex-spaces",
    "title": "Misc R code",
    "section": "4 LaTeX spaces",
    "text": "4 LaTeX spaces\n\n\n\n\n\n\n\n\n`a b`\n\\(a \\qquad b\\)\nTwo m widths\n\n\n`a b`\n\\(a \\quad b\\)\nOne m width\n\n\na \\ b\n\\(a \\ b\\)\n\\(\\frac{1}{3}\\) m widths\n\n\na \\; b\n\\(a \\; b\\)\n\\(\\frac{2}{7}\\) m widths\n\n\na \\, b\n\\(a \\, b\\)\n\\(\\frac{1}{6}\\) m widths\n\n\nab\n\\(ab\\)\nNo space\n\n\na \\! b\n\\(a \\! b\\)\n\\(-\\frac{1}{6}\\) m widths\n\n\n\\hspace{length}\n\\(a \\hspace{1cm} b\\)\nHorizontal space of specified length"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#clusterprofiler-enrichment-results-interpretation",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#clusterprofiler-enrichment-results-interpretation",
    "title": "Misc R code",
    "section": "5 clusterProfiler enrichment results interpretation",
    "text": "5 clusterProfiler enrichment results interpretation\n超几何分布：\n假定在 \\(N\\) 件产品中有 \\(M\\) 件不合理，即不合格率 \\(p=\\frac{M}{N}\\)。\n现在产品中随机抽取 \\(n\\) 件做检查，发现 \\(k\\) 件不合格品的概率为\n\\[P(X=k)=\\frac{C^k_M C^{n-k}_{N-M}}{C^n_N},\\ k=t, t+1, ...,s,\\ s=\\min(M, n),\\ t=n-(N-M) \\ge 0\\]\n在做 GO 富集时，记 \\(N\\) 为背景基因集大小（例如可用样本中所有表达的基因或 GO 数据库中的所有基因作为背景基因集），\\(m\\) 为某一通路下的基因数，\\(n\\) 为输入做富集的基因数，\\(k\\) 为属于该通路下的基因数，则依据超几何分布，我们可以计算如下指标：\n\np.adjust: adjusted p value (i.e. adjusted \\(P(X=k)\\)).\nCount: \\(k\\).\nGene ratio: \\(\\frac{k}{n}\\).\nBackground ratio: \\(\\frac{m}{N}\\).\nRich factor: \\(\\frac{k}{m}\\).\nFold enrichment: \\(\\frac{\\text{Gene ratio}}{\\text{Background ratio}}\\)."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#sort-go-terms-of-two-clusters-based-on-richfactor-and-p.adjust-and-visualize-them-using-dot-plot",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#sort-go-terms-of-two-clusters-based-on-richfactor-and-p.adjust-and-visualize-them-using-dot-plot",
    "title": "Misc R code",
    "section": "6 Sort GO terms of two clusters based on RichFactor and p.adjust and visualize them using dot plot",
    "text": "6 Sort GO terms of two clusters based on RichFactor and p.adjust and visualize them using dot plot\n\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(YRUtils)\n\ngo_file &lt;- \"go_bp.tsv\"\ncluster_levels &lt;- c(\"P2-Sul Up\", \"P10-Sul Up\")\noutput_dir &lt;- \"~/temp\"\n\ngo_df &lt;- vroom(go_file) %&gt;%\n    mutate(Cluster = factor(Cluster, levels = cluster_levels))\n\n# classify terms into five categories based on RichFactor\nsample_flag_levels &lt;- c(paste0(cluster_levels[1], c(\"@Specific\", \"@Up\")), paste0(paste0(cluster_levels, collapse = \"_vs_\"), \"@Equal\"), paste0(cluster_levels[2], c(\"@Up\", \"@Specific\")))\nsample_flag_df &lt;- go_df %&gt;%\n    pivot_wider(id_cols = \"ID\", names_from = \"Cluster\", values_from = \"RichFactor\") %&gt;%\n    mutate(\n        SampleFlag = if_else(is.na(.[[cluster_levels[1]]]), paste0(cluster_levels[2], \"@Specific\"), if_else(is.na(.[[cluster_levels[2]]]), paste0(cluster_levels[1], \"@Specific\"), if_else(.[[cluster_levels[1]]] &gt; .[[cluster_levels[2]]], paste0(cluster_levels[1], \"@Up\"), if_else(.[[cluster_levels[1]]] &lt; .[[cluster_levels[2]]], paste0(cluster_levels[2], \"@Up\"), paste0(cluster_levels[1], \"_vs_\", cluster_levels[2], \"@Equal\"))))),\n        SampleFlag = factor(SampleFlag, levels = sample_flag_levels)\n    ) %&gt;%\n    pivot_longer(cols = !all_of(c(\"ID\", \"SampleFlag\")), names_to = \"Cluster\", values_to = \"RichFactor\") %&gt;%\n    na.omit() %&gt;%\n    distinct()\n# sort terms based on RichFactor and p.adjust\ndf &lt;- inner_join(sample_flag_df, distinct(select(go_df, Cluster, ID, p.adjust)), by = c(\"Cluster\", \"ID\")) %&gt;%\n    arrange(\n        SampleFlag,\n        case_when(\n            Cluster == cluster_levels[1] ~ -RichFactor,\n            Cluster == cluster_levels[2] ~ RichFactor,\n            TRUE ~ RichFactor\n        ),\n        case_when(\n            Cluster == cluster_levels[1] ~ -p.adjust,\n            Cluster == cluster_levels[2] ~ p.adjust,\n            TRUE ~ p.adjust\n        )\n    )\n\ngo_df &lt;- go_df %&gt;%\n    mutate(ID = factor(ID, levels = unique(df[[\"ID\"]]))) %&gt;%\n    arrange(ID) %&gt;%\n    mutate(Description = factor(Description, levels = rev(unique(Description))))\n\np &lt;- ggplot(go_df, aes(Cluster, Description, size = RichFactor, color = p.adjust)) +\n    geom_point() +\n    scale_color_gradient(low = \"#e06663\", high = \"#327eba\") +\n    scale_y_discrete(position = \"right\") +\n    guides(x = guide_axis(angle = 90)) +\n    labs(x = NULL, y = NULL) +\n    theme_bw(base_size = 18, base_family = \"Arial\")\nppreview(p, file = file.path(output_dir, \"temp.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#retrieve-uniprot-entries-using-rest-api-in-batch",
    "href": "Blogs/Bioinformatics/posts/Misc/misc_r_code/index.html#retrieve-uniprot-entries-using-rest-api-in-batch",
    "title": "Misc R code",
    "section": "7 Retrieve UniProt entries using REST API in batch",
    "text": "7 Retrieve UniProt entries using REST API in batch\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(rvest)\n\n# no more than 25\nmax_length &lt;- 20\norganism_id &lt;- 9606\nurl_template &lt;- \"https://rest.uniprot.org/uniprotkb/search?query=({accession_str})%20AND%20(active:true)%20AND%20(organism_id:{organism_id})&fields=accession,gene_primary,organism_name,organism_id,protein_name,annotation_score,protein_existence,reviewed,ft_intramem,cc_subcellular_location,ft_topo_dom,ft_transmem,go,go_p,go_c,go_f,cc_interaction&format=tsv\"\nfile &lt;- \"test.tsv\"\n\naccessions &lt;- vroom(file) %&gt;%\n    pull(Entry) %&gt;%\n    na.omit() %&gt;%\n    unique()\n\naccession_ls &lt;- split(accessions, ceiling(seq_along(accessions) / max_length))\ndf &lt;- tibble()\nfor (accession_vec in accession_ls) {\n    accession_str &lt;- paste0(paste0(\"accession:\", accession_vec), collapse = \"%20OR%20\")\n    url_instance &lt;- glue(url_template)\n\n    e &lt;- try(item_raw_text &lt;- read_html(url_instance) %&gt;% html_text(), silent = T)\n    if (\"try-error\" %in% class(e)) {\n        message(\"invalid accession string: \", accession_str)\n    } else {\n        item_df &lt;- vroom(I(item_raw_text), delim = \"\\t\") %&gt;%\n            mutate(\n                `Organism (ID)` = as.integer(`Organism (ID)`),\n                Annotation = as.integer(Annotation)\n            )\n        df &lt;- bind_rows(df, item_df)\n    }\n}\ndf &lt;- distinct(df)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html",
    "title": "Retrieve high-confidence NDD genes",
    "section": "",
    "text": "Retrieve high-confidence NDD genes from various databases."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#introduction",
    "title": "Retrieve high-confidence NDD genes",
    "section": "",
    "text": "Retrieve high-confidence NDD genes from various databases."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#geisinger-dbd-genes-database",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#geisinger-dbd-genes-database",
    "title": "Retrieve high-confidence NDD genes",
    "section": "2 Geisinger DBD Genes Database",
    "text": "2 Geisinger DBD Genes Database\nGo to Geisinger DBD Genes Database.\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nwork_dir &lt;- \"/home/yangrui/data\"\nfile &lt;- \"Geisinger_DBD_Genes_Database.Full_LoF_Table_Data.csv\"\n\nsetwd(work_dir)\n# keep high confidence candidate genes:\n# 1: genes with three or more de novo pathogenic loss-of-function variants\n# AR: genes with autosomal inheiritance\ndf &lt;- vroom(file) %&gt;%\n    filter(Tier %in% c(\"1\", \"AR\")) %&gt;%\n    arrange(Tier) %&gt;%\n    distinct()\nvroom_write(df, file = \"Geisinger_DBD_Genes_Database.Full_LoF_Table_Data.high_confidence.tsv\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#sysndd",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#sysndd",
    "title": "Retrieve high-confidence NDD genes",
    "section": "3 SysNDD",
    "text": "3 SysNDD\nGo to SysNDD.\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nwork_dir &lt;- \"/home/yangrui/data\"\nfile &lt;- \"SysNDD.sysndd_gene_table.txt\"\n\nsetwd(work_dir)\n# keep high confidence candidate genes: Definitive\ndf &lt;- vroom(file) %&gt;%\n    filter(entities_category %in% c(\"Definitive\")) %&gt;%\n    arrange(entities_category) %&gt;%\n    distinct()\nvroom_write(df, file = \"SysNDD.sysndd_gene_table.high_confidence.tsv\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#sfari-gene",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#sfari-gene",
    "title": "Retrieve high-confidence NDD genes",
    "section": "4 SFARI GENE",
    "text": "4 SFARI GENE\nGo to SFARI GENE.\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nwork_dir &lt;- \"/home/yangrui/data\"\nfile &lt;- \"ASD.SFARI_Gene.genes_04-03-2025release_04-15-2025export.csv\"\n\nsetwd(work_dir)\n# keep high confidence candidate genes: 1\ndf &lt;- vroom(file) %&gt;%\n    filter(`gene-score` %in% c(1)) %&gt;%\n    arrange(`gene-score`) %&gt;%\n    distinct()\nvroom_write(df, file = \"ASD.SFARI_Gene.genes_04-03-2025release_04-15-2025export.high_confidence.tsv\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#gene2phenotype",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#gene2phenotype",
    "title": "Retrieve high-confidence NDD genes",
    "section": "5 Gene2Phenotype",
    "text": "5 Gene2Phenotype\nGo to Gene2Phenotype.\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nwork_dir &lt;- \"/home/yangrui/data\"\nfile &lt;- \"DD.Gene2Phenotype.G2P_DD_2025-04-15.csv\"\n\nsetwd(work_dir)\n# keep high confidence candidate genes: definitive\ndf &lt;- vroom(file) %&gt;%\n    filter(confidence %in% c(\"definitive\")) %&gt;%\n    arrange(confidence) %&gt;%\n    distinct()\nvroom_write(df, file = \"DD.Gene2Phenotype.G2P_DD_2025-04-15.high_confidence.tsv\")\n\n# whether a disease is NDD is recognized by AI\nhigh_confidence_file &lt;- \"DD.Gene2Phenotype.G2P_DD_2025-04-15.high_confidence.tsv\"\nndd_label_file &lt;- \"DD.Gene2Phenotype.G2P_DD_2025-04-15.high_confidence.with_NDD_flag.tsv\"\n\nhigh_confidence_df &lt;- vroom(high_confidence_file)\nndd_label_df &lt;- vroom(ndd_label_file)\ndf &lt;- inner_join(high_confidence_df, ndd_label_df, by = c(\"disease name\" = \"disease_name\")) %&gt;%\n    filter(is_ndd == \"Yes\") %&gt;%\n    distinct()\nvroom_write(df, file = \"NDD.Gene2Phenotype.G2P_DD_2025-04-15.high_confidence.tsv\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#genetrek",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#genetrek",
    "title": "Retrieve high-confidence NDD genes",
    "section": "6 GeneTrek",
    "text": "6 GeneTrek\nGo to GeneTrek.\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nwork_dir &lt;- \"/home/yangrui/data\"\nfile &lt;- \"NDD.GeneTrek.genetrek_data_2024-04-26.tsv\"\n\nsetwd(work_dir)\n# keep high confidence candidate genes:\n# High Confidence Epilepsy Genes\n# or\n# High Confidence NDD genes v3\ndf &lt;- vroom(file) %&gt;%\n    filter((`High Confidence Epilepsy Genes` | `High Confidence NDD genes v3`) & (`Gene type` == \"protein-coding\")) %&gt;%\n    distinct()\nvroom_write(df, file = \"NDD.GeneTrek.genetrek_data_2024-04-26.high_confidence.tsv\")"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#spark",
    "href": "Blogs/Bioinformatics/posts/Database/retrieve_high_confidence_ndd_genes/index.html#spark",
    "title": "Retrieve high-confidence NDD genes",
    "section": "7 SPARK",
    "text": "7 SPARK\nGo to SPARK."
  },
  {
    "objectID": "Blogs/Bioinformatics/index.html",
    "href": "Blogs/Bioinformatics/index.html",
    "title": "Bioinformatics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRetrieve high-confidence NDD genes\n\n\n\n\n\n\nNDD\n\n\ngene\n\n\n\n\n\n\n\n\n\nApr 15, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBulk RNA-Seq differential expression analysis and batch correction with DESeq2, limma, or sva\n\n\n\n\n\n\nbulk rna-seq\n\n\ndifferential expression\n\n\nbatch correction\n\n\ndeseq2\n\n\nlimma\n\n\nsva\n\n\n\n\n\n\n\n\n\nMar 2, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBuild ENCODE ATAC/ChIP-Seq pipeline indices\n\n\n\n\n\n\nencode\n\n\natac-seq\n\n\nchip-seq\n\n\nindex\n\n\n\n\n\n\n\n\n\nFeb 28, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBuild ENCODE RNA-Seq pipeline indices\n\n\n\n\n\n\nencode\n\n\nrna-seq\n\n\nindex\n\n\n\n\n\n\n\n\n\nFeb 26, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nDE-related analysis for bulk proteomics\n\n\n\n\n\n\nbulk proteomics\n\n\ndifferential expression\n\n\n\n\n\n\n\n\n\nJan 16, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMPRA analysis pipeline\n\n\n\n\n\n\nmpra\n\n\n\n\n\n\n\n\n\nJan 8, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nMisc R code\n\n\n\n\n\n\nr\n\n\nmisc\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCut&Tag analysis pipeline\n\n\n\n\n\n\ncut&tag\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nAgilent mRNA microarray analysis using limma\n\n\n\n\n\n\nagilent\n\n\nmrna\n\n\nmicroarray\n\n\nlimma\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify housekeeping genes\n\n\n\n\n\n\nbulk rna-seq\n\n\nhousekeeping genes\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBulk RNA-seq pseudotime analysis\n\n\n\n\n\n\nbulk rna-seq\n\n\npseudotime\n\n\n\n\n\n\n\n\n\nSep 9, 2024\n\n\nRui Yang\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Bioinformatics"
    ]
  },
  {
    "objectID": "Blogs/Bioinformatics/about.html",
    "href": "Blogs/Bioinformatics/about.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "Here is a collection of blogs related to bioinfomatics, such as bulk RNA-seq, scRNA-seq, etc.\nEmail: 2413667864@qq.com"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html",
    "title": "Cut&Tag analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#introduction",
    "title": "Cut&Tag analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#md5sum-check-over-raw-fastq-files",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#md5sum-check-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "2 MD5SUM check over raw FASTQ files",
    "text": "2 MD5SUM check over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-raw-fastq-files",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "3 FASTQC over raw FASTQ files",
    "text": "3 FASTQC over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#quality-trimming-over-raw-fastq-files",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#quality-trimming-over-raw-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "4 Quality trimming over raw FASTQ files",
    "text": "4 Quality trimming over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-clean-fastq-files",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#fastqc-over-clean-fastq-files",
    "title": "Cut&Tag analysis pipeline",
    "section": "5 FASTQC over clean FASTQ files",
    "text": "5 FASTQC over clean FASTQ files\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#build-bowtie2-index",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#build-bowtie2-index",
    "title": "Cut&Tag analysis pipeline",
    "section": "6 Build Bowtie2 index",
    "text": "6 Build Bowtie2 index\n\nusing YRUtils\n\nref_fa = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mm10\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#align-reads-with-bowtie2",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#align-reads-with-bowtie2",
    "title": "Cut&Tag analysis pipeline",
    "section": "7 Align reads with Bowtie2",
    "text": "7 Align reads with Bowtie2\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mm10\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) &gt; 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) &gt; 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 -X 2000 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-reads-unmapped-and-with-low-quality",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-reads-unmapped-and-with-low-quality",
    "title": "Cut&Tag analysis pipeline",
    "section": "8 Remove reads unmapped and with low quality",
    "text": "8 Remove reads unmapped and with low quality\nAfter this step, if read duplication rate is very low, you can skip the next step - removing duplicate reads.\n\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-duplicate-reads",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#remove-duplicate-reads",
    "title": "Cut&Tag analysis pipeline",
    "section": "9 Remove duplicate reads",
    "text": "9 Remove duplicate reads\n\n9.1 Add @RG line if it is not existed\nWhen you mark duplicates using Picard, tag @RG within the header section in your BAM files is mandatory.\nYou can check whether your BAM files contain tag @RG line with the following code (no if nothing appears):\nsamtools view -H &lt;your BAM file&gt; | grep '@RG'\nCheck whether your BAM files are sorted:\nsamtools view -H &lt;your BAM file&gt; | grep '@HD'\nSee here for how read group is defined by GATK and here for how to extract some info from FASTQ files.\nRead group defined by GATK:\n@RG ID:H0164.2  PL:illumina PU:H0164ALXX140820.2    LB:Solexa-272222    PI:0    DT:2014-08-20T00:00:00-0400 SM:NA12878  CN:BI\n\nID:&lt;flow cell name&gt;.&lt;lane number&gt;: read group identifier.\nPU:&lt;flow cell barcode&gt;.&lt;lane number&gt;.&lt;sample barcode&gt;: platform unit (optional). Flow cell barcode refers to the unique identifier for a particular flow cell. Sample barcode is a sample/library-specific identifier.\nSM:&lt;sample name&gt;: sample.\nPL:&lt;platform name&gt;: platform/technology used to produce the read, such as ILLUMINA, SOLID, LS454, HELICOS and PACBIO.\nLB:&lt;library name&gt;: DNA preparation library identifier. This can be used to identify duplicates derived from library preparation.\n\n\n# Construct tag @RG line by extracting some info from paired-end FASTQ files and their file names\n# Only compatible with Casava 1.8 format\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nadd_rg_bam_dir = \"add_rg_bam\"\nplatform = \"ILLUMINA\"\nsamtools_n_threads = 40\n\nmkpath(add_rg_bam_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nfastq_header_names = [\"instrument_name\", \"run_id\", \"flowcell_id\", \"flowcell_lane\", \"lane_tile_number\", \"tile_cluster_x_coord\",\n    \"tile_cluster_y_coord\", \"pair_member_number\", \"is_passed\", \"control_bits_on\", \"index_sequence\"]\nfor raw_fastq_file in raw_fastq_files\n    input_bam_file = joinpath(high_qual_bam_dir, replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \".chr_srt.bam\"))\n    output_bam_file = joinpath(add_rg_bam_dir, basename(input_bam_file))\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", raw_fastq_file, \" | grep -P '^@' | head -n 1\")]))\n    fastq_header_line = split(strip(strip(read(cmd, String)), '@'), r\" +\")\n    if length(fastq_header_line) == 2\n        fastq_header_values = YRUtils.BaseUtils.flatten_array(split.(fastq_header_line, \":\"))\n        if length(fastq_header_values) == length(fastq_header_names)\n            fastq_header_dict = Dict(zip(fastq_header_names, fastq_header_values))\n            rg_line = string(\"@RG\\\\tID:\", fastq_header_dict[\"flowcell_id\"], \".\", fastq_header_dict[\"flowcell_lane\"],\n                \"\\\\tPL:\", platform, \"\\\\tSM:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \"\"),\n                \"\\\\tLB:\", replace(basename(raw_fastq_file), r\"\\.R[12]\\.(fastq|fq)(\\.gz)*\" =&gt; \"\"))\n            if isfile(input_bam_file) && !isfile(output_bam_file)\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"samtools addreplacerg -@ \", samtools_n_threads, \" -r '\", rg_line, \"' -o \",\n                        output_bam_file, \" \", input_bam_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                @info string(\"input BAM file (\", input_bam_file, \") is invalid or output BAM file (\", output_bam_file, \") has already existed, you can add @RG line yourself: \", rg_line)\n            end\n        else\n            @error \"unsupported FASTQ header format\"\n        end\n    else\n        @error \"unsupported FASTQ header format\"\n    end\nend\n\n\n\n9.2 Remove duplicate reads\n\nusing YRUtils\n\nadd_rg_bam_dir = \"add_rg_bam\"\nmark_dup_bam_dir = \"mark_dup_bam\"\nrm_dup_bam_dir = \"rm_dup_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\npicard_path = \"/data/softwares/picard_v3.3.0/picard.jar\"\nsamtools_n_threads = 40\n\nmkpath(mark_dup_bam_dir)\nmkpath(rm_dup_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(add_rg_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    mark_dup_bam_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".mark_dup.bam\"))\n    mark_dup_metrics_file = joinpath(mark_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".metrics.txt\"))\n    rm_dup_bam_file = joinpath(rm_dup_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".rm_dup.bam\"))\n\n    cmd = pipeline(\n        Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"java -jar \", picard_path, \" MarkDuplicates --INPUT \", bam_file, \" --OUTPUT \", mark_dup_bam_file,\n                    \" --METRICS_FILE \", mark_dup_metrics_file, \" --VALIDATION_STRINGENCY LENIENT \",\n                    \" --USE_JDK_DEFLATER true --USE_JDK_INFLATER true --ASSUME_SORT_ORDER coordinate \",\n                    \" --REMOVE_DUPLICATES false --TMP_DIR \", tmp_dir)]),\n        );\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\n\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" -F 1804 -f 2 -b \",\n                mark_dup_bam_file, \" -o \", rm_dup_bam_file)]));\n        stdout=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        stderr=joinpath(log_dir, \"mark_rm_reads_dups.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"mark_rm_reads_dups.log\"), \"a\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-fragment-length-distributions",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-fragment-length-distributions",
    "title": "Cut&Tag analysis pipeline",
    "section": "10 Assess fragment length distributions",
    "text": "10 Assess fragment length distributions\n\n10.1 Extract fragment lengths\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nfrag_len_dir = \"frag_len\"\nsamtools_n_threads = 40\n\nmkpath(frag_len_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    frag_len_stat_file = joinpath(frag_len_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".frag_len_stat.tsv\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -@ \", samtools_n_threads, \" \", bam_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' 'function abs(x) {return ((x &lt; 0.0) ? -x : x)} {print abs($9)}' \",\n                raw\" | sort -n | uniq -c | awk -v OFS='\\t' '{print $2,$1/2}' &gt; \", frag_len_stat_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n\n\n\n10.2 Visualize fragment length distibutions\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(vroom)\n\nfrag_len_dir &lt;- \"frag_len\"\n\nfrag_len_stat_files &lt;- list.files(frag_len_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\ndf &lt;- tibble()\nfor (frag_len_stat_file in frag_len_stat_files) {\n    tmp_df &lt;- vroom(frag_len_stat_file, col_names = c(\"frag_len\", \"count\")) %&gt;%\n        mutate(sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(frag_len_stat_file)))\n    df &lt;- bind_rows(df, tmp_df)\n}\ndf &lt;- df %&gt;% arrange(sample, frag_len, count)\n\np &lt;- ggplot(df, aes(frag_len, count, color = sample)) +\n    geom_freqpoly(stat = \"identity\", linewidth = 0.5) +\n    labs(\n        x = \"Fragment length\",\n        y = \"Count\", color = \"Sample\"\n    ) +\n    theme_classic(base_family = \"Arial\", base_size = 20)\nppreview(p, file = file.path(frag_len_dir, \"frag_len_dist.freqpoly.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-the-reproducibility-of-replicates",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#assess-the-reproducibility-of-replicates",
    "title": "Cut&Tag analysis pipeline",
    "section": "11 Assess the reproducibility of replicates",
    "text": "11 Assess the reproducibility of replicates\n\n11.1 Convert BAM into BED\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bed_dir = \"bam2bed\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam2bed_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    bed_file = joinpath(bam2bed_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".bed\"))\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" -o \", tmp_name_srt_bam_file, \" \", bam_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"bedtools bamtobed -bedpe -i \", tmp_name_srt_bam_file,\n            raw\" | awk -v FS='\\t' -v OFS='\\t' '$1 == $4 && $6 - $2 &lt; 1000 {print $0}' \",\n            raw\" | cut -f 1,2,6 | sort -k1,1 -k2,2n -k3,3n &gt; \", bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm(tmp_name_srt_bam_file)\nend\n\n\n\n11.2 Aggregate fragments into bin counts\n\n# We use the middle point of each fragment to infer which bin this fragment belongs to.\n# Calculate stategy:\n# w - window size (e.g. 500)\n# $1 - seqname\n# $2 - start\n# $3 - end\n# int() - round down\n# int(($2 + $3)/(2*w))*w + w/2\n# e.g. all middle points belonging to the left-closed interval [1000, 1500) will have the same quotient 2 by dividing 500,\n# and then, by multiplying 500, and then, by adding 500/2, and finally, the quantity is 1250.\n# Finally we use the vaule 1250 to represent all middle points belonging to [1000, 1500),\n# and then we just need to count the number of 1250 to know how many fragments are enriched in the interval [1000, 1500)\n\nusing YRUtils\n\nbam2bed_dir = \"bam2bed\"\nbin_width = 500\n\nbed_files = YRUtils.BaseUtils.list_files(bam2bed_dir, r\"\\.bed$\", recursive=false, full_name=true)\nfor bed_file in bed_files\n    frag_bin_count_file = joinpath(bam2bed_dir, replace(basename(bed_file), r\"\\.bed$\" =&gt; string(\".bin\", bin_width, \".tsv\")))\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", bed_file, \" | awk -v w=\", bin_width,\n                raw\" -v FS='\\t' -v OFS='\\t' '{print $1, int(($2 + $3)/(2*w))*w + w/2}' \",\n                raw\" | sort -k1,1V -k2,2n | uniq -c | awk -v OFS='\\t' '{print $2,$3,$1}' \",\n                \" | sort -k1,1V -k2,2n &gt; \", frag_bin_count_file)]),\n    )\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend\n\n\n\n11.3 Assess the reproducibility of replicates\n# At this step, we convert raw counts to Counts Per Million (CPM) to normalize the sequencing depth\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(psych)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggprism)\nlibrary(ggalign)\nlibrary(YRUtils)\n\nbam2bed_dir &lt;- \"bam2bed\"\nqc_dir &lt;- \"qc\"\nbin_width &lt;- 500\ncpm_th &lt;- 1\n\ndir.create(qc_dir, showWarnings = FALSE, recursive = FALSE)\nfiles &lt;- list.files(bam2bed_dir, pattern = paste0(\"\\\\.bin\", bin_width, \"\\\\.tsv$\"), full.names = TRUE, recursive = FALSE)\ndf &lt;- tibble()\nfor (file in files) {\n    tmp_df &lt;- vroom(file, col_names = c(\"seqname\", \"mid_point\", \"count\")) %&gt;%\n        mutate(\n            id = paste0(seqname, \":\", mid_point),\n            sample = gsub(\"\\\\.\\\\w+\\\\.tsv$\", \"\", basename(file))\n        ) %&gt;%\n        select(id, count, sample)\n    df &lt;- bind_rows(df, tmp_df)\n}\ndf &lt;- df %&gt;%\n    group_by(sample) %&gt;%\n    mutate(cpm = count / sum(count) * 1e6) %&gt;%\n    pivot_wider(id_cols = \"id\", names_from = \"sample\", values_from = \"cpm\", values_fill = 0) %&gt;%\n    select(-all_of(\"id\"))\ndf &lt;- log2(df[rowSums(df &gt;= cpm_th) &gt;= 1, ] + 1)\n\n# Correlation\ncor_res &lt;- corr.test(df, use = \"pairwise\", method = \"pearson\", adjust = \"BH\")\n\np &lt;- ggheatmap(\n    cor_res$r,\n    width = ncol(cor_res$r) * unit(10, \"mm\"),\n    height = nrow(cor_res$r) * unit(10, \"mm\")\n) +\n    scheme_align(free_spaces = \"t\") +\n    scale_fill_gradient2(\n        low = \"blue\", mid = \"white\", high = \"red\",\n        midpoint = 0, limits = c(-1, 1),\n        breaks = c(-1, -0.5, 0, 0.5, 1)\n    ) +\n    labs(fill = \"R\") +\n    guides(x = guide_axis(angle = 45)) +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    anno_top() +\n    ggalign(\n        data = gsub(\"_rep\\\\d+$\", \"\", colnames(cor_res$r)),\n        size = unit(4, \"mm\")\n    ) +\n    geom_tile(aes(y = 1, fill = factor(value))) +\n    scale_y_continuous(breaks = NULL, name = NULL, expand = expansion(0)) +\n    labs(fill = \"Sample\") +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    with_quad(scheme_align(guides = \"t\"), NULL)\nppreview(p, file = file.path(qc_dir, paste0(\"correlation.bin\", bin_width, \".pdf\")))\n\n# PCA\npca &lt;- PCA(t(df), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord &lt;- as.data.frame(pca$ind$coord)\npca_coord$sample &lt;- row.names(pca_coord)\npca_coord$group &lt;- factor(gsub(\"_rep\\\\d+$\", \"\", pca_coord$sample))\npca_eig &lt;- as.data.frame(pca$eig)\n\np &lt;- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 4) +\n    xlab(paste0(\"PC1 (\", round(pca_eig[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(pca_eig[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25) +\n    theme_prism(base_family = \"Arial\", border = TRUE, base_size = 20)\nppreview(p, file = file.path(qc_dir, paste0(\"pca.bin\", bin_width, \".pdf\")))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-signal-enrichment-profile",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-signal-enrichment-profile",
    "title": "Cut&Tag analysis pipeline",
    "section": "12 Plot signal enrichment profile",
    "text": "12 Plot signal enrichment profile\nIf controls are far more enriched than treatments, you may need to call peaks of treatments without using controls.\nYou can try to call peaks of controls and to explore the reason why these regions are enriched (maybe you should remove these regions from peaks of treatments if needed).\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nqc_dir = \"qc\"\nsambamba_n_threads = 40\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 500\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"plotFingerprint -b \", join(bam_files, \" \"),\n            \" --labels \", join(replace.(basename.(bam_files), r\"\\.\\w+\\.bam$\" =&gt; \"\"), \" \"),\n            \" --outQualityMetrics \", joinpath(qc_dir, \"plotfingerprint_quality_metrics.log\"),\n            \" --skipZeros --minMappingQuality \", map_qual,\n            \" --numberOfProcessors \", deeptools_n_threads,\n            \" --binSize \", bin_size,\n            \" --plotFile \", joinpath(qc_dir, \"plotfingerprint.pdf\"),\n            \" --outRawCounts \", joinpath(qc_dir, \"plotfingerprint.tsv\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#convert-bam-into-bigwig",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#convert-bam-into-bigwig",
    "title": "Cut&Tag analysis pipeline",
    "section": "13 Convert BAM into BigWig",
    "text": "13 Convert BAM into BigWig\n\nusing YRUtils\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nbam2bw_dir = \"bam2bw\"\ndeeptools_n_threads = 40\nmap_qual = 30\nbin_size = 10\nnorm_method = \"RPKM\"\neffective_genome_size = 3372855573\n\nmkpath(bam2bw_dir)\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    bw_file = joinpath(bam2bw_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".bw\"))\n    if !isfile(bw_file)\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"bamCoverage -b \", bam_file, \" -o \", bw_file,\n                    \" --numberOfProcessors \", deeptools_n_threads,\n                    \" --binSize \", bin_size,\n                    \" --normalizeUsing \", norm_method,\n                    \" --effectiveGenomeSize \", effective_genome_size,\n                    \" --minMappingQuality \", map_qual,\n                    \" --extendReads\")]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-tss-enrichment-profile",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#plot-tss-enrichment-profile",
    "title": "Cut&Tag analysis pipeline",
    "section": "14 Plot TSS enrichment profile",
    "text": "14 Plot TSS enrichment profile\n\n14.1 Extract transcription start sites\nlibrary(tidyverse)\nlibrary(rtracklayer)\nlibrary(vroom)\n\ngff_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\ngff_format &lt;- \"gtf\"\nseq_type &lt;- \"transcript\"\nid_col &lt;- \"transcript_id\"\n\ngff &lt;- import(gff_file, format = gff_format) %&gt;%\n    as.data.frame() %&gt;%\n    as_tibble()\ndf &lt;- gff %&gt;%\n    filter(type == seq_type) %&gt;%\n    mutate(\n        tss_start = if_else(strand == \"-\", end, start - 1),\n        tss_end = if_else(strand == \"-\", end + 1, start),\n        score = 0\n    ) %&gt;%\n    select(all_of(c(\"seqnames\", \"tss_start\", \"tss_end\", id_col, \"score\", \"strand\"))) %&gt;%\n    distinct()\nvroom_write(df,\n    file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".transcription_start_sites.bed\", gff_file),\n    col_names = FALSE, append = FALSE\n)\n\n\n14.2 Plot TSS enrichment profile\n\nusing YRUtils\n\nbam2bw_dir = \"bam2bw\"\ntss_file = \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.transcription_start_sites.bed\"\nqc_dir = \"qc\"\ndeeptools_n_threads = 40\nbin_size = 10\nbefore_length = 2000\nafter_length = 2000\n\nbw_files = YRUtils.BaseUtils.list_files(bam2bw_dir, r\"\\.bw$\", recursive=false, full_name=true)\nfor bw_file in bw_files\n    tss_cov_mat_file = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".mat.txt\"))\n    tss_cov_mat_gz = joinpath(bam2bw_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".mat.txt.gz\"))\n    tss_cov_heatmap_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".heatmap.pdf\"))\n    tss_cov_profile_pdf = joinpath(qc_dir, replace(basename(bw_file), r\"\\.bw$\" =&gt; \".profile.pdf\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"computeMatrix reference-point --referencePoint TSS --numberOfProcessors \", deeptools_n_threads,\n                \" -S \", bw_file, \" -R \", tss_file, \" --binSize \", bin_size,\n                \" --beforeRegionStartLength \", before_length, \" --afterRegionStartLength \", after_length,\n                \" --skipZeros --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"),\n                \" --outFileNameMatrix \", tss_cov_mat_file, \" --outFileName \", tss_cov_mat_gz)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotHeatmap -m \", tss_cov_mat_gz, \" -o \", tss_cov_heatmap_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"plotProfile -m \", tss_cov_mat_gz, \" -o \", tss_cov_profile_pdf,\n                \" --dpi 300 --samplesLabel \", replace(basename(bw_file), r\"\\.bw$\" =&gt; \"\"))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#call-peaks-with-macs",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#call-peaks-with-macs",
    "title": "Cut&Tag analysis pipeline",
    "section": "15 Call peaks with MACS",
    "text": "15 Call peaks with MACS\n\n15.1 Pairwise merging of biological replicates\ne.g. if you have three biological replicates for a sample, you will get three pooled BAM files: rep1_vs_rep2, rep1_vs_rep3, and rep2_vs_rep3.\n\nusing YRUtils\nusing Combinatorics\nusing NaturalSort\n\nrm_dup_bam_dir = \"rm_dup_bam\"\nsamtools_n_threads = 40\nsambamba_n_threads = 40\n\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\ndict = Dict{String,Vector{String}}()\nfor bam_file in bam_files\n    sample = replace(basename(bam_file), r\"_rep\\d+\\.\\w+\\.bam$\" =&gt; \"\")\n    if !haskey(dict, sample)\n        dict[sample] = [bam_file]\n    else\n        push!(dict[sample], bam_file)\n    end\nend\nfor sample in keys(dict)\n    combns = collect(combinations(dict[sample], 2))\n    for combn in combns\n        cmd = Cmd(\n            string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                string(\"samtools merge -t \", samtools_n_threads,\n                    \" -o \", joinpath(rm_dup_bam_dir,\n                        string(join(sort(replace.(basename.(combn), r\"\\.\\w+\\.bam$\" =&gt; \"\"), lt=natural), \"_vs_\"),\n                            \".rm_dup.bam\")),\n                    \" \", join(combn, \" \"))]))\n        @info string(\"running \", cmd, \" ...\")\n        run(cmd; wait=true)\n    end\nend\nbam_files = YRUtils.BaseUtils.list_files(rm_dup_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nYRUtils.BioUtils.bam_index(bam_files, sambamba_n_threads)\n\n\n\n15.2 Call peaks with MACS\nIf you want to perform IDR analysis, then use a relax threshold (e.g. p-value = 0.01) is suitable.\nIf you only want to perform naive overlapping, then use a stringent threshold (e.g. q-value = 0.05) is suitable.\n\n15.2.1 Call peaks with controls (using p-value)\n\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" =&gt; \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.2 Call peaks with controls (using q-value)\n\n# Whether you use the --SPMR flag or not,\n# MACS will normalize the data internally to call peaks.\n# The --SPMR flag only affects the signal track produced.\n# With the flag present, the signal will be normalized to reads per million,\n# this is for comparison with other samples which have been sequenced in different depths.\n# The option --SPMR only affects the bedGraph output.\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_with_ctl_dir = \"peak_with_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n# [[\"control1\", \"treatment1\"], [\"control2\", \"treatment2\"], ...]\nctl_treat_pairs = [\n    [\"IgG_rep1.rm_dup.bam\", \"Satb2_rep1.rm_dup.bam\"],\n    [\"IgG_rep2.rm_dup.bam\", \"Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep3.rm_dup.bam\", \"Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep2.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\"],\n    [\"IgG_rep1_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\"],\n    [\"IgG_rep2_vs_IgG_rep3.rm_dup.bam\", \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\"],\n]\n\nmkpath(peak_with_ctl_dir)\nfor ctl_treat_pair in ctl_treat_pairs\n    prefix = join(reverse(replace.(ctl_treat_pair, r\"\\.\\w+\\.bam$\" =&gt; \"\")), \"_vs_\")\n    macs_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_with_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -c \", joinpath(rm_dup_bam_dir, ctl_treat_pair[1]),\n                \" -t \", joinpath(rm_dup_bam_dir, ctl_treat_pair[2]),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_with_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.3 Call peaks without controls (using p-value)\n\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_pval0.01\"\neffective_genome_size = 2652684646\npvalue = 0.01\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" =&gt; \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".pval\", pvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -p \", pvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend\n\n\n\n15.2.4 Call peaks without controls (using q-value)\n\nrm_dup_bam_dir = \"rm_dup_bam\"\ntmp_dir = \"tmp\"\npeak_without_ctl_dir = \"peak_without_ctl_qval0.05\"\neffective_genome_size = 2652684646\nqvalue = 0.05\ncap_num = 1000000\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\ntreats = [\n    \"Satb2_rep1.rm_dup.bam\",\n    \"Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep2.rm_dup.bam\",\n    \"Satb2_rep1_vs_Satb2_rep3.rm_dup.bam\",\n    \"Satb2_rep2_vs_Satb2_rep3.rm_dup.bam\",\n]\n\nmkpath(peak_without_ctl_dir)\nfor treat in treats\n    prefix = replace(treat, r\"\\.\\w+\\.bam$\" =&gt; \"\")\n    macs_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \"_peaks.narrowPeak\"))\n    final_peak_file = joinpath(peak_without_ctl_dir, string(prefix, \".qval\", qvalue, \".narrowPeak.gz\"))\n    tmp_peak_file1 = joinpath(tmp_dir, string(prefix, \".tmp1\"))\n    tmp_peak_file2 = joinpath(tmp_dir, string(prefix, \".tmp2\"))\n    tmp_peak_file3 = joinpath(tmp_dir, string(prefix, \".tmp3\"))\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 callpeak \",\n                \" -t \", joinpath(rm_dup_bam_dir, treat),\n                \" -g \", effective_genome_size,\n                \" -n \", prefix,\n                \" --outdir \", peak_without_ctl_dir,\n                \" --tempdir \", tmp_dir,\n                \" -q \", qvalue,\n                \" -f BAMPE --keep-dup all -B --SPMR --call-summits\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort by Column 8 in descending order and replace long peak names in Column 4 with Peak_&lt;rank&gt;\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k 8gr,8gr \", macs_peak_file,\n                raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{$4=\"Peak_\"NR; if ($2&lt;0) $2=0; if ($3&lt;0) $3=0; if ($10==-1) $10=$2+int(($3-$2+1)/2.0); print $0}' &gt; \"\"\", tmp_peak_file1)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"head -n \", cap_num, \" \", tmp_peak_file1, \" &gt; \", tmp_peak_file2)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", tmp_peak_file2, \" \", chrsz, \" \", tmp_peak_file3, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", tmp_peak_file3, \" | pigz -nc &gt; \", final_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([tmp_peak_file1, tmp_peak_file2, tmp_peak_file3])\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#naive-overlapping",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#naive-overlapping",
    "title": "Cut&Tag analysis pipeline",
    "section": "16 Naive overlapping",
    "text": "16 Naive overlapping\n\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_peaks\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep3.qval0.05.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.qval0.05.naive_overlap.narrowPeak.gz\",\n    ],\n]\ninput_dir = \"peak_without_ctl_qval0.05\"\noutput_dir = \"naive_overlap_without_ctl_qval0.05\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools intersect -wo \",\n                \" -a \", joinpath(input_dir, narrow_peak[1]),\n                \" -b \", joinpath(input_dir, narrow_peak[2]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 &gt;= 0.5) || ($21/s2 &gt;= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq \",\n                \" | bedtools intersect -wo \",\n                \" -a stdin -b \", joinpath(input_dir, narrow_peak[3]),\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{s1=$3-$2; s2=$13-$12; if (($21/s1 &gt;= 0.5) || ($21/s2 &gt;= 0.5)) {print $0}}' \",\n                \" | cut -f 1-10 | sort -k1,1 -k2,2n | uniq | pigz -nc &gt; \", joinpath(output_dir, narrow_peak[4]))]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#idr-analysis",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#idr-analysis",
    "title": "Cut&Tag analysis pipeline",
    "section": "17 IDR analysis",
    "text": "17 IDR analysis\n\n# [[\"pooled_peaks\", \"rep1_peaks\", \"rep2_peaks\", \"output_prefix\"], ...]\nnarrow_peaks = [\n    [\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep2.pval0.01\",\n    ],\n    [\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep1_vs_Satb2_rep3.pval0.01\",\n    ],\n    [\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep3.pval0.01.narrowPeak.gz\",\n        \"Satb2_rep2_vs_Satb2_rep3.pval0.01\",\n    ],\n]\ninput_dir = \"peak_without_ctl_pval0.01\"\noutput_dir = \"idr_without_ctl_pval0.01\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nidr_threshold = 0.05\npeak_type = \"narrowPeak\"\nrank = \"p.value\"\n\nmkpath(output_dir)\nfor narrow_peak in narrow_peaks\n    peak1_file = joinpath(input_dir, narrow_peak[2])\n    peak2_file = joinpath(input_dir, narrow_peak[3])\n    pooled_peak_file = joinpath(input_dir, narrow_peak[1])\n\n    prefix = joinpath(output_dir, string(narrow_peak[4], \".idr\", idr_threshold))\n    idr_peak_file = string(prefix, \".\", peak_type, \".gz\")\n    idr_log_file = string(prefix, \".log\")\n    idr_12col_bed_file = string(prefix, \".\", peak_type, \".12-col.bed.gz\")\n    idr_out_file = string(prefix, \".unthresholded-peaks.txt\")\n    idr_tmp_file = string(prefix, \".unthresholded-peaks.txt.tmp\")\n    idr_out_gz_file = string(prefix, \".unthresholded-peaks.txt.gz\")\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"idr \",\n                \" --samples \", peak1_file, \" \", peak2_file,\n                \" --peak-list \", pooled_peak_file,\n                \" --input-file-type \", peak_type,\n                \" --output-file \", idr_out_file,\n                \"  --rank \", rank,\n                \" --soft-idr-threshold \", idr_threshold,\n                \" --log-output-file \", idr_log_file,\n                \" --plot --use-best-multisummit-IDR\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedClip \", idr_out_file, \" \", chrsz, \" \", idr_tmp_file, \" -truncate -verbose=2\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    if rank == \"signal.value\"\n        col = 7\n    elseif rank == \"p.value\"\n        col = 8\n    elseif rank == \"q.value\"\n        return 9\n    else\n        @error \"invalid score ranking method\"\n    end\n    minus_log10_threshold = -log10(idr_threshold)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '$12&gt;=\", minus_log10_threshold,\n                raw\" {if ($2&lt;0) $2=0; print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}' \",\n                \" | sort -k1,1 -k2,2n | uniq | sort -grk\", col, \",\", col,\n                \" | pigz -nc &gt; \", idr_12col_bed_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"zcat \", idr_12col_bed_file,\n                raw\" | awk -v FS='\\t' -v OFS='\\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10}' \",\n                \" | pigz -nc &gt; \", idr_peak_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"cat \", idr_tmp_file,\n                \" | pigz -nc &gt; \", idr_out_gz_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([idr_out_file, idr_tmp_file, idr_12col_bed_file, string(idr_out_file, \".noalternatesummitpeaks.png\")])\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#pairwise-overlapping-statistics",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#pairwise-overlapping-statistics",
    "title": "Cut&Tag analysis pipeline",
    "section": "18 Pairwise overlapping statistics",
    "text": "18 Pairwise overlapping statistics\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggprism)\nlibrary(YRUtils)\nlibrary(patchwork)\n\n# [\"peaks\", \"peaks\", ...]\nnarrow_peaks &lt;- c(\n    \"Satb2_rep1_vs_Satb2_rep2.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep1_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\",\n    \"Satb2_rep2_vs_Satb2_rep3.pval0.01.naive_overlap.narrowPeak.gz\"\n)\ninput_dir &lt;- \"naive_overlap_without_ctl_pval0.01\"\noverlap_stat_dir &lt;- \"overlap_stat\"\n\ndir.create(overlap_stat_dir)\ncombns &lt;- combn(narrow_peaks, 2)\nfor (i in seq_len(ncol(combns))) {\n    narrow_peak_pair &lt;- file.path(input_dir, combns[, i])\n    raw_df1 &lt;- vroom(gzfile(narrow_peak_pair[1]), col_names = FALSE) %&gt;%\n        select(X1, X2, X3) %&gt;%\n        arrange(X1, X2, X3) %&gt;%\n        distinct()\n    raw_df2 &lt;- vroom(gzfile(narrow_peak_pair[2]), col_names = FALSE) %&gt;%\n        select(X1, X2, X3) %&gt;%\n        arrange(X1, X2, X3) %&gt;%\n        distinct()\n    overlapped_df &lt;- bt.intersect(\n        a = raw_df1,\n        b = raw_df2,\n        wo = TRUE\n    )\n    overlapped_df1 &lt;- overlapped_df %&gt;%\n        select(V1, V2, V3, V7) %&gt;%\n        arrange(V1, V2, V3) %&gt;%\n        distinct() %&gt;%\n        mutate(\n            percent = V7 / (V3 - V2),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n    overlapped_df2 &lt;- overlapped_df %&gt;%\n        select(V4, V5, V6, V7) %&gt;%\n        arrange(V4, V5, V6) %&gt;%\n        distinct() %&gt;%\n        mutate(\n            percent = V7 / (V6 - V5),\n            interval = cut(percent,\n                breaks = seq(0, 1, 0.1),\n                include.lowest = TRUE,\n                right = TRUE\n            )\n        )\n\n    qc_metrics &lt;- paste0(\n        \"&gt;&gt;&gt; \", paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \" vs. \"), \": \\n\",\n        \"&gt;&gt; \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[1])), \": \\n\",\n        \"&gt; The number of peaks in total: \", nrow(raw_df1), \"\\n\",\n        \"&gt; The number of peaks overlapped: \", nrow(overlapped_df1), \"\\n\",\n        \"&gt; Percentage: \", round(nrow(overlapped_df1) / nrow(raw_df1), digits = 4), \"\\n\",\n        \"&gt;&gt; \", gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair[2])), \": \\n\",\n        \"&gt; The number of peaks in total: \", nrow(raw_df2), \"\\n\",\n        \"&gt; The number of peaks overlapped: \", nrow(overlapped_df2), \"\\n\",\n        \"&gt; Percentage: \", round(nrow(overlapped_df2) / nrow(raw_df2), digits = 4)\n    )\n\n    vroom_write_lines(qc_metrics,\n        file = file.path(\n            overlap_stat_dir,\n            paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".txt\")\n        ),\n        append = FALSE\n    )\n\n    p1 &lt;- ggplot(\n        overlapped_df1 %&gt;%\n            group_by(interval) %&gt;%\n            reframe(n = n()) %&gt;%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"1 vs. 2\") +\n        theme_prism()\n\n    p2 &lt;- ggplot(\n        overlapped_df2 %&gt;%\n            group_by(interval) %&gt;%\n            reframe(n = n()) %&gt;%\n            mutate(percent = n / sum(n)),\n        aes(interval, percent)\n    ) +\n        geom_bar(stat = \"identity\") +\n        scale_y_continuous(\n            expand = expansion(0),\n            limits = c(0, 1)\n        ) +\n        guides(x = guide_axis(angle = 30)) +\n        labs(title = \"2 vs. 1\") +\n        theme_prism()\n\n    ppreview(p1 | p2, file = file.path(\n        overlap_stat_dir,\n        paste0(paste0(gsub(\"\\\\.narrowPeak\\\\.gz$\", \"\", basename(narrow_peak_pair)), collapse = \"_vs_\"), \".pdf\")\n    ))\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-signal-tracks-with-macs",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-signal-tracks-with-macs",
    "title": "Cut&Tag analysis pipeline",
    "section": "19 Generate signal tracks with MACS",
    "text": "19 Generate signal tracks with MACS\n\nusing YRUtils\n\nbdg_dir = \"peak_without_ctl_qval0.05\"\nchrsz = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\n\nbdg_files = YRUtils.BaseUtils.list_files(bdg_dir, r\"\\.bdg$\", recursive=false, full_name=true)\nsamples = unique(replace.(basename.(bdg_files), r\"(_control_lambda|_treat_pileup).bdg$\" =&gt; \"\"))\nfor sample in samples\n    bdg_prefix = joinpath(bdg_dir, sample)\n    treat_bdg_file = string(bdg_prefix, \"_treat_pileup.bdg\")\n    ctl_bdg_file = string(bdg_prefix, \"_control_lambda.bdg\")\n    fc_bdg_file = string(bdg_prefix, \".fc.signal.bdg\")\n    fc_srt_bdg_file = string(bdg_prefix, \".fc.signal.srt.bdg\")\n    fc_bw_file = string(bdg_prefix, \".fc.signal.bw\")\n\n    # For fold enrichment signal tracks\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"macs3 bdgcmp \",\n                \" -t \", treat_bdg_file,\n                \" -c \", ctl_bdg_file,\n                \" --o-prefix \", bdg_prefix,\n                \" -m FE\")]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedtools slop -i \", string(bdg_prefix, \"_FE.bdg\"), \" -g \", chrsz, \" -b 0 \",\n                \" | bedClip stdin \", chrsz, \" \", fc_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    # Sort and remove any overlapping regions in bedgraph by comparing two lines in a row\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"LC_COLLATE=C sort -k1,1 -k2,2n \", fc_bdg_file,\n                raw\" | awk -v OFS='\\t' '{if (NR==1 || NR&gt;1 && (prev_chr!=$1 || prev_chr==$1 && prev_chr_e&lt;=$2)) {print $0}; prev_chr=$1; prev_chr_e=$3;}' &gt; \", fc_srt_bdg_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    cmd = Cmd(\n        string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"bedGraphToBigWig \", fc_srt_bdg_file, \" \", chrsz, \" \", fc_bw_file)]))\n    @info string(\"running \", cmd, \" ...\")\n    run(cmd; wait=true)\n\n    rm.([fc_bdg_file, fc_srt_bdg_file, treat_bdg_file, ctl_bdg_file, string(bdg_prefix, \"_FE.bdg\")])\nend"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#extending-peaks-on-both-sides-from-summits",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#extending-peaks-on-both-sides-from-summits",
    "title": "Cut&Tag analysis pipeline",
    "section": "20 Extending peaks on both sides from summits",
    "text": "20 Extending peaks on both sides from summits\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nextend_width &lt;- 250\nmerge_dist &lt;- 10\nchrsz_file &lt;- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\npeak_files &lt;- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.narrowPeak.gz\"\n)\n\nchrsz &lt;- vroom(chrsz_file, col_names = FALSE)\nfor (peak_file in peak_files) {\n    tmp_df &lt;- vroom(gzfile(peak_file), col_names = FALSE) %&gt;%\n        mutate(\n            summit_seqname = X1,\n            summit_start = X2 + X10,\n            summit_end = summit_start,\n        ) %&gt;%\n        select(summit_seqname, summit_start, summit_end) %&gt;%\n        arrange(summit_seqname, summit_start, summit_end) %&gt;%\n        distinct()\n    df &lt;- bt.slop(i = tmp_df, g = chrsz, b = extend_width) %&gt;%\n        distinct() %&gt;%\n        arrange(V1, V2, V3)\n    df &lt;- bt.merge(df, d = merge_dist) %&gt;%\n        mutate(\n            V4 = paste0(V1, \":\", V2, \"-\", V3),\n            V5 = 1000,\n            V6 = \"+\"\n        )\n    vroom_write(df,\n        file = gsub(\"\\\\.narrowPeak\\\\.gz$\", paste0(\".summits.b\", 2 * extend_width, \".bed\"), peak_file),\n        col_names = FALSE,\n        append = FALSE\n    )\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-final-peak-sets",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#generate-final-peak-sets",
    "title": "Cut&Tag analysis pipeline",
    "section": "21 Generate final peak sets",
    "text": "21 Generate final peak sets\nScheme 1: (recommended)\n\nCall peaks with MACS using p-value = 0.05\nPerform pairwise IDR analysis using idr threshold = 0.05\nMerge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\nScheme 2:\n\nCall peaks with MACS using q-value = 0.05\nPerform pairwise naive overlapping\nMerge peaks belonging to the same sample and merge peaks extended from summits belonging to the same sample\n\nlibrary(bedtoolsr)\nlibrary(vroom)\nlibrary(tidyverse)\n\nmerge_dist &lt;- 10\noutput_dir &lt;- \"final_peak\"\npeak_files &lt;- c(\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep2.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep1_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\",\n    \"idr_without_ctl_pval0.01/Satb2_rep2_vs_Satb2_rep3.pval0.01.idr0.05.summits.b500.bed\"\n)\noutput_file &lt;- \"Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\n\ndir.create(output_dir)\ndf &lt;- tibble()\nfor (peak_file in peak_files) {\n    df &lt;- bind_rows(\n        df,\n        vroom(gzfile(peak_file), col_names = FALSE) %&gt;%\n            select(X1, X2, X3)\n    )\n}\ndf &lt;- distinct(df) %&gt;%\n    arrange(X1, X2, X3)\ndf &lt;- bt.merge(df, d = merge_dist) %&gt;%\n    mutate(\n        V4 = paste0(V1, \":\", V2, \"-\", V3),\n        V5 = 1000,\n        V6 = \"+\"\n    )\nvroom_write(df, file = file.path(output_dir, output_file), col_names = FALSE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#de-novo-motif-finding-with-homer",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#de-novo-motif-finding-with-homer",
    "title": "Cut&Tag analysis pipeline",
    "section": "22 De novo motif finding with Homer",
    "text": "22 De novo motif finding with Homer\n\n22.1 De novo motif finding with Homer\n\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/homer/de_novo/narrow_peak\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" =&gt; \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' &gt; \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" \", extra_args, \" \", if !isempty(known_motifs_file)\n                string(\" -mknown \", known_motifs_file)\n            else\n                \"\"\n            end)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\n\n\n22.2 Finding instances of specific motifs\n\nusing YRUtils\n\noutput_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\noutput_file = \"motif_instances.txt\"\ntmp_dir = \"/data/users/yangrui/mouse/cuttag_v20250108/tmp\"\n# Peak file must have at least six columns\npeak_file = \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.summits.b500.merged.bed\"\ngenome = \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt_analysis_set_ENCODE.fasta.gz\"\nknown_motifs_file = \"/data/biodatabase/motifs/all_motifs_rmdup.from_peca2.txt\"\nscan_size = \"given\"\nhomer_n_threads = 40\nextra_args = \"\"\n\nnew_peak_file = joinpath(tmp_dir, replace(basename(peak_file), r\"\\.gz$\" =&gt; \"\"))\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"zcat -f \", peak_file,\n            raw\"\"\" | awk -v FS=\"\\t\" -v OFS=\"\\t\" '{if ($6==\".\") {$6=\"+\"}; print $0}' &gt; \"\"\", new_peak_file)]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\nif !isnothing(match(r\"\\.gz$\", genome))\n    new_genome = joinpath(tmp_dir, replace(basename(genome), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(genome, new_genome; decompress=true, keep=true)\nelse\n    new_genome = genome\nend\n\ncmd = Cmd(\n    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n        string(\"findMotifsGenome.pl \", new_peak_file, \" \", new_genome, \" \", output_dir,\n            \" -size \", scan_size, \" -p \", homer_n_threads, \" -preparsedDir \", tmp_dir,\n            \" -find \", known_motifs_file, \" \", extra_args, \" &gt; \", joinpath(output_dir, output_file))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#peak-annotation",
    "href": "Blogs/Bioinformatics/posts/Cut&Tag/cuttag_analysis_pipeline/index.html#peak-annotation",
    "title": "Cut&Tag analysis pipeline",
    "section": "23 Peak annotation",
    "text": "23 Peak annotation\n\n23.1 Make TxDb object from GFF3/GTF file\nlibrary(txdbmaker)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(GenomeInfoDb)\n\nanno_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.gtf.gz\"\nchrsz_file &lt;- \"/data/biodatabase/species/mm10/genome/genome/mm10_no_alt.chrom.sizes.tsv\"\nanno_format &lt;- \"gtf\"\norganism &lt;- \"Mus musculus\"\n# Sugar glider (Petaurus breviceps): 34899\n# Mouse (Mus musculus): 10090\ntaxonomy_id &lt;- 10090\ngenome &lt;- \"mm10\"\ncircular_chrs &lt;- c(\"chrM\")\n\nchrsz &lt;- vroom(chrsz_file, col_names = FALSE)\nseqinfo &lt;- Seqinfo(\n    seqnames = chrsz$X1,\n    seqlengths = chrsz$X2,\n    isCircular = if_else(chrsz$X1 %in% circular_chrs, TRUE, FALSE),\n    genome = genome\n)\ntxdb &lt;- makeTxDbFromGFF(\n    file = anno_file,\n    format = anno_format,\n    organism = organism,\n    taxonomyId = taxonomy_id,\n    chrominfo = seqinfo\n)\n# loadDB()\nsaveDb(txdb, file = gsub(\"\\\\.\\\\w+(\\\\.gz)*$\", \".TxDb.sqlite\", anno_file))\n\n\n23.2 Profile of peaks binding to TSS/body/TTS regions\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggprism)\nlibrary(ggplot2)\n\npeak_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir &lt;- \"final_peak/peak_anno/narrow_peak\"\nupstream_dist &lt;- 2000\ndownstream_dist &lt;- 2000\nby_what &lt;- \"gene\"\ntypes &lt;- c(\"start_site\", \"body\", \"end_site\")\nnbin &lt;- 400\n\npeak &lt;- readPeakFile(peak_file)\ntxdb &lt;- loadDb(txdb_file)\nfor (type in types) {\n    profile_p &lt;- plotPeakProf2(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme_prism(\n        base_size = 14,\n        base_family = \"Arial\",\n        border = TRUE\n    )\n\n    profile_p_file &lt;- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".profile.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(profile_p, file = profile_p_file)\n\n    heatmap_p &lt;- peakHeatmap(\n        peak = peak,\n        upstream = upstream_dist,\n        downstream = downstream_dist,\n        by = by_what,\n        type = type,\n        nbin = if (type == \"body\") nbin else NULL,\n        TxDb = txdb\n    ) + theme(\n        text = element_text(family = \"Arial\", size = 14)\n    )\n\n    heatmap_p_file &lt;- file.path(\n        peak_anno_dir,\n        gsub(\n            \"\\\\.[a-zA-Z0-9]+$\",\n            paste0(\".\", by_what, \".\", type, \".heatmap.pdf\"),\n            basename(peak_file)\n        )\n    )\n    ppreview(heatmap_p, file = heatmap_p_file)\n}\n\n\n23.3 Peak annotation\nlibrary(ChIPseeker)\nlibrary(YRUtils)\nlibrary(AnnotationDbi)\nlibrary(ggplot2)\nlibrary(ggprism)\nlibrary(vroom)\n\npeak_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.bed\"\ntxdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.primary_assembly.annotation_UCSC_names.TxDb.sqlite\"\npeak_anno_dir &lt;- \"final_peak/peak_anno/narrow_peak\"\ntss_region &lt;- c(-2000, 2000)\nby_what &lt;- \"transcript\"\n\npeak &lt;- readPeakFile(peak_file)\ntxdb &lt;- loadDb(txdb_file)\npeak_anno &lt;- annotatePeak(\n    peak = peak,\n    tssRegion = tss_region,\n    TxDb = txdb,\n    level = by_what\n)\n\npeak_anno_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno.tsv\"),\n        basename(peak_file)\n    )\n)\nvroom_write(as.data.frame(peak_anno),\n    file = peak_anno_file,\n    col_names = TRUE, append = FALSE\n)\n\npeak_anno_stat_df &lt;- getAnnoStat(peak_anno)\n\npie_p &lt;- ggplot(peak_anno_stat_df, aes(x = \"\", y = Frequency, fill = Feature)) +\n    geom_col() +\n    coord_polar(theta = \"y\") +\n    theme_void(base_size = 20, base_family = \"Arial\")\n\npie_p_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_pie.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(pie_p, file = pie_p_file)\n\ntss_distribution_p &lt;- plotDistToTSS(peak_anno) +\n    labs(title = NULL) +\n    theme_prism(border = TRUE, base_size = 20, base_family = \"Arial\") +\n    theme(legend.title = element_text())\n\ntss_distribution_p_file &lt;- file.path(\n    peak_anno_dir,\n    gsub(\n        \"\\\\.[a-zA-Z0-9]+$\",\n        paste0(\".\", by_what, \".anno_tss_distribution.pdf\"),\n        basename(peak_file)\n    )\n)\nppreview(tss_distribution_p, file = tss_distribution_p_file)\n\n\n23.4 Functional enrichment analysis\nlibrary(vroom)\nlibrary(magrittr)\nlibrary(clusterProfiler)\nlibrary(AnnotationDbi)\nlibrary(tidyverse)\n\npeak_anno_file &lt;- \"/data/users/yangrui/mouse/cuttag_v20250108/final_peak/peak_anno/narrow_peak/Satb2.without_ctl.pval0.01.idr0.05.narrowPeak.merged.transcript.anno.tsv\"\nmpt_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/gencode.vM21.trna.ercc.phix.gtf.gz.gene_id_name_mapping_table.tsv\"\norgdb_file &lt;- \"/data/biodatabase/species/mm10/genome/anno/org.Mm.eg.db.sqlite\"\n\nmpt_df &lt;- vroom(mpt_file) %&gt;%\n    select(gene_id, gene_name) %&gt;%\n    distinct()\npeak_anno_df &lt;- vroom(peak_anno_file) %&gt;%\n    inner_join(mpt_df, by = c(\"geneId\" = \"gene_id\")) %&gt;%\n    select(seqnames, start, end, V4, V5, V6, gene_name, distanceToTSS) %&gt;%\n    set_colnames(c(\"seqname\", \"start\", \"end\", \"name\", \"score\", \"strand\", \"gene_name\", \"dist_to_tss\")) %&gt;%\n    distinct()\nvroom_write(peak_anno_df,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.with_names.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)\n\norgdb &lt;- loadDb(orgdb_file)\nego &lt;- enrichGO(\n    unique(na.omit(peak_anno_df[[\"gene_name\"]])),\n    OrgDb = orgdb,\n    keyType = \"SYMBOL\",\n    ont = \"ALL\",\n    pvalueCutoff = 0.05,\n    qvalueCutoff = 0.05,\n    pAdjustMethod = \"BH\",\n    minGSSize = 10,\n    maxGSSize = 1000,\n    readable = FALSE,\n    pool = FALSE\n)\nego\nnrow(ego@result)\nsaveRDS(ego, file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.rds\", peak_anno_file))\nvroom_write(ego@result,\n    file = gsub(\"\\\\.tsv$\", \"\\\\.GO_ALL.tsv\", peak_anno_file),\n    col_names = TRUE, append = FALSE\n)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html",
    "href": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html",
    "title": "MPRA analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230\"\n\nsetwd(work_dir)\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n\n\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n\n\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)\n\n\n\n\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n\n\n\n\nWrite each reference sequence containing CRE as well as other necessary sequences and its ID into a FASTA file, which will be used later to build Bowtie2 reference index.\n\nusing CSV, DataFrames\n\nleft_seq = \"TTCTCTGGCCTAACTGTCTAGACCTGCAGGAGGACCGGATCAACT\"\nright_seq = \"CATTGCGTGAACCGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTGCAAAGTGAACACATCGCTAAGCGAAAGCTAAGNNNNNNNNNNNNNNNACCGGTCGCCACCATGGTGAGCAAGG\"\nmpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.tsv\"\nmpra_ctl_file = \"ref/null_sequences.tsv\"\noutput_mpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.dealed.tsv\"\noutput_mpra_ctl_file = \"ref/null_sequences.dealed.tsv\"\noutput_ref_fa_file = \"ref/mpra_ref.fa\"\n\nmpra_test = CSV.read(mpra_test_file, DataFrame)\nmpra_test = unique(mpra_test)\nmpra_test = transform(mpra_test, \"PSCE\", \"extended_mm10_seq\" =&gt; (x -&gt; string.(left_seq, x, right_seq)) =&gt; \"attached_seq\")\nmpra_test = groupby(mpra_test, \"PSCE\")\nmpra_test = transform(mpra_test, nrow =&gt; \"num_per_PSCE\", eachindex =&gt; \"PSCE_sub_rank\")\nmpra_test = transform(mpra_test, [\"PSCE\", \"PSCE_sub_rank\", \"num_per_PSCE\"] =&gt; ByRow((x, y, z) -&gt; begin\n    if z == 1\n        x\n    else\n        string.(x, \"_\", y)\n    end\nend) =&gt; \"PSCE_new_id\")\n\nCSV.write(output_mpra_test_file, mpra_test; delim=\"\\t\", append=false)\n\nmpra_ctl = CSV.read(mpra_ctl_file, DataFrame)\nmpra_ctl = unique(mpra_ctl)\nmpra_ctl = transform(mpra_ctl, eachindex =&gt; \"rank\")\nmpra_ctl = transform(mpra_ctl, \"rank\" =&gt; (x -&gt; string.(\"CTL\", x)) =&gt; \"PSCE\", \"seq\" =&gt; (x -&gt; string.(left_seq, x, right_seq)) =&gt; \"attached_seq\")\n\nCSV.write(output_mpra_ctl_file, mpra_ctl; delim=\"\\t\", append=false)\n\nref_fa = vcat(string.(\"&gt;\", mpra_test[!, \"PSCE_new_id\"], \"\\n\", mpra_test[!, \"attached_seq\"]),\n    string.(\"&gt;\", mpra_ctl[!, \"PSCE\"], \"\\n\", mpra_ctl[!, \"attached_seq\"]))\n\nopen(output_ref_fa_file, \"w\") do io\n    for line in ref_fa\n        println(io, line)\n    end\nend\n\n\n\n\n\nusing YRUtils\n\nref_fa = \"ref/mpra_ref.fa\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mpra_ref\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend\n\n\n\n\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mpra_ref\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nmkpath(log_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) &gt; 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) &gt; 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 --np 0 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\n\n\n\n\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend\n\n\n\n\n\nusing XAM, FASTX, CSV, DataFrames, YRUtils, Serialization\n\nfunction extract_cre_bc_pairs(bam_file::AbstractString, ref_dict::Dict{String,String};\n    barcode_length::Int=15, quality_scheme::Int=33)\n    valid_dna_bases = (\"A\", \"T\", \"C\", \"G\")\n\n    total_num_records = 0\n    xn_num_records = 0\n    complete_xn_num_records = 0\n    valid_xn_num_records = 0\n    typical_aln_vec_dict = Dict{String,Vector{String}}()\n\n    cre_bc_vec = Tuple{String,String}[]\n    reader = open(BAM.Reader, bam_file)\n    record = BAM.Record()\n    while !eof(reader)\n        empty!(record)\n        read!(reader, record)\n        total_num_records += 1\n        # The optional field XN:i:&lt;N&gt; reports the number of ambiguous reference characters (e.g. N) overlapped by an alignment\n        if haskey(record, \"XN\") && record[\"XN\"] == barcode_length\n            xn_num_records += 1\n            ref_name = BAM.refname(record)\n            # The leftmost mapping position\n            # BAM is 0-based, while SAM is 1-based\n            # BAM.position() gets the 1-based leftmost mapping position of record\n            ref_pos = BAM.position(record)\n            ref_seq = ref_dict[ref_name]\n            cigar_str = BAM.cigar(record)\n            query_seq = string(BAM.sequence(record))\n            query_qual_char_seq = join(Char.(BAM.quality(record) .+ quality_scheme))\n\n            aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_seq, ref_pos; truncate_ref=false))\n            qual_aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_qual_char_seq, ref_pos; truncate_ref=false))\n\n            ref_m = match(Regex(string(\"N{\", barcode_length, \"}\")), aln_vec[1])\n            if !isnothing(ref_m)\n                complete_xn_num_records += 1\n                extract_range = ref_m.offset:(ref_m.offset+barcode_length-1)\n                barcode_seq = aln_vec[2][extract_range]\n                barcode_qual_char_seq = qual_aln_vec[2][extract_range]\n                if all(split(barcode_seq, \"\") .∈ Ref(valid_dna_bases)) && all([Int(c) - quality_scheme for c in barcode_qual_char_seq] .&gt;= base_qual)\n                    valid_xn_num_records += 1\n                    push!(cre_bc_vec, (ref_name, barcode_seq))\n                    typical_aln_vec_dict[string(ref_name, \":\", barcode_seq)] = aln_vec\n                end\n            end\n        end\n    end\n    close(reader)\n\n    cre_bc_gdf = groupby(DataFrame(cre_bc_vec, [:cre, :barcode]), [:cre, :barcode])\n    uniq_cre_bc_df = sort(combine(cre_bc_gdf, nrow =&gt; \"num\", proprow =&gt; \"prop\"), :num, rev=true)\n\n    open(replace(bam_file, r\"\\.bam$\" =&gt; \".extract_cre_bc_pairs.log\"), \"w\") do io\n        println(io, string(\n            \"The number of records in total: \", total_num_records, \"\\n\",\n            \"The number of records with XN field: \", xn_num_records, \"\\n\",\n            \"The number of records with complete barcode: \", complete_xn_num_records, \"\\n\",\n            \"The number of records passing base and quality check: \", valid_xn_num_records, \"\\n\",\n            \"The number of records non-redundant: \", nrow(uniq_cre_bc_df)\n        ))\n    end\n\n    return [uniq_cre_bc_df, typical_aln_vec_dict]\nend\n\nref_file = \"ref/mpra_ref.fa\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nbase_qual = 20\n\n# Read in reference sequences\nref_dict = FASTAReader(open(ref_file, \"r\")) do reader\n    dict = Dict{String,String}()\n    for record in reader\n        dict[identifier(record)] = sequence(record)\n    end\n    return dict\nend\n\nbam_files = YRUtils.BaseUtils.list_files(high_qual_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    # Extract CRE-Barcode pairs\n    cre_bc_res = extract_cre_bc_pairs(bam_file, ref_dict)\n\n    CSV.write(replace(bam_file, r\"\\.bam$\" =&gt; \".uniq_cre_bc_pairs.tsv\"),\n        cre_bc_res[1]; delim=\"\\t\", append=false, writeheader=true)\n\n    # obj = open(jls_file, \"r\") do io\n    #     deserialize(io)\n    # end\n    open(replace(bam_file, r\"\\.bam$\" =&gt; \".typical_cre_bc_aligned_sequences.jls\"), \"w\") do io\n        serialize(io, cre_bc_res[2])\n    end\n\n    rand_keys = rand(keys(cre_bc_res[2]), 100)\n    rand_dict = Dict(k =&gt; cre_bc_res[2][k] for k in rand_keys)\n    YRUtils.BioUtils.show_align(rand_dict,\n        replace(bam_file, r\"\\.bam$\" =&gt; \".typical_cre_bc_aligned_sequences.100.html\");\n        wrap_width=120)\nend\n\n\n\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(ggprism)\n\ninput_dir &lt;- \"high_qual_bam\"\n\ncre_bc_files &lt;- list.files(input_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\nfor (cre_bc_file in cre_bc_files) {\n    cre_bc_df &lt;- vroom(cre_bc_file) %&gt;%\n        select(cre, barcode) %&gt;%\n        distinct()\n\n    cre_count_df &lt;- count(cre_bc_df, cre) %&gt;%\n        mutate(type = if_else(str_detect(cre, \"^CTL\"), \"CTL\", \"CRE\")) %&gt;%\n        rename(cre_bc = cre)\n    bc_count_df &lt;- count(cre_bc_df, barcode) %&gt;%\n        mutate(type = \"BC\") %&gt;%\n        rename(cre_bc = barcode)\n    count_df &lt;- bind_rows(cre_count_df, bc_count_df)\n\n    cre_quantiles &lt;- quantile(count_df$n[count_df$type %in% c(\"CTL\", \"CRE\")], probs = seq(0, 1, 0.1))\n    cre_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CRE\"], probs = seq(0, 1, 0.1))\n    ctl_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CTL\"], probs = seq(0, 1, 0.1))\n    bc_quantiles &lt;- quantile(count_df$n[count_df$type == \"BC\"], probs = seq(0, 1, 0.1))\n    type_nums &lt;- table(count_df$type)\n\n    paste0(\n        \"1. CRE/CTL quantiles (\", type_nums[\"CTL\"] + type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_quantiles), \"\\t\", cre_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"2. CRE only quantiles (\", type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_only_quantiles), \"\\t\", cre_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"3. CTL only quantiles (\", type_nums[\"CTL\"], \"): \\n\",\n        paste0(paste0(names(ctl_only_quantiles), \"\\t\", ctl_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"4. BC quantiles (\", type_nums[\"BC\"], \"): \\n\",\n        paste0(paste0(names(bc_quantiles), \"\\t\", bc_quantiles), collapse = \"\\n\")\n    ) %&gt;% vroom_write_lines(file = gsub(\"\\\\.tsv$\", \"\\\\.quantiles.txt\", cre_bc_file))\n\n    p &lt;- ggplot(count_df, aes(type, log2(n), fill = type, color = type)) +\n        geom_violin(scale = \"width\", alpha = 0.25, trim = TRUE) +\n        geom_boxplot(width = 0.2, outliers = FALSE, alpha = 0.25) +\n        scale_y_continuous(expand = expansion(mult = c(0.05, 0))) +\n        labs(\n            x = \"Sequence Type\",\n            y = \"log2(Count)\"\n        ) +\n        theme_prism(base_size = 20, base_family = \"Arial\", border = FALSE) +\n        theme(legend.position = \"none\")\n    ppreview(p, file = gsub(\"\\\\.tsv$\", \"\\\\.violin.pdf\", cre_bc_file))\n}\n\n\n\nlibrary(vroom)\nlibrary(tidyverse)\n\ninput_dirs &lt;- c(\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20231027/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/pcr_v20230922/high_qual_bam\"\n)\noutput_dir &lt;- \"/data/users/dell/mpra/link_barcode_to_cre/final_result\"\n\nfiles &lt;- list.files(input_dirs, pattern = \"\\\\.tsv$\", recursive = FALSE, full.names = TRUE)\ncre_bc_df &lt;- tibble()\nfor (file in files) {\n    cre_bc_df &lt;- bind_rows(\n        cre_bc_df,\n        vroom(file) %&gt;% select(cre, barcode)\n    )\n}\ncre_bc_df &lt;- distinct(cre_bc_df)\none_cre_barocdes &lt;- cre_bc_df %&gt;%\n    group_by(barcode) %&gt;%\n    count() %&gt;%\n    filter(n == 1) %&gt;%\n    pull(barcode) %&gt;%\n    unique()\none_cre_bc_df &lt;- cre_bc_df %&gt;%\n    filter(barcode %in% one_cre_barocdes) %&gt;%\n    distinct()\n\nvroom_write(cre_bc_df, file = file.path(output_dir, \"redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)\nvroom_write(one_cre_bc_df, file = file.path(output_dir, \"non_redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html#link-barcodes-to-cres",
    "href": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html#link-barcodes-to-cres",
    "title": "MPRA analysis pipeline",
    "section": "",
    "text": "Before running any of the following steps, you should rename your FASTQ files according to these rules.\n\nwork_dir = \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230\"\n\nsetwd(work_dir)\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n\n\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n\n\n\n\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nclean_fastq_dir = \"clean_fastq\"\n\nmkpath(clean_fastq_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(raw_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nYRUtils.BioUtils.trimgalore(files_dict, files_read_type, clean_fastq_dir;\n    trimgalore_options=\"--cores 4 --phred33 --quality 20 --length 30 --trim-n\",\n    num_jobs=1)\n\n\n\n\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nclean_fastqc_dir = \"clean_fastqc\"\n\nmkpath(clean_fastqc_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(clean_fastq_files, clean_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n\n\n\n\nWrite each reference sequence containing CRE as well as other necessary sequences and its ID into a FASTA file, which will be used later to build Bowtie2 reference index.\n\nusing CSV, DataFrames\n\nleft_seq = \"TTCTCTGGCCTAACTGTCTAGACCTGCAGGAGGACCGGATCAACT\"\nright_seq = \"CATTGCGTGAACCGACACTAGAGGGTATATAATGGAAGCTCGACTTCCAGCTTGGCAATCCGGTACTGTGCAAAGTGAACACATCGCTAAGCGAAAGCTAAGNNNNNNNNNNNNNNNACCGGTCGCCACCATGGTGAGCAAGG\"\nmpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.tsv\"\nmpra_ctl_file = \"ref/null_sequences.tsv\"\noutput_mpra_test_file = \"ref/2w_library.165bp.no_enzyme_cutting_sites.dealed.tsv\"\noutput_mpra_ctl_file = \"ref/null_sequences.dealed.tsv\"\noutput_ref_fa_file = \"ref/mpra_ref.fa\"\n\nmpra_test = CSV.read(mpra_test_file, DataFrame)\nmpra_test = unique(mpra_test)\nmpra_test = transform(mpra_test, \"PSCE\", \"extended_mm10_seq\" =&gt; (x -&gt; string.(left_seq, x, right_seq)) =&gt; \"attached_seq\")\nmpra_test = groupby(mpra_test, \"PSCE\")\nmpra_test = transform(mpra_test, nrow =&gt; \"num_per_PSCE\", eachindex =&gt; \"PSCE_sub_rank\")\nmpra_test = transform(mpra_test, [\"PSCE\", \"PSCE_sub_rank\", \"num_per_PSCE\"] =&gt; ByRow((x, y, z) -&gt; begin\n    if z == 1\n        x\n    else\n        string.(x, \"_\", y)\n    end\nend) =&gt; \"PSCE_new_id\")\n\nCSV.write(output_mpra_test_file, mpra_test; delim=\"\\t\", append=false)\n\nmpra_ctl = CSV.read(mpra_ctl_file, DataFrame)\nmpra_ctl = unique(mpra_ctl)\nmpra_ctl = transform(mpra_ctl, eachindex =&gt; \"rank\")\nmpra_ctl = transform(mpra_ctl, \"rank\" =&gt; (x -&gt; string.(\"CTL\", x)) =&gt; \"PSCE\", \"seq\" =&gt; (x -&gt; string.(left_seq, x, right_seq)) =&gt; \"attached_seq\")\n\nCSV.write(output_mpra_ctl_file, mpra_ctl; delim=\"\\t\", append=false)\n\nref_fa = vcat(string.(\"&gt;\", mpra_test[!, \"PSCE_new_id\"], \"\\n\", mpra_test[!, \"attached_seq\"]),\n    string.(\"&gt;\", mpra_ctl[!, \"PSCE\"], \"\\n\", mpra_ctl[!, \"attached_seq\"]))\n\nopen(output_ref_fa_file, \"w\") do io\n    for line in ref_fa\n        println(io, line)\n    end\nend\n\n\n\n\n\nusing YRUtils\n\nref_fa = \"ref/mpra_ref.fa\"\nbowtie2_index_dir = \"bowtie2_index\"\nbowtie2_index_prefix = \"mpra_ref\"\nbowtie2_n_threads = 40\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\n\nmkpath(bowtie2_index_dir)\nmkpath(log_dir)\nmkpath(tmp_dir)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    new_ref_fa = joinpath(tmp_dir, replace(basename(ref_fa), r\"\\.gz$\" =&gt; \"\"))\n    YRUtils.ShellUtils.pigz(ref_fa, new_ref_fa; decompress=true, keep=true)\nelse\n    new_ref_fa = ref_fa\nend\ncmd = pipeline(Cmd(string.([\"bowtie2-build\", \"--threads\", bowtie2_n_threads, \"-f\", new_ref_fa, joinpath(bowtie2_index_dir, bowtie2_index_prefix)]));\n    stdout=joinpath(log_dir, \"build_bowtie2_index.log\"),\n    stderr=joinpath(log_dir, \"build_bowtie2_index.log\"))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\nif !isnothing(match(r\"\\.gz$\", ref_fa))\n    rm(new_ref_fa)\nend\n\n\n\n\n\nusing YRUtils\n\nclean_fastq_dir = \"clean_fastq\"\nbam_dir = \"bam\"\ntmp_dir = \"tmp\"\nlog_dir = \"log\"\nbowtie2_n_threads = 40\nbowtie2_index = \"bowtie2_index/mpra_ref\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\n\nmkpath(bam_dir)\nmkpath(log_dir)\nclean_fastq_files = YRUtils.BaseUtils.list_files(clean_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\ndict = YRUtils.BioUtils.auto_detect_fastq_read_type(clean_fastq_files)\nfiles_dict = if dict[\"paired\"][\"status\"] == \"yes\"\n    dict[\"paired\"][\"dict\"]\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    dict[\"single\"][\"dict\"]\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nfiles_read_type = if dict[\"paired\"][\"status\"] == \"yes\"\n    \"paired\"\nelseif dict[\"single\"][\"status\"] == \"yes\"\n    \"single\"\nelse\n    @error \"did not detect any paired-end or single-end files\"\nend\nif files_read_type == \"paired\"\n    for sample in keys(files_dict)\n        for replicate in keys(files_dict[sample])\n            r1_fq_files = files_dict[sample][replicate][\"R1\"]\n            r2_fq_files = files_dict[sample][replicate][\"R2\"]\n            bam_file = joinpath(bam_dir, string(sample, \"_\", replicate, \".chr_srt.bam\"))\n\n            if length(r1_fq_files) &gt; 1\n                r1_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R1.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r1_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r1_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r1_fq_file = r1_fq_files[1]\n            end\n            if length(r2_fq_files) &gt; 1\n                r2_fq_file = joinpath(tmp_dir, string(sample, \"_\", replicate, \".R2.fq.gz\"))\n                cmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                    string(\"zcat -f \", join(r2_fq_files, \" \"),\n                        \" | pigz -n -c &gt; \",\n                        r2_fq_file)]))\n                @info string(\"running \", cmd, \" ...\")\n                run(cmd; wait=true)\n            else\n                r2_fq_file = r2_fq_files[1]\n            end\n\n            cmd = pipeline(\n                Cmd(\n                    string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n                        string(\"bowtie2 --np 0 -p \", bowtie2_n_threads, \" -x \", bowtie2_index, \" -1 \", r1_fq_file, \" -2 \", r2_fq_file,\n                            \" | samtools view -S -u - | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", bam_file)]),\n                );\n                stdout=joinpath(log_dir, \"bowtie2_align.log\"),\n                stderr=joinpath(log_dir, \"bowtie2_align.log\"),\n                append=true)\n            @info string(\"running \", cmd, \" ...\")\n            open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n                joinpath(log_dir, \"bowtie2_align.log\"), \"a\")\n            run(cmd; wait=true)\n        end\n    end\nend\n\ncmd = Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\", string(\"rm -rf \", joinpath(tmp_dir, \"*\"))]))\n@info string(\"running \", cmd, \" ...\")\nrun(cmd; wait=true)\n\n\n\n\n\nusing YRUtils\n\nbam_dir = \"bam\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nlog_dir = \"log\"\ntmp_dir = \"tmp\"\nsamtools_n_threads = 40\nsamtools_mem = \"768M\"\nmap_qual = 30\n\nmkpath(high_qual_bam_dir)\nbam_files = YRUtils.BaseUtils.list_files(bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    tmp_name_srt_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".name_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 -q \", map_qual, \" \", bam_file,\n                \" | samtools sort -n -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", tmp_name_srt_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    tmp_fixmate_bam_file = joinpath(tmp_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".fixmate.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools fixmate -@ \", samtools_n_threads, \" -r \", tmp_name_srt_bam_file, \" \", tmp_fixmate_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    filtered_bam_file = joinpath(high_qual_bam_dir, replace(basename(bam_file), r\"\\.\\w+\\.bam$\" =&gt; \".chr_srt.bam\"))\n    cmd = pipeline(Cmd(string.([\"/usr/bin/bash\", \"-e\", \"-c\",\n            string(\"samtools view -u -F 1804 -f 2 \", tmp_fixmate_bam_file,\n                \" | samtools sort -@ \", samtools_n_threads, \" -m \", samtools_mem, \" - -o \", filtered_bam_file)]));\n        stdout=joinpath(log_dir, \"reads_filter.log\"),\n        stderr=joinpath(log_dir, \"reads_filter.log\"),\n        append=true)\n    @info string(\"running \", cmd, \" ...\")\n    open(io -&gt; println(io, string(\"running \", cmd, \" ...\")),\n        joinpath(log_dir, \"reads_filter.log\"), \"a\")\n    run(cmd; wait=true)\n\n    rm.([tmp_name_srt_bam_file, tmp_fixmate_bam_file])\nend\n\n\n\n\n\nusing XAM, FASTX, CSV, DataFrames, YRUtils, Serialization\n\nfunction extract_cre_bc_pairs(bam_file::AbstractString, ref_dict::Dict{String,String};\n    barcode_length::Int=15, quality_scheme::Int=33)\n    valid_dna_bases = (\"A\", \"T\", \"C\", \"G\")\n\n    total_num_records = 0\n    xn_num_records = 0\n    complete_xn_num_records = 0\n    valid_xn_num_records = 0\n    typical_aln_vec_dict = Dict{String,Vector{String}}()\n\n    cre_bc_vec = Tuple{String,String}[]\n    reader = open(BAM.Reader, bam_file)\n    record = BAM.Record()\n    while !eof(reader)\n        empty!(record)\n        read!(reader, record)\n        total_num_records += 1\n        # The optional field XN:i:&lt;N&gt; reports the number of ambiguous reference characters (e.g. N) overlapped by an alignment\n        if haskey(record, \"XN\") && record[\"XN\"] == barcode_length\n            xn_num_records += 1\n            ref_name = BAM.refname(record)\n            # The leftmost mapping position\n            # BAM is 0-based, while SAM is 1-based\n            # BAM.position() gets the 1-based leftmost mapping position of record\n            ref_pos = BAM.position(record)\n            ref_seq = ref_dict[ref_name]\n            cigar_str = BAM.cigar(record)\n            query_seq = string(BAM.sequence(record))\n            query_qual_char_seq = join(Char.(BAM.quality(record) .+ quality_scheme))\n\n            aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_seq, ref_pos; truncate_ref=false))\n            qual_aln_vec = collect(YRUtils.BioUtils.parse_cigar(cigar_str, ref_seq, query_qual_char_seq, ref_pos; truncate_ref=false))\n\n            ref_m = match(Regex(string(\"N{\", barcode_length, \"}\")), aln_vec[1])\n            if !isnothing(ref_m)\n                complete_xn_num_records += 1\n                extract_range = ref_m.offset:(ref_m.offset+barcode_length-1)\n                barcode_seq = aln_vec[2][extract_range]\n                barcode_qual_char_seq = qual_aln_vec[2][extract_range]\n                if all(split(barcode_seq, \"\") .∈ Ref(valid_dna_bases)) && all([Int(c) - quality_scheme for c in barcode_qual_char_seq] .&gt;= base_qual)\n                    valid_xn_num_records += 1\n                    push!(cre_bc_vec, (ref_name, barcode_seq))\n                    typical_aln_vec_dict[string(ref_name, \":\", barcode_seq)] = aln_vec\n                end\n            end\n        end\n    end\n    close(reader)\n\n    cre_bc_gdf = groupby(DataFrame(cre_bc_vec, [:cre, :barcode]), [:cre, :barcode])\n    uniq_cre_bc_df = sort(combine(cre_bc_gdf, nrow =&gt; \"num\", proprow =&gt; \"prop\"), :num, rev=true)\n\n    open(replace(bam_file, r\"\\.bam$\" =&gt; \".extract_cre_bc_pairs.log\"), \"w\") do io\n        println(io, string(\n            \"The number of records in total: \", total_num_records, \"\\n\",\n            \"The number of records with XN field: \", xn_num_records, \"\\n\",\n            \"The number of records with complete barcode: \", complete_xn_num_records, \"\\n\",\n            \"The number of records passing base and quality check: \", valid_xn_num_records, \"\\n\",\n            \"The number of records non-redundant: \", nrow(uniq_cre_bc_df)\n        ))\n    end\n\n    return [uniq_cre_bc_df, typical_aln_vec_dict]\nend\n\nref_file = \"ref/mpra_ref.fa\"\nhigh_qual_bam_dir = \"high_qual_bam\"\nbase_qual = 20\n\n# Read in reference sequences\nref_dict = FASTAReader(open(ref_file, \"r\")) do reader\n    dict = Dict{String,String}()\n    for record in reader\n        dict[identifier(record)] = sequence(record)\n    end\n    return dict\nend\n\nbam_files = YRUtils.BaseUtils.list_files(high_qual_bam_dir, r\"\\.bam$\", recursive=false, full_name=true)\nfor bam_file in bam_files\n    # Extract CRE-Barcode pairs\n    cre_bc_res = extract_cre_bc_pairs(bam_file, ref_dict)\n\n    CSV.write(replace(bam_file, r\"\\.bam$\" =&gt; \".uniq_cre_bc_pairs.tsv\"),\n        cre_bc_res[1]; delim=\"\\t\", append=false, writeheader=true)\n\n    # obj = open(jls_file, \"r\") do io\n    #     deserialize(io)\n    # end\n    open(replace(bam_file, r\"\\.bam$\" =&gt; \".typical_cre_bc_aligned_sequences.jls\"), \"w\") do io\n        serialize(io, cre_bc_res[2])\n    end\n\n    rand_keys = rand(keys(cre_bc_res[2]), 100)\n    rand_dict = Dict(k =&gt; cre_bc_res[2][k] for k in rand_keys)\n    YRUtils.BioUtils.show_align(rand_dict,\n        replace(bam_file, r\"\\.bam$\" =&gt; \".typical_cre_bc_aligned_sequences.100.html\");\n        wrap_width=120)\nend\n\n\n\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(YRUtils)\nlibrary(ggprism)\n\ninput_dir &lt;- \"high_qual_bam\"\n\ncre_bc_files &lt;- list.files(input_dir, pattern = \"\\\\.tsv$\", full.names = TRUE, recursive = FALSE)\nfor (cre_bc_file in cre_bc_files) {\n    cre_bc_df &lt;- vroom(cre_bc_file) %&gt;%\n        select(cre, barcode) %&gt;%\n        distinct()\n\n    cre_count_df &lt;- count(cre_bc_df, cre) %&gt;%\n        mutate(type = if_else(str_detect(cre, \"^CTL\"), \"CTL\", \"CRE\")) %&gt;%\n        rename(cre_bc = cre)\n    bc_count_df &lt;- count(cre_bc_df, barcode) %&gt;%\n        mutate(type = \"BC\") %&gt;%\n        rename(cre_bc = barcode)\n    count_df &lt;- bind_rows(cre_count_df, bc_count_df)\n\n    cre_quantiles &lt;- quantile(count_df$n[count_df$type %in% c(\"CTL\", \"CRE\")], probs = seq(0, 1, 0.1))\n    cre_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CRE\"], probs = seq(0, 1, 0.1))\n    ctl_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CTL\"], probs = seq(0, 1, 0.1))\n    bc_quantiles &lt;- quantile(count_df$n[count_df$type == \"BC\"], probs = seq(0, 1, 0.1))\n    type_nums &lt;- table(count_df$type)\n\n    paste0(\n        \"1. CRE/CTL quantiles (\", type_nums[\"CTL\"] + type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_quantiles), \"\\t\", cre_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"2. CRE only quantiles (\", type_nums[\"CRE\"], \"): \\n\",\n        paste0(paste0(names(cre_only_quantiles), \"\\t\", cre_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"3. CTL only quantiles (\", type_nums[\"CTL\"], \"): \\n\",\n        paste0(paste0(names(ctl_only_quantiles), \"\\t\", ctl_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n        \"4. BC quantiles (\", type_nums[\"BC\"], \"): \\n\",\n        paste0(paste0(names(bc_quantiles), \"\\t\", bc_quantiles), collapse = \"\\n\")\n    ) %&gt;% vroom_write_lines(file = gsub(\"\\\\.tsv$\", \"\\\\.quantiles.txt\", cre_bc_file))\n\n    p &lt;- ggplot(count_df, aes(type, log2(n), fill = type, color = type)) +\n        geom_violin(scale = \"width\", alpha = 0.25, trim = TRUE) +\n        geom_boxplot(width = 0.2, outliers = FALSE, alpha = 0.25) +\n        scale_y_continuous(expand = expansion(mult = c(0.05, 0))) +\n        labs(\n            x = \"Sequence Type\",\n            y = \"log2(Count)\"\n        ) +\n        theme_prism(base_size = 20, base_family = \"Arial\", border = FALSE) +\n        theme(legend.position = \"none\")\n    ppreview(p, file = gsub(\"\\\\.tsv$\", \"\\\\.violin.pdf\", cre_bc_file))\n}\n\n\n\nlibrary(vroom)\nlibrary(tidyverse)\n\ninput_dirs &lt;- c(\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20241230/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/enzyme_v20231027/high_qual_bam\",\n    \"/data/users/dell/mpra/link_barcode_to_cre/pcr_v20230922/high_qual_bam\"\n)\noutput_dir &lt;- \"/data/users/dell/mpra/link_barcode_to_cre/final_result\"\n\nfiles &lt;- list.files(input_dirs, pattern = \"\\\\.tsv$\", recursive = FALSE, full.names = TRUE)\ncre_bc_df &lt;- tibble()\nfor (file in files) {\n    cre_bc_df &lt;- bind_rows(\n        cre_bc_df,\n        vroom(file) %&gt;% select(cre, barcode)\n    )\n}\ncre_bc_df &lt;- distinct(cre_bc_df)\none_cre_barocdes &lt;- cre_bc_df %&gt;%\n    group_by(barcode) %&gt;%\n    count() %&gt;%\n    filter(n == 1) %&gt;%\n    pull(barcode) %&gt;%\n    unique()\none_cre_bc_df &lt;- cre_bc_df %&gt;%\n    filter(barcode %in% one_cre_barocdes) %&gt;%\n    distinct()\n\nvroom_write(cre_bc_df, file = file.path(output_dir, \"redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)\nvroom_write(one_cre_bc_df, file = file.path(output_dir, \"non_redundant_cre_bc_pairs.tsv\"), col_names = TRUE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html#count-barcodes",
    "href": "Blogs/Bioinformatics/posts/MPRA/mpra_analysis_pipeline/index.html#count-barcodes",
    "title": "MPRA analysis pipeline",
    "section": "2 Count barcodes",
    "text": "2 Count barcodes\nBefore running any of the following steps, you should rename your FASTQ files in this form: ID_(RNA|DNA)_repN[_partN].R[123].(fq|fastq).gz (ID can only contain [a-zA-Z0-9]; N can only contain [0-9]).\n\nwork_dir = \"/data/users/dell/mpra/count_barcode/15bp_v20240627\"\n\ncd(work_dir)\n\nwork_dir &lt;- \"/data/users/dell/mpra/count_barcode/15bp_v20240627\"\n\nsetwd(work_dir)\n\n2.1 MD5SUM check over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nmd5_file = \"md5.txt\"\nmd5_check_file = \"md5_check.txt\"\n\ncd(raw_fastq_dir)\nYRUtils.BaseUtils.md5_check(md5_file, md5_check_file)\ncd(work_dir)\n\n\n\n2.2 FASTQC over raw FASTQ files\n\nusing YRUtils\n\nraw_fastq_dir = \"raw_fastq\"\nraw_fastqc_dir = \"raw_fastqc\"\n\nmkpath(raw_fastqc_dir)\nraw_fastq_files = YRUtils.BaseUtils.list_files(raw_fastq_dir, r\"\\.(fastq|fq)\\.gz$\", recursive=false, full_name=true)\nYRUtils.BioUtils.fastqc(raw_fastq_files, raw_fastqc_dir;\n    fastqc_options=\"--threads 4\", multiqc_options=\"--zip-data-dir\", num_jobs=4)\n\n\n\n2.3 Count barcodes\n\nusing FASTX, DataFrames, CSV, YRUtils, CodecZlib\n\nraw_fastq_dir = \"/data/users/dell/mpra/count_barcode/15bp_v20240627/raw_fastq\"\ncre_bc_file = \"/data/users/dell/mpra/link_barcode_to_cre/final_result/redundant_cre_bc_pairs.tsv\"\nraw_bc_umi_dir = \"raw_bc_umi\"\nbase_qual = 20\nseqkit_nthreads = 40\n\nmkpath(raw_bc_umi_dir)\ncre_bc_df = CSV.read(cre_bc_file, DataFrame; header=true, delim=\"\\t\")\nuniq_barcodes = Set(cre_bc_df[:, :barcode])\n\nraw_fastq_file_name_pattern = r\".+/(?&lt;id&gt;[a-zA-Z0-9]+)_(?&lt;type&gt;RNA|DNA)_(?&lt;rep&gt;rep[0-9]+)(_(?&lt;tech&gt;part[0-9]+))?\\.(?&lt;read&gt;R[123])\\.(fq|fastq)\\.gz$\"\nraw_fastq_files = YRUtils.BioUtils.list_files(raw_fastq_dir, raw_fastq_file_name_pattern; recursive=false, full_name=true)\nraw_fastq_nums = YRUtils.BioUtils.fq_num(raw_fastq_files, seqkit_nthreads)\nms = match.(raw_fastq_file_name_pattern, raw_fastq_files)\nmetadata_vec = Vector{NTuple{7,String}}(undef, length(ms))\nfor i in 1:length(ms)\n    metadata_vec[i] = (ms[i].match, ms[i][\"id\"], ms[i][\"type\"], ms[i][\"rep\"], ms[i][\"tech\"], ms[i][\"read\"], string(raw_fastq_nums[ms[i].match]))\nend\ndf = unique(DataFrame(metadata_vec, [:file, :id, :type, :rep, :tech, :read, :num_seqs]))\ndf = transform(\n    df,\n    [:id, :type, :rep, :tech] =&gt; ByRow((id, type, rep, tech) -&gt; join([id, type, rep, tech], \"_\")) =&gt; :tech_sample,\n    [:id, :type, :rep] =&gt; ByRow((id, type, rep) -&gt; join([id, type, rep], \"_\")) =&gt; :rep_sample,\n    [:id, :type] =&gt; ByRow((id, type) -&gt; join([id, type], \"_\")) =&gt; :type_sample\n)\ntech_gdf = groupby(df, :tech_sample)\nCSV.write(joinpath(raw_bc_umi_dir, \"fq_metadata.tsv\"), df; header=true, delim=\"\\t\", append=false)\n\ndf_dict = Dict(unique(read_df[:, :tech_sample])[1] =&gt; DataFrame() for read_df in tech_gdf)\nfor read_df in tech_gdf\n    # Read in reads\n    read_dict = Dict(read_type =&gt; Vector{Tuple{String,String,String}}(undef, parse(Int64, num_seqs)) for (read_type, num_seqs) in collect(zip(read_df[:, :read], read_df[:, :num_seqs])))\n    Threads.@threads for (read_type, num_seqs, fq_file) in collect(zip(read_df[:, :read], read_df[:, :num_seqs], read_df[:, :file]))\n        @info string(\"start parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" ...\")\n        FASTQReader(GzipDecompressorStream(open(fq_file))) do reader\n            record = FASTQ.Record()\n            i = 0\n            while !eof(reader)\n                i += 1\n                empty!(record)\n                read!(reader, record)\n                read_dict[read_type][i] = (identifier(record), sequence(record), join(collect(quality_scores(record)), \"/\"))\n            end\n            @info string(\"read in \", i, \" sequences in total for \", fq_file)\n            if i != parse(Int64, num_seqs)\n                @error string(\"parsing file \", fq_file, \" failed!\")\n            end\n        end\n        @info string(\"parsing \", fq_file, \" with read type \", read_type, \" and the number of sequences \", num_seqs, \" done!\")\n    end\n\n    # Count qualified barcodes and their UMIs\n    len_vec = length.(values(read_dict))\n    # The three files should have the same number of lines\n    if length(unique(len_vec)) == 1\n        bc_umi_vec = Vector{Tuple{String,String,String,Vararg{Bool,9}}}(undef, len_vec[1])\n        Threads.@threads for i in 1:len_vec[1]\n            # Read IDs should be identical across R1, R2, and R3\n            if read_dict[\"R1\"][i][1] == read_dict[\"R2\"][i][1] == read_dict[\"R3\"][i][1]\n                bc_umi_vec[i] = (\n                    # Read 1\n                    read_dict[\"R1\"][i][2],\n                    # Read 2\n                    read_dict[\"R2\"][i][2],\n                    # UMI\n                    read_dict[\"R3\"][i][2],\n                    # Read sequences should only contain A, T, C, and G across R1, R2, and R3\n                    !occursin(\"N\", string(read_dict[\"R1\"][i][2], read_dict[\"R2\"][i][2], read_dict[\"R3\"][i][2])),\n                    # All base qualities &gt;= base_qual across R1, R2, and R3\n                    all(parse.(Int, split(string(read_dict[\"R1\"][i][3], \"/\", read_dict[\"R2\"][i][3], \"/\", read_dict[\"R3\"][i][3]), \"/\")) .&gt;= base_qual),\n                    # Read 1 and read 2 should be reverse and complementary\n                    YRUtils.BioUtils.rev_com_dna_seq(read_dict[\"R1\"][i][2]) == read_dict[\"R2\"][i][2],\n                    # Either read 1 or read 2 should be in the barcode library (not both in theory)\n                    # Read 1 in the barcode library?\n                    read_dict[\"R1\"][i][2] in uniq_barcodes,\n                    # Read 2 in the barcode library?\n                    read_dict[\"R2\"][i][2] in uniq_barcodes,\n                    # The reverse sequence of read 1 in the barcode library?\n                    YRUtils.BioUtils.rev_seq(read_dict[\"R1\"][i][2]) in uniq_barcodes,\n                    # The complementary sequence of read 1 in the barcode library?\n                    YRUtils.BioUtils.com_dna_seq(read_dict[\"R1\"][i][2]) in uniq_barcodes,\n                    # The reverse sequence of read 2 in the barcode library?\n                    YRUtils.BioUtils.rev_seq(read_dict[\"R2\"][i][2]) in uniq_barcodes,\n                    # The complementary sequence of read 2 in the barcode library?\n                    YRUtils.BioUtils.com_dna_seq(read_dict[\"R2\"][i][2]) in uniq_barcodes\n                )\n            else\n                @error \"read IDs are not identical across R1, R2, and R3\"\n            end\n        end\n    else\n        @error \"length(R1) == length(R2) == length(R3) is not true\"\n    end\n\n    # Write statistics\n    col4, col5, col6, col7, col8, col9, col10, col11, col12 = (\n        getindex.(bc_umi_vec, 4),\n        getindex.(bc_umi_vec, 5),\n        getindex.(bc_umi_vec, 6),\n        getindex.(bc_umi_vec, 7),\n        getindex.(bc_umi_vec, 8),\n        getindex.(bc_umi_vec, 9),\n        getindex.(bc_umi_vec, 10),\n        getindex.(bc_umi_vec, 11),\n        getindex.(bc_umi_vec, 12)\n    )\n    open(joinpath(raw_bc_umi_dir, \"fq_read_stats.txt\"), \"a\") do io\n        stat_str = string(\n            \"==&gt; \", unique(read_df[:, :tech_sample])[1], \" &lt;==\\n\",\n            \"1. The number of reads in total: \", len_vec[1], \"\\n\\n\",\n            \"2. % reads without Ns (\", sum(col4), \"): \", sum(col4) / len_vec[1], \"\\n\",\n            \"3. % reads with base qualities &gt;= \", base_qual, \" (\", sum(col5), \"): \", sum(col5) / len_vec[1], \"\\n\",\n            \"4. % reads passing 2 and 3: (\", sum(col4 .&& col5), \"): \", sum(col4 .&& col5) / len_vec[1], \"\\n\\n\",\n            \"5. % reads (R1 and R2 are reverse and complementary) (\", sum(col6), \"): \", sum(col6) / len_vec[1], \"\\n\",\n            \"6. % reads passing 2, 3, and 5 (\", sum(col4 .&& col5 .&& col6), \"): \", sum(col4 .&& col5 .&& col6) / len_vec[1], \"\\n\\n\",\n            \"7. % reads of R1 in the library (\", sum(col7), \"): \", sum(col7) / len_vec[1], \"\\n\",\n            \"8. % reads passing 2, 3, 5 and 7 (\", sum(col4 .&& col5 .&& col6 .&& col7), \"): \", sum(col4 .&& col5 .&& col6 .&& col7) / len_vec[1], \"\\n\\n\",\n            \"9. % reads of R2 in the library (\", sum(col8), \"): \", sum(col8) / len_vec[1], \"\\n\",\n            \"10. % reads passing 2, 3, 5 and 9 (\", sum(col4 .&& col5 .&& col6 .&& col8), \"): \", sum(col4 .&& col5 .&& col6 .&& col8) / len_vec[1], \"\\n\\n\",\n            \"11. % reads (both R1 and R2 are in the library) (\", sum(col7 .&& col8), \"): \", sum(col7 .&& col8) / len_vec[1], \"\\n\",\n            \"12. % reads passing 2, 3, 5 and 11 (\", sum(col4 .&& col5 .&& col6 .&& col7 .&& col8), \"): \", sum(col4 .&& col5 .&& col6 .&& col7 .&& col8) / len_vec[1], \"\\n\\n\",\n            \"13. % reads (one of R1 and R2 in the library, not both) (\", sum(col7 .⊻ col8), \"): \", sum(col7 .⊻ col8) / len_vec[1], \"\\n\",\n            \"14. % reads passing 2, 3, 5 and 13 (\", sum(col4 .&& col5 .&& col6 .&& (col7 .⊻ col8)), \"): \", sum(col4 .&& col5 .&& col6 .&& (col7 .⊻ col8)) / len_vec[1], \"\\n\\n\",\n            \"15. % reverse reads of R1 in the library (\", sum(col9), \"): \", sum(col9) / len_vec[1], \"\\n\",\n            \"16. % reads passing 2, 3, 5 and 15 (\", sum(col4 .&& col5 .&& col6 .&& col9), \"): \", sum(col4 .&& col5 .&& col6 .&& col9) / len_vec[1], \"\\n\\n\",\n            \"17. % complementary reads of R1 in the library (\", sum(col10), \"): \", sum(col10) / len_vec[1], \"\\n\",\n            \"18. % reads passing 2, 3, 5 and 17 (\", sum(col4 .&& col5 .&& col6 .&& col10), \"): \", sum(col4 .&& col5 .&& col6 .&& col10) / len_vec[1], \"\\n\\n\",\n            \"19. % reverse reads of R2 in the library (\", sum(col11), \"): \", sum(col11) / len_vec[1], \"\\n\",\n            \"20. % reads passing 2, 3, 5 and 19 (\", sum(col4 .&& col5 .&& col6 .&& col11), \"): \", sum(col4 .&& col5 .&& col6 .&& col11) / len_vec[1], \"\\n\\n\",\n            \"21. % complementary reads of R2 in the library (\", sum(col12), \"): \", sum(col12) / len_vec[1], \"\\n\",\n            \"22. % reads passing 2, 3, 5 and 21 (\", sum(col4 .&& col5 .&& col6 .&& col12), \"): \", sum(col4 .&& col5 .&& col6 .&& col12) / len_vec[1], \"\\n\\n\\n\"\n        )\n        print(io, stat_str)\n    end\n\n    # Filter out invalid Barcode-UMI pairs\n    valid_bc_umi_vec = bc_umi_vec[col4.&&col5.&&col6.&&(col7.⊻col8)]\n    df_dict[unique(read_df[:, :tech_sample])[1]] = DataFrame(\n        [[(x[1], x[3]) for x in valid_bc_umi_vec[getindex.(valid_bc_umi_vec, 7)]]; [(x[2], x[3]) for x in valid_bc_umi_vec[getindex.(valid_bc_umi_vec, 8)]]],\n        [:barcode, :umi]\n    )\n    df_dict[unique(read_df[:, :tech_sample])[1]][!, :tech_sample] .= unique(read_df[:, :tech_sample])[1]\nend\nbc_umi_df = reduce(vcat, collect(values(df_dict)); cols=:setequal)\nCSV.write(joinpath(raw_bc_umi_dir, \"raw_bc_umi_pairs.tsv\"), bc_umi_df; header=true, delim=\"\\t\", append=false)\n\n\n\n2.4 Attach barcodes to CREs\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggprism)\nlibrary(YRUtils)\n\nraw_bc_umi_pairs_file &lt;- \"raw_bc_umi/raw_bc_umi_pairs.tsv\"\n# Here, we only keep those CRE-Barcode pairs, where each barcode is assigned to only one CRE\ncre_bc_file &lt;- \"/data/users/dell/mpra/link_barcode_to_cre/final_result/non_redundant_cre_bc_pairs.tsv\"\noutput_dir &lt;- \"cre_bc_count\"\n\ndir.create(output_dir)\ncre_bc_df &lt;- vroom(cre_bc_file)\nraw_bc_umi_pairs_df &lt;- vroom(raw_bc_umi_pairs_file)\n\n# Count barcodes based on UMIs\nbiorep_bc_count_df &lt;- raw_bc_umi_pairs_df %&gt;%\n    mutate(rep_sample = gsub(\"_part[0-9]+$\", \"\", tech_sample)) %&gt;%\n    select(-all_of(c(\"tech_sample\"))) %&gt;%\n    distinct() %&gt;%\n    group_by(rep_sample, barcode) %&gt;%\n    # Count the number of occurrences of each barcode in each biological replicate\n    count(name = \"barcode_count\") %&gt;%\n    ungroup() %&gt;%\n    group_by(rep_sample) %&gt;%\n    arrange(desc(barcode_count), .by_group = TRUE) %&gt;%\n    ungroup()\nvroom_write(biorep_bc_count_df, file = file.path(output_dir, \"biorep_bc_count.tsv\"))\n\n# Attach CREs to barcodes\nbiorep_cre_bc_count_df &lt;- biorep_bc_count_df %&gt;%\n    inner_join(cre_bc_df, by = \"barcode\", relationship = \"many-to-many\") %&gt;%\n    group_by(rep_sample) %&gt;%\n    arrange(desc(barcode_count), .by_group = TRUE) %&gt;%\n    ungroup()\nvroom_write(biorep_cre_bc_count_df, file = file.path(output_dir, \"biorep_cre_bc_count.tsv\"))\n\n# Count the number of unique barcodes detected in each biological replicate\nbc_num_each_biorep &lt;- biorep_bc_count_df %&gt;%\n    group_by(rep_sample) %&gt;%\n    count(name = \"barcode_num\") %&gt;%\n    ungroup()\nvroom_write(bc_num_each_biorep, file = file.path(output_dir, \"bc_num_each_biorep.tsv\"))\n\n# The distribution of the number of barcodes belonging to each CRE\n# The distribution of the number of CREs belonging to each barcode\nleft_cre_bc_df &lt;- biorep_cre_bc_count_df %&gt;%\n    select(barcode, cre) %&gt;%\n    distinct()\nbc_count_df &lt;- left_cre_bc_df %&gt;%\n    group_by(barcode) %&gt;%\n    count() %&gt;%\n    rename(cre_bc = barcode) %&gt;%\n    mutate(type = \"BC\")\ncre_count_df &lt;- left_cre_bc_df %&gt;%\n    group_by(cre) %&gt;%\n    count() %&gt;%\n    rename(cre_bc = cre) %&gt;%\n    mutate(type = if_else(str_detect(cre_bc, \"^CTL\"), \"CTL\", \"CRE\"))\ncount_df &lt;- bind_rows(cre_count_df, bc_count_df)\n\ncre_quantiles &lt;- quantile(count_df$n[count_df$type %in% c(\"CTL\", \"CRE\")], probs = seq(0, 1, 0.1))\ncre_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CRE\"], probs = seq(0, 1, 0.1))\nctl_only_quantiles &lt;- quantile(count_df$n[count_df$type == \"CTL\"], probs = seq(0, 1, 0.1))\nbc_quantiles &lt;- quantile(count_df$n[count_df$type == \"BC\"], probs = seq(0, 1, 0.1))\ntype_nums &lt;- table(count_df$type)\n\npaste0(\n    \"1. CRE/CTL quantiles (\", type_nums[\"CTL\"] + type_nums[\"CRE\"], \"): \\n\",\n    paste0(paste0(names(cre_quantiles), \"\\t\", cre_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"2. CRE only quantiles (\", type_nums[\"CRE\"], \"): \\n\",\n    paste0(paste0(names(cre_only_quantiles), \"\\t\", cre_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"3. CTL only quantiles (\", type_nums[\"CTL\"], \"): \\n\",\n    paste0(paste0(names(ctl_only_quantiles), \"\\t\", ctl_only_quantiles), collapse = \"\\n\"), \"\\n\\n\",\n    \"4. BC quantiles (\", type_nums[\"BC\"], \"): \\n\",\n    paste0(paste0(names(bc_quantiles), \"\\t\", bc_quantiles), collapse = \"\\n\")\n) %&gt;% vroom_write_lines(file = file.path(output_dir, \"quantiles.txt\"))\n\np &lt;- ggplot(count_df, aes(type, log2(n), fill = type, color = type)) +\n    geom_violin(scale = \"width\", alpha = 0.25, trim = TRUE) +\n    geom_boxplot(width = 0.2, outliers = FALSE, alpha = 0.25) +\n    scale_y_continuous(expand = expansion(mult = c(0.05, 0))) +\n    labs(\n        x = \"Sequence Type\",\n        y = \"log2(Count)\"\n    ) +\n    theme_prism(base_size = 20, base_family = \"Arial\", border = FALSE) +\n    theme(legend.position = \"none\")\nppreview(p, file = file.path(output_dir, \"violin.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html",
    "title": "Build ENCODE ATAC/ChIP-Seq pipeline indices",
    "section": "",
    "text": "For more details, refer to ENCODE ATAC-Seq pipeline from GitHub, ENCODE ATAC-Seq pipeline from Google doc, ENCODE ChIP-Seq pipeline from GitHub, and ENCODE ChIP-Seq pipeline from Google doc."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#introduction",
    "title": "Build ENCODE ATAC/ChIP-Seq pipeline indices",
    "section": "",
    "text": "For more details, refer to ENCODE ATAC-Seq pipeline from GitHub, ENCODE ATAC-Seq pipeline from Google doc, ENCODE ChIP-Seq pipeline from GitHub, and ENCODE ChIP-Seq pipeline from Google doc."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#extract-mitochondrial-genome-if-present",
    "href": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#extract-mitochondrial-genome-if-present",
    "title": "Build ENCODE ATAC/ChIP-Seq pipeline indices",
    "section": "2 Extract mitochondrial genome if present",
    "text": "2 Extract mitochondrial genome if present\n\npigz -k -c -d Rattus_norvegicus.mRatBN7.2.dna_sm.toplevel.fa.gz | faOneRecord stdin MT &gt; Rattus_norvegicus.mRatBN7.2.dna_sm.toplevel.mito_only.fa"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#prepare-bowtie2-index-of-speciesmitochondrial-genome",
    "href": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#prepare-bowtie2-index-of-speciesmitochondrial-genome",
    "title": "Build ENCODE ATAC/ChIP-Seq pipeline indices",
    "section": "3 Prepare Bowtie2 index of species/mitochondrial genome",
    "text": "3 Prepare Bowtie2 index of species/mitochondrial genome\n\n#!/usr/bin/bash\n\nref_fa=/data/biodatabase/species/sugar_glider_slb_v1/encode_references/bulk_atac_chip_seq/sugarglider.mito_only.fasta.gz\nbowtie2_index_dir=/data/biodatabase/species/sugar_glider_slb_v1/encode_references/bulk_atac_chip_seq\nbowtie2_index_prefix=mito\nbowtie2_n_threads=60\ntmp_dir=/data/tmp\n\ncd ${bowtie2_index_dir}\n\nif [ ${ref_fa##*.} == \"gz\" ]\nthen\n    new_ref_fa=${tmp_dir}/$(basename ${ref_fa} .gz)\n    pigz -k -c -d ${ref_fa} &gt; ${new_ref_fa}\nelse\n    new_ref_fa=${ref_fa}\nfi\n\nbowtie2-build --threads ${bowtie2_n_threads} -f ${new_ref_fa} ${bowtie2_index_prefix}\n\ntar -cvf ${bowtie2_index_prefix}.bt2_index.tar *.bt2\n\nrm *.bt2\n\nif [ ${ref_fa##*.} == \"gz\" ]\nthen\n    rm ${new_ref_fa}\nfi"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#prepare-transcription-start-sites",
    "href": "Blogs/Bioinformatics/posts/bulk ATAC&ChIP-Seq/build_encode_atac_chip_seq_pipeline_indices/index.html#prepare-transcription-start-sites",
    "title": "Build ENCODE ATAC/ChIP-Seq pipeline indices",
    "section": "4 Prepare transcription start sites",
    "text": "4 Prepare transcription start sites\nNote: GFF3 is 1-based coordinate system, and BED is 0-based coordinate system.\n\nlibrary(rtracklayer)\nlibrary(vroom)\nlibrary(tidyverse)\n\ngff_file &lt;- \"/data/biodatabase/species/mRatBN7/genome/anno/Rattus_norvegicus.mRatBN7.2.111.gff3.gz\"\noutput_file &lt;- \"/data/biodatabase/species/mRatBN7/encode_references/bulk_atac_chip_seq/mRatBN7.tss.bed\"\ntarget &lt;- \"gene\"\n\ndf &lt;- import(gff_file, format = \"gff3\") %&gt;%\n    as.data.frame() %&gt;%\n    as_tibble()\ndf &lt;- df %&gt;%\n    filter(type %in% target) %&gt;%\n    select(seqnames, start, end, strand) %&gt;%\n    mutate(\n        tss_start = if_else(strand %in% c(\"+\", \".\", \"*\"), start - 1, end - 1),\n        tss_end = if_else(strand %in% c(\"+\", \".\", \"*\"), start, end)\n    ) %&gt;%\n    select(seqnames, tss_start, tss_end) %&gt;%\n    arrange(seqnames, tss_start, tss_end) %&gt;%\n    distinct()\n\nvroom_write(df, file = output_file, col_names = FALSE, append = FALSE)\nsystem2(\"/usr/bin/pigz\", output_file)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "",
    "text": "For more details, refer to ENCODE RNA-Seq pipeline."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#introduction",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "",
    "text": "For more details, refer to ENCODE RNA-Seq pipeline."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#prepare-necessary-files",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#prepare-necessary-files",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "2 Prepare necessary files",
    "text": "2 Prepare necessary files\n\n2.1 Convert GFF to GTF\n\ngffread /data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gff3 -T -o /data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gtf\n\n\n\n2.2 Merge annotations if needed\n\n2.2.1 Prepare input JSON file\n\n{\n    \"merge_anno.annotation\": \"gencode.v29.primary_assembly.annotation_UCSC_names.gtf.gz\",\n    \"merge_anno.tRNA\": \"gencode.v29.tRNAs.gtf.gz\",\n    \"merge_anno.spikeins\": \"ERCC_phiX.fa.gz\",\n    \"merge_anno.output_filename\": \"merged_annotation_V29.gtf.gz\"\n}\n\n\n\n2.2.2 Merge annotations\n\ncaper run /data/softwares/encode_pipeline/rna-seq-pipeline_v1.2.4/make_index_wdl/merge_anno.wdl -c /data/softwares/encode_pipeline/caper/local.conf -i merge_anno_input.json --max-concurrent-tasks 2 --singularity"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#build-star-index",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#build-star-index",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "3 Build STAR index",
    "text": "3 Build STAR index\n\n3.1 Prepare input JSON file\n\n{\n    \"build_index.reference_sequence\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/genome/sugarglider.fasta.gz\",\n    \"build_index.spikeins\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/genome/ERCC_phix.spikeins.fasta.gz\",\n    \"build_index.annotation\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gtf.gz\",\n    \"build_index.anno_version\": \"v1\",\n    \"build_index.genome\": \"sugarglider\",\n    \"build_index.index_type\": \"prep_star\",\n    \"build_index.ncpu\": 60,\n    \"build_index.memGB\": 512\n}\n\n\n\n3.2 Build STAR index\n\ncaper run /data/softwares/encode_pipeline/rna-seq-pipeline_v1.2.4/make_index_wdl/build_genome_index.wdl -c /data/softwares/encode_pipeline/caper/local.conf -i /data/biodatabase/species/sugar_glider_slb_v1/encode_references/bulk_rna_seq/star_index_input.json --max-concurrent-tasks 1 --singularity"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#build-rsem-index",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#build-rsem-index",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "4 Build RSEM index",
    "text": "4 Build RSEM index\n\n4.1 Prepare input JSON file\n\n{\n    \"build_index.reference_sequence\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/genome/sugarglider.fasta.gz\",\n    \"build_index.spikeins\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/genome/ERCC_phix.spikeins.fasta.gz\",\n    \"build_index.annotation\": \"/data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gtf.gz\",\n    \"build_index.anno_version\": \"v1\",\n    \"build_index.genome\": \"sugarglider\",\n    \"build_index.index_type\": \"prep_rsem\",\n    \"build_index.ncpu\": 60,\n    \"build_index.memGB\": 512\n}\n\n\n\n4.2 Build RSEM index\n\ncaper run /data/softwares/encode_pipeline/rna-seq-pipeline_v1.2.4/make_index_wdl/build_genome_index.wdl -c /data/softwares/encode_pipeline/caper/local.conf -i /data/biodatabase/species/sugar_glider_slb_v1/encode_references/bulk_rna_seq/rsem_index_input.json --max-concurrent-tasks 1 --singularity"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#prepare-other-files",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/build_encode_rna_seq_pipeline_indices/index.html#prepare-other-files",
    "title": "Build ENCODE RNA-Seq pipeline indices",
    "section": "5 Prepare other files",
    "text": "5 Prepare other files\nIn most cases, you need to modify the following code to suit yourself.\n\n5.1 Prepare transcript IDs to gene biotypes mapping table\nIn this example, due to the lack of gene biotype field, we assign all transcripts with the gene biotype “protein_coding”.\n\nlibrary(rtracklayer)\nlibrary(tidyverse)\nlibrary(vroom)\n\ngtf_file &lt;- \"/data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gtf.gz\"\noutput_file &lt;- \"/data/biodatabase/species/sugar_glider_slb_v1/encode_references/bulk_rna_seq/sugarglider.v1.transcript_id_to_gene_type.tsv\"\ntranscript_field &lt;- \"transcript\"\ntranscript_type &lt;- \"protein_coding\"\n\ndf &lt;- as.data.frame(rtracklayer::import(gtf_file, format = \"gtf\"))\ndf &lt;- df %&gt;%\n    filter(type == transcript_field) %&gt;%\n    mutate(transcript_type = transcript_type) %&gt;%\n    select(transcript_id, transcript_type) %&gt;%\n    arrange(transcript_id) %&gt;%\n    distinct()\nvroom_write(df, file = output_file, col_names = FALSE, append = FALSE)\n\n\n\n5.2 Prepare gene/transcript IDs to names mapping table\n\nlibrary(YRUtils)\nlibrary(vroom)\n\ngff_file &lt;- \"/data/biodatabase/species/sugar_glider_slb_v1/genome/anno/sugarglider.gff3.gz\"\ntarget_type &lt;- \"transcript\"\n\ndf &lt;- extract_gene_id_from_gff(gff_file, target_type = target_type)\nvroom_write(df, file = paste0(gff_file, \".\", target_type, \"_id_name_mapping_table.tsv\"), col_names = TRUE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "",
    "text": "In scRNA-seq, various tools, such as Monocle3, provide the capability of performing pseudotime analysis. In brief, assume that there are both progenitors and more differentiated progenies in an scRNA-seq dataset. If we consider the most undeferentiated progenitors as the developmental origin (assigning them the number \\(0\\)), and the most differentiated progenies the developmental ends (assigning them the number 10), then we can assign each intermediate cell within them a number between \\(0\\) and \\(10\\). For cells with numbers approaching \\(0\\) more, they are more similar with the progenitors in terms of their RNA expression patterns and vice versa. Once we assign each cell a number (i.e. a developmental pseudotime point) and order them based on their pseudotime, we can arrange highly variable genes based on their peaking expression patterns (i.e. genes with peaking expression patterns at early stages are placed at the left, etc.).\nIn bulk RNA-seq, the number of samples is far less than the number of cells in scRNA-seq, where each cell can be regarded as a sample, so the gene expression dynamics along the developmental stages are not so smooth (i.e. jagged) as we have seen in scRNA-seq if we do the same analysis in bulk RNA-seq as in scRNA-seq. Therefore, to make the gene expression dynamics smoother along the developmental stages, we need to obtain more pseudo/interpolated time points than those we have.\nBriefly, to achieve this goal, we need to do the following things:\n\nDefine the time scale among developmental samples based on their mutual Euclidean distances calculated from their coordinates (Dim.1, Dim.2) obtained from their PCA space (i.e. consider the earliest sample as the developmental origin, assign it \\(0\\), and for the remaining samples, use their Euclidean disntances accumulated from the origin as their developmental time points).\nScale the time scale to (0, 10).\nFit a spline for each gene based its \\((time, expression)\\) pairs along the actual developmental stages, and use this fitted spline to interpolate more \\((time, expression)\\) pairs (using the loess method in modelr package).\nFor each gene, obtain its PCA coordinate (Dim.1, Dim.2), and then feed all possible signed combinations of Dim.1 and Dim.2 of all genes to atan2 to get a sequence of values used to sort genes.\nVisualize gene expression dynamics along interpolated time points to pick the expected one."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html#introduction",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "",
    "text": "In scRNA-seq, various tools, such as Monocle3, provide the capability of performing pseudotime analysis. In brief, assume that there are both progenitors and more differentiated progenies in an scRNA-seq dataset. If we consider the most undeferentiated progenitors as the developmental origin (assigning them the number \\(0\\)), and the most differentiated progenies the developmental ends (assigning them the number 10), then we can assign each intermediate cell within them a number between \\(0\\) and \\(10\\). For cells with numbers approaching \\(0\\) more, they are more similar with the progenitors in terms of their RNA expression patterns and vice versa. Once we assign each cell a number (i.e. a developmental pseudotime point) and order them based on their pseudotime, we can arrange highly variable genes based on their peaking expression patterns (i.e. genes with peaking expression patterns at early stages are placed at the left, etc.).\nIn bulk RNA-seq, the number of samples is far less than the number of cells in scRNA-seq, where each cell can be regarded as a sample, so the gene expression dynamics along the developmental stages are not so smooth (i.e. jagged) as we have seen in scRNA-seq if we do the same analysis in bulk RNA-seq as in scRNA-seq. Therefore, to make the gene expression dynamics smoother along the developmental stages, we need to obtain more pseudo/interpolated time points than those we have.\nBriefly, to achieve this goal, we need to do the following things:\n\nDefine the time scale among developmental samples based on their mutual Euclidean distances calculated from their coordinates (Dim.1, Dim.2) obtained from their PCA space (i.e. consider the earliest sample as the developmental origin, assign it \\(0\\), and for the remaining samples, use their Euclidean disntances accumulated from the origin as their developmental time points).\nScale the time scale to (0, 10).\nFit a spline for each gene based its \\((time, expression)\\) pairs along the actual developmental stages, and use this fitted spline to interpolate more \\((time, expression)\\) pairs (using the loess method in modelr package).\nFor each gene, obtain its PCA coordinate (Dim.1, Dim.2), and then feed all possible signed combinations of Dim.1 and Dim.2 of all genes to atan2 to get a sequence of values used to sort genes.\nVisualize gene expression dynamics along interpolated time points to pick the expected one."
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html#pipeline",
    "href": "Blogs/Bioinformatics/posts/bulk RNA-Seq/bulk_rna-seq_pseudotime_analysis/index.html#pipeline",
    "title": "Bulk RNA-seq pseudotime analysis",
    "section": "2 Pipeline",
    "text": "2 Pipeline\n\nsuppressWarnings(suppressMessages(library(vroom)))\nsuppressWarnings(suppressMessages(library(tidyverse)))\nsuppressWarnings(suppressMessages(library(ggplot2)))\nsuppressWarnings(suppressMessages(library(ggrepel)))\nsuppressWarnings(suppressMessages(library(magrittr)))\nsuppressWarnings(suppressMessages(library(FactoMineR)))\nsuppressWarnings(suppressMessages(library(ComplexHeatmap)))\nsuppressWarnings(suppressMessages(library(scales)))\nsuppressWarnings(suppressMessages(library(modelr)))\nsuppressWarnings(suppressMessages(library(RColorBrewer)))\nsuppressWarnings(suppressMessages(library(patchwork)))\nsuppressWarnings(suppressMessages(library(showtext)))\n\n\nfont_family &lt;- \"Arial\"\n\nfont_df &lt;- filter(font_files(), family == font_family)\nfont_add(\n    family = font_family,\n    regular = if (\"Regular\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Regular\"] else stop(\"no font file found\"),\n    bold = if (\"Bold\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Bold\"] else NULL,\n    italic = if (\"Bold Italic\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Bold Italic\"] else NULL,\n    bolditalic = if (\"Italic\" %in% font_df[[\"face\"]]) font_df[[\"file\"]][font_df[[\"face\"]] == \"Italic\"] else NULL\n)\nshowtext_auto()\n\n\n# specify input gene expression matrix\n# containing one ID column named \"GeneID\"\n# the remaining columns are sample columns named in the form of \"SampleID.Replicate\" (e.g. Skin.1, Skin.2, etc.)\n# SampleID must not contain \".\"\n# Replicate must be one or more integers\nexpr_file &lt;- \"./data/RNA_TPM.txt\"\n# specify the sample levels, reflecting their actual developmental stages\nsample_dev_order &lt;- c(\"DAI0\", \"DAI3\", \"DAI6\", \"DAI9\", \"DAI12\")\ntime_points_num &lt;- 500\n\n\nexpr &lt;- vroom(expr_file) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"GeneID\"]]) %&gt;%\n    select(-all_of(\"GeneID\")) %&gt;%\n    distinct()\nsample_df &lt;- strsplit(names(expr), \".\", fixed = T) %&gt;%\n    do.call(rbind, .) %&gt;%\n    as.data.frame() %&gt;%\n    set_colnames(c(\"SampleID\", \"Replicate\")) %&gt;%\n    mutate(Sample = paste0(SampleID, \".\", Replicate))\n\n\n# calculate the mean expression value of each gene within each sample\ndata &lt;- data.frame(GeneID = row.names(expr))\nfor (id in unique(sample_df[[\"SampleID\"]])) {\n    id_reps &lt;- filter(sample_df, SampleID == id) %&gt;%\n        pull(Sample) %&gt;%\n        unique()\n    id_mean_expr &lt;- data.frame(Expr = rowMeans(expr[, id_reps]))\n    names(id_mean_expr) &lt;- id\n    data &lt;- bind_cols(data, id_mean_expr)\n}\ndata &lt;- as.data.frame(data) %&gt;%\n    set_rownames(.[[\"GeneID\"]]) %&gt;%\n    select(-all_of(\"GeneID\"))\n\n\n# use row variances to identify the top 3000 most variable genes\n# log2-trsanformation is recommended for reducing variance variation among genes\ndata &lt;- log2(data + 1)\ndata[[\"var\"]] &lt;- apply(data, 1, var)\ndata &lt;- data %&gt;%\n    arrange(desc(var)) %&gt;%\n    slice_head(n = 3000) %&gt;%\n    select(-all_of(\"var\"))\ndata &lt;- data[, sample_dev_order]\n\n\n# perform PCA analysis over samples (samples as observations)\n# calculate Euclidean distances among samples based on their coordinates (Dim.1, Dim.2) in sample PCA space\n# obtain the developmental time scale by accumulating distances of mutual samples\nsample_pca &lt;- PCA(t(data), scale.unit = T, ncp = 5, graph = F)\nsample_pca_coords &lt;- sample_pca$ind$coord[, 1:2]\n\n# visualize sample positions in PCA space\nsample_pca_coords_vis &lt;- as.data.frame(sample_pca_coords)\nsample_pca_coords_vis[[\"Sample\"]] &lt;- row.names(sample_pca_coords_vis)\nsample_pca_eig_vis &lt;- as.data.frame(sample_pca$eig)\n\nggplot(sample_pca_coords_vis, aes(Dim.1, Dim.2)) +\n    geom_point(size = 2) +\n    geom_text_repel(aes(label = Sample), size = 5, min.segment.length = 3) +\n    xlab(paste0(\"PC1 (\", round(sample_pca_eig_vis[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(sample_pca_eig_vis[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.title.x = element_text(size = 26),\n        axis.title.y = element_text(size = 26),\n        axis.text.x = element_text(size = 24),\n        axis.text.y = element_text(size = 24),\n        legend.text = element_text(size = 24),\n        legend.title = element_text(size = 26),\n        text = element_text(family = \"Arial\")\n    )\n\n\n\n\n\n\n\n\n\nsample_dists &lt;- as.matrix(dist(sample_pca_coords, method = \"euclidean\"))\n\n# visualize sample distances via heatmap\nHeatmap(sample_dists, cluster_rows = F, cluster_columns = F)\n\n\n\n\n\n\n\n\n\n# calculate the developmental time scale by accumulating distances of mutual samples along the actual developmental stages\nraw_timeline &lt;- cumsum(c(0, sapply(2:ncol(data), function(x) {\n    sample_dists[x - 1, x]\n})))\n# scale the raw time scale to (0, 10)\nnew_timeline &lt;- scales::rescale(raw_timeline, to = c(0, 10))\n\n\n# fit a spline for each gene and obtain 500 time points by interpolation\ndata_scale &lt;- as.data.frame(t(scale(t(data))))\n\n# interpolate more time points (e.g., 500) to make the expression dynamics smoother along the developmental stages\n# based on the fitted spline for each gene (using the loess method in modelr package)\npseudotime_model_fun &lt;- function(sample_value, sample_timeline, time_points_num = 500) {\n    grid &lt;- data.frame(time = seq(0, 10, length.out = time_points_num))\n    data &lt;- tibble(value = sample_value, time = sample_timeline)\n    model &lt;- loess(value ~ time, data)\n    predict &lt;- add_predictions(grid, model)\n    return(predict)\n}\n\npseudotime_model_res &lt;- apply(data_scale, 1, pseudotime_model_fun, sample_timeline = new_timeline, time_points_num = time_points_num)\nres &lt;- lapply(pseudotime_model_res, function(x) {\n    x[[\"pred\"]]\n}) %&gt;%\n    do.call(rbind, .) %&gt;%\n    as.data.frame() %&gt;%\n    set_colnames(pseudotime_model_res[[1]][[\"time\"]])\n\n\n# perform PCA analysis over genes\n# use atan2 method to sort genes based on their coordinates (Dim.1, Dim.2) in gene PCA space\ngene_pca &lt;- PCA(res, scale.unit = T, ncp = 5, graph = F)\ngene_pca_coords &lt;- gene_pca$ind$coord[, 1:2]\nres &lt;- bind_cols(res, gene_pca_coords)\n\n# we have four signed combinations of Dim.1 and Dim.2\nres[[\"atan2.1\"]] &lt;- atan2(res[[\"Dim.1\"]], res[[\"Dim.2\"]])\nres[[\"atan2.2\"]] &lt;- atan2(res[[\"Dim.1\"]], -res[[\"Dim.2\"]])\nres[[\"atan2.3\"]] &lt;- atan2(-res[[\"Dim.1\"]], res[[\"Dim.2\"]])\nres[[\"atan2.4\"]] &lt;- atan2(-res[[\"Dim.1\"]], -res[[\"Dim.2\"]])\n\n# sort genes based on their atan2 values in ascending order\nres_order1 &lt;- arrange(res, res[[\"atan2.1\"]])\nres_order2 &lt;- arrange(res, res[[\"atan2.2\"]])\nres_order3 &lt;- arrange(res, res[[\"atan2.3\"]])\nres_order4 &lt;- arrange(res, res[[\"atan2.4\"]])\n\n# pick the expected one\np1 &lt;- Heatmap(as.matrix(res_order1[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order1\",\n    heatmap_legend_param = list(title = \"Order1\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np2 &lt;- Heatmap(as.matrix(res_order2[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order2\",\n    heatmap_legend_param = list(title = \"Order2\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np3 &lt;- Heatmap(as.matrix(res_order3[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order3\",\n    heatmap_legend_param = list(title = \"Order3\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\np4 &lt;- Heatmap(as.matrix(res_order4[, 1:time_points_num]),\n    cluster_rows = F,\n    cluster_columns = F,\n    show_row_names = F,\n    show_column_names = F,\n    column_title = \"Order4\",\n    heatmap_legend_param = list(title = \"Order4\", legend_height = unit(2, \"cm\")),\n    col = colorRampPalette(rev(brewer.pal(n = 11, name = \"RdYlBu\")))(100)\n)\n\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html",
    "title": "DE-related analysis for bulk proteomics",
    "section": "",
    "text": "At present, this pipeline only allows two sample groups contained in your input files.\n\nwork_dir &lt;- \"/data/users/yangrui/lab_projs/lym_new\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#introduction",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#introduction",
    "title": "DE-related analysis for bulk proteomics",
    "section": "",
    "text": "At present, this pipeline only allows two sample groups contained in your input files.\n\nwork_dir &lt;- \"/data/users/yangrui/lab_projs/lym_new\"\n\nsetwd(work_dir)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#quality-check",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#quality-check",
    "title": "DE-related analysis for bulk proteomics",
    "section": "2 Quality check",
    "text": "2 Quality check\n\nexpr_file must include two ID columns:\n\n\nprotein_groups: in which each row must be unique;\ngene_name: in which each gene name may not be unique.\n\nIn addition, it should also contain a number of sample columns containing protein abundance levels.\nEach sample name must be in the form of ID_N, where N is the replicate number.\n\nsample_file must include the following three columns:\n\n\nsample: with the same sample names as those in the abundance file above in the form of ID_N;\ngroup: in the form of ID;\nreplicate: in the form of N.\n\n\nlibrary(tidyverse)\nlibrary(vroom)\nlibrary(psych)\nlibrary(FactoMineR)\nlibrary(ggforce)\nlibrary(ggprism)\nlibrary(ggalign)\nlibrary(YRUtils)\n\nexpr_file &lt;- \"input/BTN3A2_proteomic_quantification_data.tsv\"\nsample_file &lt;- \"input/sample_sheet.tsv\"\nqc_dir &lt;- \"qc\"\n# Keep those rows detected in at least N replicates in at least one group\nexpr_sample_num &lt;- 2\n\ndir.create(qc_dir, showWarnings = FALSE, recursive = FALSE)\nsample_df &lt;- vroom(sample_file) %&gt;%\n    distinct()\nexpr_df &lt;- vroom(expr_file) %&gt;%\n    distinct()\ntable(duplicated(expr_df$protein_groups))\n\nfor (s in sample_df$sample) {\n    expr_df[[s]][is.na(expr_df[[s]])] &lt;- 0\n}\n\nfilter_flag &lt;- rep(F, nrow(expr_df))\nfor (g in unique(sample_df$group)) {\n    tmp_df &lt;- expr_df[, filter(sample_df, group == g) %&gt;% pull(sample) %&gt;% unique(), drop = F]\n    filter_flag &lt;- filter_flag | (rowSums(tmp_df &gt; 0) &gt;= expr_sample_num)\n}\nexpr_df &lt;- expr_df[filter_flag, ]\n\nall_vs_none_expr_df &lt;- tibble()\nsample_groups &lt;- unique(sample_df$group)\nfor (g in sample_groups) {\n    tmp_df &lt;- expr_df[rowSums(expr_df[, filter(sample_df, group == g) %&gt;% pull(sample) %&gt;% unique(), drop = F] &gt; 0) == 0, ] %&gt;%\n        mutate(diff_flag = paste0(sample_groups[sample_groups != g], \" Up\"))\n    all_vs_none_expr_df &lt;- bind_rows(all_vs_none_expr_df, tmp_df)\n}\nboth_expr_df &lt;- filter(expr_df, !(protein_groups %in% all_vs_none_expr_df$protein_groups))\nvroom_write(all_vs_none_expr_df,\n    file = gsub(\"\\\\.\\\\w+$\", \".all_vs_none.tsv\", expr_file),\n    col_names = TRUE, append = FALSE\n)\nvroom_write(both_expr_df,\n    file = gsub(\"\\\\.\\\\w+$\", \".both.tsv\", expr_file),\n    col_names = TRUE, append = FALSE\n)\nlog_expr_df &lt;- log2(expr_df[, unique(sample_df$sample)] + 1)\n\n# Correlation\ncor_res &lt;- corr.test(log_expr_df, use = \"pairwise\", method = \"pearson\", adjust = \"BH\")\n\np &lt;- ggheatmap(\n    cor_res$r,\n    width = ncol(cor_res$r) * unit(10, \"mm\"),\n    height = nrow(cor_res$r) * unit(10, \"mm\")\n) +\n    scheme_align(free_spaces = \"t\") +\n    scale_fill_gradient2(\n        low = \"blue\", mid = \"white\", high = \"red\",\n        midpoint = 0, limits = c(-1, 1),\n        breaks = c(-1, -0.5, 0, 0.5, 1)\n    ) +\n    labs(fill = \"R\") +\n    guides(x = guide_axis(angle = 45)) +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    anno_top() +\n    ggalign(\n        data = gsub(\"_\\\\d+$\", \"\", colnames(cor_res$r)),\n        size = unit(4, \"mm\")\n    ) +\n    geom_tile(aes(y = 1, fill = factor(value))) +\n    scale_y_continuous(breaks = NULL, name = NULL, expand = expansion(0)) +\n    labs(fill = \"Sample\") +\n    theme(\n        text = element_text(size = 20, family = \"Arial\", color = \"black\"),\n        axis.text = element_text(size = 20, family = \"Arial\", color = \"black\")\n    ) +\n    with_quad(scheme_align(guides = \"t\"), NULL)\nppreview(p, file = file.path(qc_dir, \"correlation.pdf\"))\n\n# PCA\npca &lt;- PCA(t(log_expr_df), ncp = 10, scale.unit = TRUE, graph = FALSE)\n\npca_coord &lt;- as.data.frame(pca$ind$coord)\npca_coord$sample &lt;- row.names(pca_coord)\npca_coord$group &lt;- factor(gsub(\"_\\\\d+$\", \"\", pca_coord$sample))\npca_eig &lt;- as.data.frame(pca$eig)\n\np &lt;- ggplot(pca_coord, aes(Dim.1, Dim.2)) +\n    geom_point(aes(color = group), size = 4) +\n    xlab(paste0(\"PC1 (\", round(pca_eig[\"comp 1\", \"percentage of variance\"]), \"%)\")) +\n    ylab(paste0(\"PC2 (\", round(pca_eig[\"comp 2\", \"percentage of variance\"]), \"%)\")) +\n    geom_mark_ellipse(aes(fill = group), color = NA, alpha = 0.25) +\n    theme_prism(base_family = \"Arial\", border = TRUE, base_size = 20)\nppreview(p, file = file.path(qc_dir, \"pca.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#differential-abundance-analysis",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#differential-abundance-analysis",
    "title": "DE-related analysis for bulk proteomics",
    "section": "3 Differential abundance analysis",
    "text": "3 Differential abundance analysis\nFor spectra count (MS2), the distribution of which can be approximated by a Poisson distribution. In this case, DESeq2, edgeR, etc. may be used.\nFor signal intensity (MS1), the distribution of which can be approximated by a Normal distribution. In this case, limma may be used.\n\n3.1 DEP2 (limma-based method)\n\nlibrary(DEP2)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(ggridges)\nlibrary(YRUtils)\n\nexpr_file &lt;- \"input/BTN3A2_proteomic_quantification_data.both.tsv\"\nsample_file &lt;- \"input/sample_sheet.tsv\"\noutput_dir &lt;- \"de\"\n\ndir.create(output_dir, showWarnings = FALSE, recursive = FALSE)\nsample_df &lt;- vroom(sample_file) %&gt;%\n    distinct()\nexpr_df &lt;- vroom(expr_file) %&gt;%\n    distinct()\n\n\n### 1. Perform differential expression analysis over genes detected in both treatment and control\n# For duplicated IDs, add tailed numbers, delimiter is \".\"\nexpr_df &lt;- make_unique(expr_df, names = \"gene_name\", ids = \"protein_groups\", delim = \";\")\n# Take expression columns\nexpr_cols &lt;- which(names(expr_df) %in% sample_df$sample)\n\n\n## 1. Construct SummarizedExperiment object\nse &lt;- make_se_parse(expr_df,\n    columns = expr_cols,\n    mode = \"delim\", sep = \"_\",\n    log2transform = T\n)\n\n\n## 2. Filtering\n# Filtering out reverse, contaminant, and low-quality features with many missing values\n# Allow up to N missing values in each condition\nmv_num &lt;- 1\n\n# filter_se() also provides filtering options based on filter_formula,\n# which can be used to filter features based on given columns\nfilter_se &lt;- filter_se(se, thr = mv_num)\n\np &lt;- (plot_frequency(se) + ggtitle(\"Protein identifications overlap before filter\")) /\n    (plot_frequency(filter_se) + ggtitle(\"Protein identifications overlap after filter\"))\nppreview(p, file = file.path(output_dir, \"filter_freq_hists.pdf\"))\n\n# Even after filtering, a considerable proportion of missing values may still remain in the assay\nplot_missval(filter_se)\n\n\n## 3. Normalization\n# log1p --&gt; variance stabilizing transformation (vst)\nnorm_se &lt;- normalize_vsn(filter_se)\n\np &lt;- plot_normalization(norm_se)\nppreview(p, file = file.path(output_dir, \"norm_boxplot.pdf\"))\n\n# Density and CumSum plots of intensities of proteins with and without missing values\nplot_detect(norm_se)\n\n## 4. Imputation\n# 1. Some may be suitable for missing at random\n# In this case, both small and large values may be missing,\n# so imputing relatively larger values is reasonable\n# 2. Some may be suitable for missing not at random\n# In this case, I think a large proportion of missing values are so small that they cannot be detected,\n# so using imputation with cautions, it may introduce unwanted bias\n# You should have a deep comprehension for the sources of missing values\n# c(\"QRILC\", \"bpca\", \"knn\", \"MLE\", \"MinDet\", \"MinProb\", \"man\", \"min\", \"zero\", \"mixed\", \"nbavg\", \"RF\", \"GSimp\")\n# It seems that \"MLE\" is really time-consuming\n# Remove method \"mixed\" because it needs specifying which rows are missing at random (the remaining rows are missing not at random), and specifying two methods for dealing with missing at random and missing not at random respectively\n# Remove method \"MLE\" because it seems that we need to adjust parameters more carefully otherwise the imputation values of NAs are much larger than the normal\nimpute_methods &lt;- c(\"QRILC\", \"bpca\", \"knn\", \"MinDet\", \"MinProb\", \"man\", \"min\", \"zero\", \"nbavg\", \"RF\", \"GSimp\")\n\nnames(impute_methods) &lt;- impute_methods\n# Keep only those sample-feature pairs having NAs\nnas_df &lt;- assay(norm_se) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(gene_name = row.names(.)) %&gt;%\n    pivot_longer(cols = !gene_name, names_to = \"label\", values_to = \"value\") %&gt;%\n    mutate(is_na = is.na(value)) %&gt;%\n    filter(is_na) %&gt;%\n    select(gene_name, label)\n# Keep only those sample-feature pairs not having NAs\nnon_nas &lt;- nas_df %&gt;%\n    mutate(id = paste0(label, \":\", gene_name)) %&gt;%\n    pull(id) %&gt;%\n    unique()\n# Try each imputation method\nimpute_se_ls &lt;- lapply(impute_methods, function(x) {\n    message(paste0(\"\\n\\nrunning \", x, \" ...\"))\n    impute(norm_se, fun = x)\n})\n\nimps &lt;- tibble()\nfor (m in names(impute_se_ls)) {\n    tmp_df &lt;- assay(impute_se_ls[[m]]) %&gt;%\n        as.data.frame() %&gt;%\n        mutate(gene_name = row.names(.)) %&gt;%\n        pivot_longer(cols = !gene_name, names_to = \"label\", values_to = \"value\") %&gt;%\n        left_join(as.data.frame(colData(impute_se_ls[[m]])[, c(\"label\", \"condition\")]), by = \"label\") %&gt;%\n        right_join(nas_df, by = c(\"gene_name\", \"label\")) %&gt;%\n        mutate(method = m)\n    imps &lt;- bind_rows(imps, tmp_df)\n}\nnon_imps &lt;- assay(norm_se) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(gene_name = row.names(.)) %&gt;%\n    pivot_longer(cols = !gene_name, names_to = \"label\", values_to = \"value\") %&gt;%\n    left_join(as.data.frame(colData(norm_se)[, c(\"label\", \"condition\")]), by = \"label\") %&gt;%\n    mutate(id = paste0(label, \":\", gene_name), method = \"non_impute\") %&gt;%\n    filter(!(id %in% non_nas)) %&gt;%\n    select(!id)\n\n# Check the distribution of imputation values of NAs under each method\np &lt;- ggplot(bind_rows(imps, non_imps), aes(x = value, y = factor(method, level = unique(method)))) +\n    geom_density_ridges(\n        fill = \"#027AD450\", scale = 1.2,\n        jittered_points = TRUE, position = position_points_jitter(height = 0),\n        point_shape = \"|\", point_size = 2, point_alpha = 1, alpha = 0.7\n    ) +\n    ylab(\"Impute method\") +\n    xlab(\"log2 Intensity\") +\n    theme_classic()\nppreview(p, file = file.path(output_dir, \"impute_density.pdf\"))\n\n\n## 5. Hypothesis testing\n# T-test using limma\n# Test all the other samples vs. control\ncontrol_sample &lt;- \"Control\"\npadj_th &lt;- 0.05\nlfc_th &lt;- 1\n\ncontrast &lt;- \"BTN3A2_vs_Control\"\n\npair &lt;- strsplit(contrast, \"_vs_\")[[1]]\nfor (m in names(impute_se_ls)) {\n    impute_se &lt;- impute_se_ls[[m]]\n\n    diff_se &lt;- test_diff(impute_se, type = \"control\", control = control_sample, fdr.type = \"BH\")\n    sig_se &lt;- add_rejections(diff_se, alpha = padj_th, lfc = lfc_th)\n    saveRDS(sig_se, file = file.path(output_dir, paste0(contrast, \"_\", m, \"_de.rds\")))\n\n    clean_degs &lt;- as.data.frame(sig_se@elementMetadata)\n    names(clean_degs)[grep(\"_diff$\", names(clean_degs))] &lt;- \"log2FoldChange\"\n    names(clean_degs)[grep(\"_p.adj$\", names(clean_degs))] &lt;- \"padj\"\n    clean_degs &lt;- clean_degs %&gt;%\n        group_by(gene_name) %&gt;%\n        slice_min(num_NAs) %&gt;%\n        slice_max(sequence_number) %&gt;%\n        slice_min(padj) %&gt;%\n        slice_max(log2FoldChange) %&gt;%\n        slice_sample(n = 1) %&gt;%\n        ungroup() %&gt;%\n        arrange(desc(log2FoldChange)) %&gt;%\n        mutate(diff_flag = if_else(padj &lt; padj_th,\n            if_else(abs(log2FoldChange) &gt; lfc_th,\n                if_else(log2FoldChange &gt; lfc_th,\n                    paste0(pair[1], \" Up\"),\n                    paste0(pair[2], \" Up\")\n                ),\n                \"NO\"\n            ),\n            \"NO\"\n        ))\n\n    vroom_write(clean_degs, file = file.path(output_dir, paste0(contrast, \"_\", m, \"_de.tsv\")))\n\n    p &lt;- plot_volcano(sig_se, contrast = contrast, adjusted = T, add_threshold_line = \"intersect\", pCutoff = padj_th, fcCutoff = lfc_th, add_names = F)\n    ppreview(p, file = file.path(output_dir, paste0(contrast, \"_\", m, \"_volcano.pdf\")))\n\n    p &lt;- plot_heatmap(sig_se, manual_contrast = contrast, kmeans = T, k = 2, split_order = c(2, 1), show_row_names = F, show_row_dend = F, heatmap_width = unit(6, \"cm\"), heatmap_height = unit(0.04, \"mm\") * nrow(sig_se@elementMetadata))\n    ppreview(p, file = file.path(output_dir, paste0(contrast, \"_\", m, \"_heatmap.pdf\")))\n}\n\n\n\n3.2 Only using limma package\nFor the following two methods, there must be two columns with names protein_name (which must be unique) and gene_name in the expression table file, in which all sample column names must be in the form ID_repN, which will be matched using the regular expression [a-zA-Z0-9]+_rep[0-9]+.\n\nCompare two samples at a time\n\n\nlibrary(vroom)\nlibrary(limma)\nlibrary(tidyverse)\nlibrary(magrittr)\n\nexpr_file &lt;- \"expression_sheet.tsv\"\n# keep those proteins detected in at least N replicates in at least one sample\nmin_expr_num &lt;- 2\npadj_th &lt;- 0.05\nlogfc_th &lt;- 1\n# the first is the control sample\ngroup_levels &lt;- c(\"WT\", \"S541N\")\n\nexpr_df &lt;- vroom(expr_file) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein_name\"]])\n\nsamples &lt;- names(expr_df)[str_detect(names(expr_df), \"[a-zA-Z0-9]+_rep[0-9]+\")]\nnames(samples) &lt;- gsub(\"_rep[0-9]+$\", \"\", samples)\n\nexpr_mat &lt;- expr_df[, samples]\nexpr_mat[is.na(expr_mat)] &lt;- 0\n\nflag &lt;- rep(FALSE, nrow(expr_mat))\nfor (group in unique(names(samples))) {\n    flag &lt;- flag | (rowSums(expr_mat[, samples[names(samples) == group]] &gt; 0) &gt;= min_expr_num)\n}\nexpr_mat &lt;- expr_mat[flag, ]\n\nexpr_mat_long_df &lt;- expr_mat %&gt;%\n    mutate(protein_name = row.names(.)) %&gt;%\n    pivot_longer(cols = -protein_name, names_to = \"sample\", values_to = \"value\") %&gt;%\n    mutate(group = gsub(\"_rep[0-9]+$\", \"\", sample))\n\nmean_df &lt;- expr_mat_long_df %&gt;%\n    filter(value != 0) %&gt;%\n    group_by(protein_name, group) %&gt;%\n    reframe(mean = mean(value))\n\nexpr_mat_filled_df &lt;- left_join(expr_mat_long_df, mean_df, by = c(\"protein_name\", \"group\")) %&gt;%\n    mutate(value = if_else(!is.na(mean) & (value == 0), mean, value))\n\nexpr_mat &lt;- expr_mat_filled_df %&gt;%\n    select(-all_of(c(\"group\", \"mean\"))) %&gt;%\n    pivot_wider(names_from = \"sample\", values_from = \"value\") %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein_name\"]]) %&gt;%\n    select(-protein_name)\n\nexpr_mat &lt;- expr_mat[, samples]\nexpr_df &lt;- inner_join(expr_df, expr_mat %&gt;% mutate(protein_name = row.names(.)), suffix = c(\".raw\", \".mean_filled\"), by = \"protein_name\")\n\nexpr_mat &lt;- log2(expr_mat + 1)\n\ngroup_list &lt;- factor(names(samples), levels = group_levels)\ndesign &lt;- model.matrix(~ 0 + group_list)\ncolnames(design) &lt;- levels(group_list)\nrow.names(design) &lt;- colnames(expr_mat)\n\ncontrasts &lt;- t(combn(group_levels, 2)) %&gt;%\n    as.data.frame() %&gt;%\n    mutate(contrast = paste0(V2, \"-\", V1)) %&gt;%\n    pull(contrast)\n\nfit &lt;- lmFit(expr_mat, design)\ncontrast_matrix &lt;- makeContrasts(contrasts = contrasts, levels = design)\nfit2 &lt;- contrasts.fit(fit, contrast_matrix)\nfit2 &lt;- eBayes(fit2, trend = TRUE, robust = TRUE)\n\n# check valid coefficients\ncolnames(fit2$coefficients)\n\ncoef &lt;- \"S541N-WT\"\n\nres &lt;- topTable(fit2, coef = coef, number = Inf) %&gt;%\n    mutate(protein_name = row.names(.))\n\nres &lt;- inner_join(expr_df, res, by = \"protein_name\") %&gt;%\n    mutate(\n        P.Value = if_else(P.Value == 0, .Machine$double.xmin, P.Value),\n        adj.P.Val = if_else(adj.P.Val == 0, .Machine$double.xmin, adj.P.Val),\n        diff_flag = if_else(adj.P.Val &lt; 0.05,\n            if_else(abs(logFC) &gt; logfc_th,\n                if_else(logFC &gt; logfc_th, paste0(strsplit(coef, \"-\")[[1]][1], \" Up\"), paste0(strsplit(coef, \"-\")[[1]][2], \" Up\")),\n                \"NO\"\n            ),\n            \"NO\"\n        )\n    ) %&gt;%\n    arrange(diff_flag)\n\nvroom_write(res, file = paste0(coef, \".tsv\"), col_names = TRUE, append = FALSE)\n\n\nCompare all possible pairs at a time\n\n\nlibrary(vroom)\nlibrary(limma)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(gtools)\n\nwork_dir &lt;- \"test\"\nexpr_file &lt;- \"xbf_expression_sheet.tsv\"\n# keep those proteins detected in at least N replicates in at least one sample\nmin_expr_num &lt;- 2\npadj_th &lt;- 0.05\nlogfc_th &lt;- 1\n\nsetwd(work_dir)\n\nexpr_df &lt;- vroom(expr_file) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein_name\"]])\n\nsamples &lt;- names(expr_df)[str_detect(names(expr_df), \"[a-zA-Z0-9]+_rep[0-9]+\")]\nnames(samples) &lt;- gsub(\"_rep[0-9]+$\", \"\", samples)\nsample_pair_combns &lt;- permutations(length(unique(names(samples))), 2, v = unique(names(samples)))\n\nfor (i in seq_len(nrow(sample_pair_combns))) {\n    group_levels &lt;- sample_pair_combns[i, ]\n    pair_samples &lt;- samples[names(samples) %in% group_levels]\n\n    expr_mat &lt;- expr_df[, pair_samples]\n    expr_mat[is.na(expr_mat)] &lt;- 0\n\n    flag &lt;- rep(FALSE, nrow(expr_mat))\n    for (group in group_levels) {\n        flag &lt;- flag | (rowSums(expr_mat[, samples[names(samples) == group]] &gt; 0) &gt;= min_expr_num)\n    }\n    expr_mat &lt;- expr_mat[flag, ]\n\n    expr_mat_long_df &lt;- expr_mat %&gt;%\n        mutate(protein_name = row.names(.)) %&gt;%\n        pivot_longer(cols = -protein_name, names_to = \"sample\", values_to = \"value\") %&gt;%\n        mutate(group = gsub(\"_rep[0-9]+$\", \"\", sample))\n\n    mean_df &lt;- expr_mat_long_df %&gt;%\n        filter(value != 0) %&gt;%\n        group_by(protein_name, group) %&gt;%\n        reframe(mean = mean(value))\n\n    expr_mat_filled_df &lt;- left_join(expr_mat_long_df, mean_df, by = c(\"protein_name\", \"group\")) %&gt;%\n        mutate(value = if_else(!is.na(mean) & (value == 0), mean, value))\n\n    expr_mat &lt;- expr_mat_filled_df %&gt;%\n        select(-all_of(c(\"group\", \"mean\"))) %&gt;%\n        pivot_wider(names_from = \"sample\", values_from = \"value\") %&gt;%\n        as.data.frame() %&gt;%\n        set_rownames(.[[\"protein_name\"]]) %&gt;%\n        select(-protein_name)\n\n    expr_mat &lt;- expr_mat[, pair_samples]\n    expr_mat &lt;- log2(expr_mat + 1)\n\n    group_list &lt;- factor(names(pair_samples), levels = group_levels)\n    design &lt;- model.matrix(~ 0 + group_list)\n    colnames(design) &lt;- levels(group_list)\n    row.names(design) &lt;- colnames(expr_mat)\n\n    contrasts &lt;- t(combn(group_levels, 2)) %&gt;%\n        as.data.frame() %&gt;%\n        mutate(contrast = paste0(V2, \"-\", V1)) %&gt;%\n        pull(contrast)\n\n    fit &lt;- lmFit(expr_mat, design)\n    contrast_matrix &lt;- makeContrasts(contrasts = contrasts, levels = design)\n    fit2 &lt;- contrasts.fit(fit, contrast_matrix)\n    fit2 &lt;- eBayes(fit2, trend = TRUE, robust = TRUE)\n\n    coef &lt;- colnames(fit2$coefficients)\n\n    res &lt;- topTable(fit2, coef = coef, number = Inf) %&gt;%\n        mutate(protein_name = row.names(.))\n\n    res &lt;- inner_join(expr_df[, c(\"protein_name\", \"gene_name\")], res, by = \"protein_name\") %&gt;%\n        mutate(\n            P.Value = if_else(P.Value == 0, .Machine$double.xmin, P.Value),\n            adj.P.Val = if_else(adj.P.Val == 0, .Machine$double.xmin, adj.P.Val),\n            diff_flag = if_else(adj.P.Val &lt; 0.05,\n                if_else(abs(logFC) &gt; logfc_th,\n                    if_else(logFC &gt; logfc_th, paste0(strsplit(coef, \"-\")[[1]][1], \" Up\"), paste0(strsplit(coef, \"-\")[[1]][2], \" Up\")),\n                    \"NO\"\n                ),\n                \"NO\"\n            )\n        ) %&gt;%\n        arrange(diff_flag)\n\n    vroom_write(res, file = paste0(gsub(\"-\", \"_vs_\", coef), \".tsv\"), col_names = TRUE, append = FALSE)\n}"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#merge-de-files",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#merge-de-files",
    "title": "DE-related analysis for bulk proteomics",
    "section": "4 Merge DE files",
    "text": "4 Merge DE files\n\nlibrary(vroom)\nlibrary(tidyverse)\n\nfiles &lt;- c(\n    \"input/BTN3A2_proteomic_quantification_data.all_vs_none.tsv\",\n    \"de/BTN3A2_vs_Control_bpca_de.tsv\"\n)\noutput_file &lt;- \"de/BTN3A2_vs_Control_bpca_de.all_vs_none.tsv\"\n\ndf &lt;- tibble()\nfor (file in files) {\n    df &lt;- bind_rows(\n        df,\n        vroom(file) %&gt;%\n            select(protein_name, gene_name, diff_flag) %&gt;%\n            filter(diff_flag != \"NO\")\n    )\n}\ndf &lt;- distinct(df) %&gt;%\n    arrange(diff_flag)\n\nvroom_write(df, file = output_file, col_names = TRUE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#retrieve-sub-cellular-location-info-from-uniprot-and-synaptome-database",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#retrieve-sub-cellular-location-info-from-uniprot-and-synaptome-database",
    "title": "DE-related analysis for bulk proteomics",
    "section": "5 Retrieve sub-cellular location info from UniProt and Synaptome database",
    "text": "5 Retrieve sub-cellular location info from UniProt and Synaptome database\n\nlibrary(vroom)\nlibrary(glue)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(synaptome.db)\n\norganism_id &lt;- 10116\nurl_template &lt;- \"https://rest.uniprot.org/uniprotkb/search?query=(accession:{accession})%20AND%20(active:true)%20AND%20(organism_id:{organism_id})&fields=accession,gene_primary,organism_name,organism_id,protein_name,annotation_score,protein_existence,reviewed,ft_intramem,cc_subcellular_location,ft_topo_dom,ft_transmem,go,go_p,go_c,go_f&format=tsv\"\nde_file &lt;- \"de/BTN3A2_vs_Control_bpca_de.all_vs_none.tsv\"\noutput_dir &lt;- \"subcellular_location\"\nkeep_cols &lt;- c(\"protein_name\", \"gene_name\", \"diff_flag\")\nkeep_de_types &lt;- c(\"BTN3A2 Up\", \"Control Up\")\n\ndir.create(output_dir, showWarnings = FALSE, recursive = FALSE)\ndf &lt;- vroom(de_file) %&gt;%\n    select(all_of(keep_cols)) %&gt;%\n    filter(diff_flag %in% keep_de_types)\n\nres_df &lt;- tibble()\nfor (accession in unique(df$protein_name)) {\n    url_instance &lt;- glue(url_template)\n    e &lt;- try(item_raw_text &lt;- read_html(url_instance) %&gt;% html_text(), silent = T)\n    if (\"try-error\" %in% class(e)) {\n        message(\"invalid accession: \", accession)\n    } else {\n        item_df &lt;- vroom(I(item_raw_text), delim = \"\\t\")\n        res_df &lt;- bind_rows(res_df, item_df)\n    }\n}\nres_df &lt;- distinct(res_df)\n\nres_df[[\"scl_secret\"]] &lt;- grepl(\"secret\", res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\nres_df[[\"scl_membrane\"]] &lt;- grepl(\"membrane\", res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\nres_df[[\"scl_secret_membrane\"]] &lt;- res_df[[\"scl_secret\"]] | res_df[[\"scl_membrane\"]]\n\nres_df[[\"uniprot_scl_synapse\"]] &lt;- str_extract_all(res_df[[\"Subcellular location [CC]\"]], regex(\"[\\\\w-]*synap[\\\\w-]*\", ignore_case = TRUE)) %&gt;%\n    sapply(function(x) {\n        paste0(unique(na.omit(x)), collapse = \";\")\n    })\nres_df[[\"uniprot_go_synapse\"]] &lt;- str_extract_all(res_df[[\"Gene Ontology (GO)\"]], regex(\"[\\\\w-]*synap[\\\\w-]*\", ignore_case = T)) %&gt;%\n    sapply(function(x) {\n        paste0(unique(na.omit(x)), collapse = \";\")\n    })\nres_df &lt;- left_join(res_df,\n    findGeneByCompartmentPaperCnt(cnt = 1) %&gt;%\n        filter(RatName %in% res_df[[\"Gene Names (primary)\"]]) %&gt;%\n        select(RatName, Localisation) %&gt;%\n        distinct() %&gt;%\n        group_by(RatName) %&gt;%\n        reframe(synaptome_db_synapse = paste0(unique(Localisation), collapse = \";\")),\n    by = c(\"Gene Names (primary)\" = \"RatName\")\n)\nres_df[[\"final_synapse\"]] &lt;- apply(res_df, 1, function(x) {\n    vec &lt;- na.omit(c(x[\"uniprot_scl_synapse\"], x[\"uniprot_go_synapse\"], x[\"synaptome_db_synapse\"]))\n    paste0(vec[vec != \"\"], collapse = \";\")\n}, simplify = T)\nres_df[[\"scl_secret_membrane_final_synapse\"]] &lt;- res_df[[\"scl_secret_membrane\"]] & (res_df[[\"final_synapse\"]] != \"\")\n\ntarget_res_df &lt;- filter(res_df, scl_secret_membrane_final_synapse)\ntarget_res_df[[\"final_synapse_label\"]] &lt;- sapply(target_res_df[[\"final_synapse\"]], function(x) {\n    vec &lt;- NA\n    if (grepl(\"postsynap|post-synap\", x, ignore.case = TRUE)) {\n        vec &lt;- c(vec, \"Postsynapse\")\n    }\n    if (grepl(\"presynap|pre-synap\", x, ignore.case = TRUE)) {\n        vec &lt;- c(vec, \"Presynapse\")\n    }\n    if (grepl(\"synap\", x, ignore.case = TRUE)) {\n        vec &lt;- c(vec, \"Synapse\")\n    }\n    if (grepl(\"Synaptic_Vesicle\", x, ignore.case = TRUE)) {\n        vec &lt;- c(vec, \"Synaptic vesicle\")\n    }\n    vec &lt;- na.omit(vec)\n    if (length(vec) == 0) {\n        stop(\"no matching\")\n    }\n    if (length(vec) &gt; 1) {\n        vec &lt;- vec[vec != \"Synapse\"]\n    }\n    paste0(sort(unique(vec)), collapse = \"/\")\n})\n\nres_df &lt;- inner_join(df, res_df, by = c(\"protein_name\" = \"Entry\"))\ntarget_res_df &lt;- inner_join(df, target_res_df, by = c(\"protein_name\" = \"Entry\"))\nvroom_write(res_df, file = file.path(output_dir, gsub(\"\\\\.\\\\w+$\", \".scl.raw.tsv\", basename(de_file))))\nvroom_write(target_res_df, file = file.path(output_dir, gsub(\"\\\\.\\\\w+$\", \".scl.target.tsv\", basename(de_file))))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#go-enrichment-analysis",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#go-enrichment-analysis",
    "title": "DE-related analysis for bulk proteomics",
    "section": "6 GO enrichment analysis",
    "text": "6 GO enrichment analysis\n\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(clusterProfiler)\nlibrary(AnnotationDbi)\n\ngo_db_file &lt;- \"/data/biodatabase/species/mRatBN7/genome/anno/org.Rn.eg.db.sqlite\"\ndegs_file &lt;- \"de/BTN3A2_vs_Control_bpca_de.all_vs_none.tsv\"\nroot_dir &lt;- \"go\"\n\noutput_dir &lt;- file.path(root_dir, gsub(\"\\\\.\\\\w+$\", \"\", basename(degs_file)))\ndir.create(output_dir, recursive = TRUE)\n\ndegs &lt;- vroom(degs_file)\ngene_set_ls &lt;- degs[, c(\"gene_name\", \"diff_flag\")] %&gt;%\n    filter(diff_flag != \"NO\") %&gt;%\n    split(.[[\"diff_flag\"]]) %&gt;%\n    lapply(function(x) {\n        unique(na.omit(x[[\"gene_name\"]]))\n    })\n\norgdb &lt;- loadDb(go_db_file)\nego &lt;- compareCluster(gene_set_ls,\n    fun = \"enrichGO\",\n    keyType = \"SYMBOL\",\n    OrgDb = orgdb,\n    ont = \"ALL\",\n    pAdjustMethod = \"BH\",\n    minGSSize = 10,\n    maxGSSize = 1000,\n    pvalueCutoff = 0.05,\n    qvalueCutoff = 0.05,\n    readable = FALSE,\n    pool = FALSE\n)\nnrow(ego@compareClusterResult)\n\nsaveRDS(ego, file = file.path(output_dir, \"GO_ALL.rds\"))\nvroom_write(ego@compareClusterResult,\n    file = file.path(output_dir, \"GO_ALL.tsv\"),\n    col_names = TRUE, append = FALSE\n)"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#plot-expression-heatmap",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#plot-expression-heatmap",
    "title": "DE-related analysis for bulk proteomics",
    "section": "7 Plot expression heatmap",
    "text": "7 Plot expression heatmap\nThe followings are just various examples:\n\nlibrary(ComplexHeatmap)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(circlize)\nlibrary(YRUtils)\n\nsample_file &lt;- \"/data/users/yangrui/lab_projs/lym_new/input/sample_sheet.tsv\"\ngene_file &lt;- \"/data/users/yangrui/lab_projs/lym_new/de/BTN3A2_vs_Control_bpca_de.all_vs_none.tsv\"\nquant_file &lt;- \"/data/users/yangrui/lab_projs/lym_new/input/BTN3A2_proteomic_quantification_data.tsv\"\nlabel_file &lt;- \"/data/users/yangrui/lab_projs/lym_new/subcellular_location/BTN3A2_vs_Control_bpca_de.all_vs_none.scl.target.cys.tsv\"\noutput_dir &lt;- \"/home/yangrui/temp\"\n\nsample_df &lt;- vroom(sample_file)\ngene_df &lt;- vroom(gene_file) %&gt;%\n    filter(diff_flag == \"BTN3A2 Up\")\nlabel_df &lt;- vroom(label_file)\n\nquant_df &lt;- vroom(quant_file) %&gt;%\n    select(all_of(c(\"protein_name\", sample_df$sample))) %&gt;%\n    filter(protein_name %in% gene_df$protein_name) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein_name\"]]) %&gt;%\n    select(-protein_name)\nquant_df[is.na(quant_df)] &lt;- 0\nquant_mat &lt;- scale(t(log2(quant_df + 1)))\n\ncol_fun &lt;- colorRamp2(c(floor(min(quant_mat)), 0, ceiling(max(quant_mat))), c(\"skyblue\", \"white\", \"orange\"))\nrow_order &lt;- c(\"BTN3A2_1\", \"BTN3A2_2\", \"BTN3A2_3\", \"Control_1\", \"Control_2\", \"Control_3\")\nrow_split &lt;- gsub(\"_\\\\d+$\", \"\", row.names(quant_mat))\nrow_anno &lt;- rowAnnotation(\n    Sample = gsub(\"_\\\\d+$\", \"\", row.names(quant_mat)),\n    show_legend = FALSE,\n    show_annotation_name = FALSE,\n    simple_anno_size = unit(3, \"mm\"),\n    col = list(Sample = c(\"BTN3A2\" = \"orange\", \"Control\" = \"skyblue\"))\n)\ncol_anno &lt;- columnAnnotation(\n    Gene = anno_mark(\n        at = sapply(label_df$protein_name, function(x) {\n            which(colnames(quant_mat) == x)\n        },\n        simplify = TRUE, USE.NAMES = FALSE\n        ),\n        labels = label_df$gene_name\n    )\n)\n\np &lt;- Heatmap(\n    quant_mat,\n    col = col_fun,\n    cluster_columns = TRUE,\n    cluster_column_slices = TRUE,\n    cluster_rows = FALSE,\n    cluster_row_slices = FALSE,\n    show_column_names = FALSE,\n    show_column_dend = FALSE,\n    show_row_names = FALSE,\n    row_order = row_order,\n    row_names_side = \"left\",\n    row_split = row_split,\n    row_gap = unit(0, \"mm\"),\n    left_annotation = row_anno,\n    top_annotation = col_anno,\n    heatmap_legend_param = list(title = \"Expression\")\n)\nppreview(p, file = file.path(output_dir, \"heatmap.pdf\"))\n\n\nlibrary(ComplexHeatmap)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(circlize)\nlibrary(YRUtils)\n\nquant_file &lt;- \"temp/S541N-WT.tsv\"\nlabeled_genes &lt;- c(\"Ddx5\", \"Gapdh\", \"Actb\", \"Actg1\", \"Actn1\", \"Dhx9\", \"Draxin\", \"Pelp1\", \"Hnrnpa3\")\noutput_dir &lt;- \"temp\"\n\nsample_df &lt;- expand.grid(c(\"S541N\", \"WT\"), \"_rep\", 1:3, \".raw\") %&gt;%\n    mutate(sample = paste0(Var1, Var2, Var3, Var4)) %&gt;%\n    rename(group = Var1) %&gt;%\n    select(group, sample)\nquant_df &lt;- vroom(quant_file) %&gt;%\n    filter(diff_flag == \"S541N Up\") %&gt;%\n    select(all_of(c(\"protein_name\", \"gene_name\", sample_df$sample))) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein_name\"]])\n\nquant_mat &lt;- quant_df %&gt;%\n    select(-protein_name, -gene_name)\nquant_mat[is.na(quant_mat)] &lt;- 0\nquant_mat &lt;- scale(t(log2(quant_mat + 1)))\n\ncol_fun &lt;- colorRamp2(c(floor(min(quant_mat)), 0, ceiling(max(quant_mat))), c(\"skyblue\", \"white\", \"orange\"))\nrow_order &lt;- c(\"S541N_rep1.raw\", \"S541N_rep2.raw\", \"S541N_rep3.raw\", \"WT_rep1.raw\", \"WT_rep2.raw\", \"WT_rep3.raw\")\nrow_split &lt;- gsub(\"_rep\\\\d+.raw$\", \"\", row.names(quant_mat))\nleft_anno &lt;- rowAnnotation(\n    Sample = gsub(\"_rep\\\\d+.raw$\", \"\", row.names(quant_mat)),\n    show_legend = FALSE,\n    show_annotation_name = FALSE,\n    simple_anno_size = unit(3, \"mm\"),\n    col = list(Sample = c(\"S541N\" = \"#EE6A50\", \"WT\" = \"#76EEC6\"))\n)\n\nquant_df &lt;- quant_df %&gt;%\n    mutate(\n        group = if_else(is.na(gene_name), \"Other\",\n            if_else(str_detect(gene_name, \"^Rpl\"), \"Rpl\",\n                if_else(str_detect(gene_name, \"^Rps\"), \"Rps\",\n                    if_else(str_detect(gene_name, \"^Tuba\"), \"Tuba\",\n                        if_else(str_detect(gene_name, \"^Tubb\"), \"Tubb\", \"Other\")\n                    )\n                )\n            )\n        ),\n        group = factor(group, levels = c(\"Rpl\", \"Rps\", \"Tuba\", \"Tubb\", \"Other\"))\n    )\ncolumn_split &lt;- quant_df$group\nbottom_anno &lt;- columnAnnotation(\n    Group = quant_df$group,\n    show_legend = TRUE,\n    show_annotation_name = FALSE,\n    simple_anno_size = unit(3, \"mm\"),\n    col = list(Group = c(\"Rpl\" = \"#B23AEE\", \"Rps\" = \"#B4EEB4\", \"Tuba\" = \"#EE7600\", \"Tubb\" = \"#00B2EE\", \"Other\" = \"#EEA2AD\"))\n)\n\nlabel_df &lt;- filter(quant_df, gene_name %in% labeled_genes) %&gt;%\n    select(protein_name, gene_name)\ntop_anno &lt;- columnAnnotation(\n    Gene = anno_mark(\n        at = sapply(label_df$protein_name, function(x) {\n            which(colnames(quant_mat) == x)\n        },\n        simplify = TRUE, USE.NAMES = FALSE\n        ),\n        labels = label_df$gene_name\n    )\n)\n\np &lt;- Heatmap(\n    quant_mat,\n    col = col_fun,\n    cluster_columns = TRUE,\n    cluster_column_slices = FALSE,\n    cluster_rows = FALSE,\n    cluster_row_slices = FALSE,\n    show_column_names = FALSE,\n    show_column_dend = FALSE,\n    show_row_names = FALSE,\n    row_order = row_order,\n    row_names_side = \"left\",\n    row_split = row_split,\n    row_gap = unit(0, \"mm\"),\n    column_gap = unit(0, \"mm\"),\n    column_split = column_split,\n    left_annotation = left_anno,\n    top_annotation = top_anno,\n    bottom_annotation = bottom_anno,\n    column_title = NULL,\n    column_title_rot = 90,\n    column_title_side = \"bottom\",\n    heatmap_legend_param = list(title = \"scaled LFQ intensity\")\n)\n\nppreview(p, file = file.path(output_dir, \"heatmap.pdf\"))\n\n\nlibrary(ComplexHeatmap)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(circlize)\nlibrary(YRUtils)\n\nquant_file &lt;- \"temp/data.tsv\"\noutput_dir &lt;- \"temp\"\n\nquant_df &lt;- vroom(quant_file) %&gt;%\n    as.data.frame() %&gt;%\n    set_rownames(.[[\"protein\"]]) %&gt;%\n    select(-protein)\n\ncol_fun &lt;- colorRamp2(c(floor(min(quant_df) * 10) / 10, 1.0, ceiling(max(quant_df) * 10) / 10), c(\"skyblue\", \"white\", \"orange\"))\ncolumn_order &lt;- c(\n    \"WT\",\n    \"P152A\", \"H70Q\", \"A200V\", \"Y48N\", \"A200T\",\n    \"D99G\", \"L04P\", \"N199I\", \"N199H\", \"L54P\", \"K217R\", \"R18K\", \"R198Q\", \"R213W\",\n    \"C203S\", \"C206F\", \"C206Y\", \"V63L\", \"L66H\", \"P201S\", \"K209N\", \"R112P\", \"Q82H\"\n)\ncolumn_order_df &lt;- tibble(\n    cluster = c(\"WT\", rep(\"NO\", 5), rep(\"LOF\", 9), rep(\"GOF\", 9)),\n    protein = column_order\n) %&gt;%\n    mutate(\n        cluster = factor(cluster, levels = c(\"WT\", \"NO\", \"LOF\", \"GOF\")),\n        protein = factor(protein, levels = colnames(quant_df))\n    ) %&gt;%\n    arrange(protein)\nrow_order &lt;- c(\"GluA1_rep1\", \"GluA1_rep2\", \"GluA1_rep3\", \"GluA2_rep1\", \"GluA2_rep2\", \"GluA2_rep3\", \"GABAARAlpha1_rep1\", \"GABAARAlpha1_rep2\", \"GABAARAlpha1_rep3\", \"GABAARBeta1_rep1\", \"GABAARBeta1_rep2\", \"GABAARBeta1_rep3\")\nrow_split &lt;- gsub(\"_rep\\\\d+$\", \"\", row.names(quant_df))\nrow_split &lt;- factor(row_split, levels = unique(row_split))\nleft_anno &lt;- rowAnnotation(\n    Sample = gsub(\"_rep\\\\d+$\", \"\", row.names(quant_df)),\n    show_legend = FALSE,\n    show_annotation_name = FALSE,\n    simple_anno_size = unit(3, \"mm\"),\n    col = list(Sample = c(\"GluA1\" = \"#76EEC6\", \"GluA2\" = \"#EEAEEE\", \"GABAARAlpha1\" = \"#9F79EE\", \"GABAARBeta1\" = \"#EE799F\"))\n)\ntop_anno &lt;- columnAnnotation(\n    Type = column_order_df$cluster,\n    show_legend = FALSE,\n    show_annotation_name = FALSE,\n    simple_anno_size = unit(3, \"mm\"),\n    col = list(Type = c(\"WT\" = \"#BEBEBE\", \"NO\" = \"#EED8AE\", \"LOF\" = \"#EE8262\", \"GOF\" = \"#BCD2EE\"))\n)\nfont_family &lt;- \"Arial\"\n\n# `gpar()` is used to handle grid graphical parameters\n# for how to use `gpar()`, see `?gpar`\n# for ComplexHeatmap, you can set some parameters globally using `ht_opt`\n# e.g. `ht_opt$legend_title_gp &lt;- gpar(fontfamily = \"Times New Roman\")`\np &lt;- Heatmap(\n    quant_df,\n    col = col_fun,\n    cluster_columns = FALSE,\n    cluster_column_slices = FALSE,\n    cluster_rows = FALSE,\n    cluster_row_slices = FALSE,\n    show_column_names = TRUE,\n    show_column_dend = FALSE,\n    show_row_names = FALSE,\n    show_row_dend = FALSE,\n    column_order = column_order,\n    column_split = column_order_df$cluster,\n    row_order = row_order,\n    row_split = row_split,\n    row_gap = unit(0.8, \"mm\"),\n    column_gap = unit(0.8, \"mm\"),\n    left_annotation = left_anno,\n    top_anno = top_anno,\n    height = nrow(quant_df) * unit(5, \"mm\"),\n    width = ncol(quant_df) * unit(5, \"mm\"),\n    column_title_rot = 90,\n    row_title_rot = 0,\n    heatmap_legend_param = list(\n        title = \"normalized expression level\",\n        title_gp = gpar(fontfamily = font_family),\n        labels_gp = gpar(fontfamily = font_family)\n    ),\n    row_names_gp = gpar(fontfamily = font_family),\n    column_names_gp = gpar(fontfamily = font_family),\n    row_title_gp = gpar(fontfamily = font_family),\n    column_title_gp = gpar(fontfamily = font_family)\n)\nppreview(p, file = file.path(output_dir, \"heatmap.pdf\"))"
  },
  {
    "objectID": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#some-examples-about-usages-of-uniprot-and-string-apis",
    "href": "Blogs/Bioinformatics/posts/bulk proteomics/de_related_analysis_for_bulk_proteomics/index.html#some-examples-about-usages-of-uniprot-and-string-apis",
    "title": "DE-related analysis for bulk proteomics",
    "section": "8 Some examples about usages of UniProt and STRING APIs",
    "text": "8 Some examples about usages of UniProt and STRING APIs\n\nRetrieve subcellular locations and interactions from UniProt\n\n\nlibrary(vroom)\nlibrary(glue)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\n\nwork_dir &lt;- \"test\"\norganism_id &lt;- 9606\nurl_template &lt;- \"https://rest.uniprot.org/uniprotkb/search?query=(accession:{accession})%20AND%20(active:true)%20AND%20(organism_id:{organism_id})&fields=accession,gene_primary,organism_name,organism_id,protein_name,annotation_score,protein_existence,reviewed,ft_intramem,cc_subcellular_location,ft_topo_dom,ft_transmem,go,go_p,go_c,go_f,cc_interaction&format=tsv\"\nde_file &lt;- \"degs/xbfGlia_vs_xbfAstro.tsv\"\noutput_dir &lt;- \"subcellular_locations_and_interactions\"\n\nsetwd(work_dir)\ndir.create(output_dir, showWarnings = FALSE, recursive = FALSE)\n\ndf &lt;- vroom(de_file) %&gt;%\n    select(all_of(c(\"protein_name\", \"gene_name\", \"diff_flag\"))) %&gt;%\n    filter(diff_flag != \"NO\")\n\nres_df &lt;- tibble()\nfor (accession in unique(df$protein_name)) {\n    url_instance &lt;- glue(url_template)\n    e &lt;- try(item_raw_text &lt;- read_html(url_instance) %&gt;% html_text(), silent = T)\n    if (\"try-error\" %in% class(e)) {\n        message(\"invalid accession: \", accession)\n    } else {\n        item_df &lt;- vroom(I(item_raw_text), delim = \"\\t\") %&gt;%\n            mutate(\n                `Organism (ID)` = as.integer(`Organism (ID)`),\n                Annotation = as.integer(Annotation)\n            )\n        res_df &lt;- bind_rows(res_df, item_df)\n    }\n}\nres_df &lt;- distinct(res_df)\n\nres_df[[\"scl_secret\"]] &lt;- grepl(\"secret\", res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\nres_df[[\"scl_membrane\"]] &lt;- grepl(\"membrane\", res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\nres_df[[\"scl_extracellular\"]] &lt;- grepl(\"extracellular\", res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\nres_df[[\"scl_pass\"]] &lt;- res_df[[\"scl_secret\"]] | res_df[[\"scl_membrane\"]] | res_df[[\"scl_extracellular\"]]\n\nres_df &lt;- filter(res_df, scl_pass)\nnrow(res_df)\n\nres_df &lt;- inner_join(df, res_df, by = c(\"protein_name\" = \"Entry\")) %&gt;%\n    arrange(diff_flag)\nvroom_write(res_df, file = file.path(output_dir, gsub(\"\\\\.(txt|tsv)$\", \".secreted.tsv\", basename(de_file))), col_names = TRUE, append = FALSE)\n\nres_df &lt;- res_df %&gt;%\n    separate_rows(all_of(\"Interacts with\"), sep = \"; \") %&gt;%\n    mutate(\n        interacted_protein = `Interacts with`,\n        interacted_protein = if_else(\n            str_detect(interacted_protein, \".*\\\\[[a-zA-Z0-9-]+\\\\]$\"),\n            str_extract(interacted_protein, \"(?&lt;=\\\\[)[a-zA-Z0-9-]+(?=\\\\])\"),\n            interacted_protein\n        ),\n        interacted_protein = gsub(\"-[0-9]+$\", \"\", interacted_protein)\n    ) %&gt;%\n    filter(!is.na(interacted_protein))\n\ninteract_res_df &lt;- tibble()\nfor (accession in unique(res_df[[\"interacted_protein\"]])) {\n    url_instance &lt;- glue(url_template)\n    e &lt;- try(item_raw_text &lt;- read_html(url_instance) %&gt;% html_text(), silent = T)\n    if (\"try-error\" %in% class(e)) {\n        message(\"invalid accession: \", accession)\n    } else {\n        item_df &lt;- vroom(I(item_raw_text), delim = \"\\t\") %&gt;%\n            mutate(\n                `Organism (ID)` = as.integer(`Organism (ID)`),\n                Annotation = as.integer(Annotation)\n            )\n        interact_res_df &lt;- bind_rows(interact_res_df, item_df)\n    }\n}\ninteract_res_df &lt;- distinct(interact_res_df)\n\ninteract_res_df[[\"scl_secret\"]] &lt;- grepl(\"secret\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_membrane\"]] &lt;- grepl(\"membrane\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_extracellular\"]] &lt;- grepl(\"extracellular\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_pass\"]] &lt;- interact_res_df[[\"scl_secret\"]] | interact_res_df[[\"scl_membrane\"]] | interact_res_df[[\"scl_extracellular\"]]\n\ninteract_res_df &lt;- filter(interact_res_df, scl_pass) %&gt;%\n    mutate(interact_database = \"UniProt\")\nnrow(interact_res_df)\n\nfinal_res_df &lt;- inner_join(res_df, interact_res_df, by = c(\"interacted_protein\" = \"Entry\"), suffix = c(\".dep\", \".interacted\"))\nvroom_write(final_res_df, file = file.path(output_dir, gsub(\"\\\\.(txt|tsv)$\", \".interacted.uniprot.tsv\", basename(de_file))), col_names = TRUE, append = FALSE)\n\n\nRetrieve protein interactions from STRING\n\n\nlibrary(vroom)\nlibrary(glue)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(jsonlite)\n\nwork_dir &lt;- \"test\"\norganism_id &lt;- 9606\nurl_template &lt;- \"https://rest.uniprot.org/uniprotkb/search?query=(accession:{accession})%20AND%20(active:true)%20AND%20(organism_id:{organism_id})&fields=accession,gene_primary,organism_name,organism_id,protein_name,annotation_score,protein_existence,reviewed,ft_intramem,cc_subcellular_location,ft_topo_dom,ft_transmem,go,go_p,go_c,go_f,cc_interaction&format=tsv\"\nsubmit_cmd &lt;- \"curl --request POST 'https://rest.uniprot.org/idmapping/run' --form 'ids=\\\"{ids}\\\"' --form 'from=\\\"UniProtKB_AC-ID\\\"' --form 'to=\\\"UniProtKB\\\"'\"\nstatus_cmd &lt;- \"curl -i 'https://rest.uniprot.org/idmapping/status/{submit_cmd_instance_response_job_id}'\"\nresult_cmd &lt;- \"curl -s 'https://rest.uniprot.org/idmapping/results/{submit_cmd_instance_response_job_id}'\"\nmax_length &lt;- 20\nnetwork_file &lt;- \"string_db/9606.protein.physical.links.full.v12.0.txt.gz\"\ninfo_file &lt;- \"string_db/9606.protein.info.v12.0.txt.gz\"\nalias_file &lt;- \"string_db/9606.protein.aliases.v12.0.txt.gz\"\nsecret_file &lt;- \"subcellular_locations_and_interactions/xbfGlia_vs_xbfAstro.secreted.tsv\"\noutput_dir &lt;- \"subcellular_locations_and_interactions\"\n\nsetwd(work_dir)\ndir.create(output_dir, showWarnings = FALSE, recursive = FALSE)\n\ninfo_df &lt;- vroom(info_file) %&gt;%\n    select(all_of(c(\"#string_protein_id\", \"preferred_name\"))) %&gt;%\n    distinct()\n\nsecret_df &lt;- vroom(secret_file) %&gt;%\n    inner_join(info_df, by = c(\"gene_name\" = \"preferred_name\"))\n\nnetwork_df &lt;- vroom(network_file) %&gt;%\n    filter(protein1 %in% secret_df[[\"#string_protein_id\"]]) %&gt;%\n    inner_join(info_df, by = c(\"protein2\" = \"#string_protein_id\"))\n\nsource_levels &lt;- c(\"Ensembl_HGNC_uniprot_ids\", \"UniProt_ID\", \"UniProt_AC\")\nalias_df &lt;- vroom(alias_file) %&gt;%\n    filter((`#string_protein_id` %in% network_df$protein2) & (source %in% source_levels)) %&gt;%\n    mutate(source = factor(source, levels = source_levels)) %&gt;%\n    group_by(`#string_protein_id`) %&gt;%\n    slice_min(source) %&gt;%\n    slice_sample(n = 1) %&gt;%\n    ungroup()\n\nnetwork_df &lt;- inner_join(network_df, alias_df, by = c(\"protein2\" = \"#string_protein_id\"))\n\nuniq_ids &lt;- unique(network_df$alias)\nmpt_df &lt;- tibble()\nfor (id_vec in split(uniq_ids, ceiling(seq_along(uniq_ids) / max_length))) {\n    ids &lt;- paste0(id_vec, collapse = \",\")\n\n    submit_cmd_instance &lt;- glue(submit_cmd)\n    submit_cmd_instance_response &lt;- system(submit_cmd_instance, intern = TRUE, ignore.stdout = FALSE, ignore.stderr = FALSE, wait = TRUE)\n    submit_cmd_instance_response_ls &lt;- fromJSON(submit_cmd_instance_response)\n    submit_cmd_instance_response_job_id &lt;- submit_cmd_instance_response_ls$jobId\n\n    status_cmd_instance &lt;- glue(status_cmd)\n    while (TRUE) {\n        status_cmd_instance_response &lt;- system(status_cmd_instance, intern = TRUE, ignore.stdout = FALSE, ignore.stderr = FALSE, wait = TRUE)\n        status_cmd_instance_response &lt;- trimws(status_cmd_instance_response)\n        status_cmd_instance_response_ls &lt;- fromJSON(status_cmd_instance_response[length(status_cmd_instance_response)])\n        if ((status_cmd_instance_response[1] == \"HTTP/2 303\") && (status_cmd_instance_response_ls$jobStatus == \"FINISHED\")) {\n            message(\"results can be retrieved!\")\n            break\n        }\n        Sys.sleep(3)\n    }\n\n    result_cmd_instance &lt;- glue(result_cmd)\n    result_cmd_instance_response &lt;- system(result_cmd_instance, intern = TRUE, ignore.stdout = FALSE, ignore.stderr = FALSE, wait = TRUE)\n    result_cmd_instance_response_ls &lt;- fromJSON(result_cmd_instance_response)\n    if (length(result_cmd_instance_response_ls$results) == 0) {\n        stop(\"no ID was mapped successfully\")\n    }\n    tmp_df &lt;- result_cmd_instance_response_ls$results\n    mpt_df &lt;- bind_rows(mpt_df, tmp_df)\n}\nmpt_df &lt;- distinct(mpt_df)\n\nnetwork_df &lt;- inner_join(network_df, mpt_df, by = c(\"alias\" = \"from\"))\n\ninteract_res_df &lt;- tibble()\nfor (accession in unique(network_df$to)) {\n    url_instance &lt;- glue(url_template)\n    e &lt;- try(item_raw_text &lt;- read_html(url_instance) %&gt;% html_text(), silent = T)\n    if (\"try-error\" %in% class(e)) {\n        message(\"invalid accession: \", accession)\n    } else {\n        item_df &lt;- vroom(I(item_raw_text), delim = \"\\t\") %&gt;%\n            mutate(\n                `Organism (ID)` = as.integer(`Organism (ID)`),\n                Annotation = as.integer(Annotation)\n            )\n        interact_res_df &lt;- bind_rows(interact_res_df, item_df)\n    }\n}\ninteract_res_df &lt;- distinct(interact_res_df)\n\ninteract_res_df[[\"scl_secret\"]] &lt;- grepl(\"secret\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_membrane\"]] &lt;- grepl(\"membrane\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_extracellular\"]] &lt;- grepl(\"extracellular\", interact_res_df[[\"Subcellular location [CC]\"]], ignore.case = TRUE)\ninteract_res_df[[\"scl_pass\"]] &lt;- interact_res_df[[\"scl_secret\"]] | interact_res_df[[\"scl_membrane\"]] | interact_res_df[[\"scl_extracellular\"]]\n\ninteract_res_df &lt;- filter(interact_res_df, scl_pass) %&gt;%\n    mutate(interact_database = \"STRING\")\nnrow(interact_res_df)\n\nfinal_res_df &lt;- inner_join(secret_df, network_df, by = c(\"#string_protein_id\" = \"protein1\")) %&gt;%\n    inner_join(interact_res_df, by = c(\"to\" = \"Entry\"), suffix = c(\".dep\", \".interacted\"))\nvroom_write(final_res_df, file = file.path(output_dir, gsub(\"\\\\.secreted\\\\.(txt|tsv)$\", \".interacted.string.tsv\", basename(secret_file))), col_names = TRUE, append = FALSE)"
  },
  {
    "objectID": "Blogs/Computer/index.html",
    "href": "Blogs/Computer/index.html",
    "title": "Computer",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCommonly used Windows commands\n\n\n\n\n\n\nwindows\n\n\ncommand\n\n\n\n\n\n\n\n\n\nDec 23, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCommonly used Linux commands\n\n\n\n\n\n\nlinux\n\n\ncommand\n\n\n\n\n\n\n\n\n\nDec 19, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nSticky notes\n\n\n\n\n\n\nmisc\n\n\nnote\n\n\n\n\n\n\n\n\n\nOct 13, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure mstsc RDP\n\n\n\n\n\n\nwindows\n\n\nmstsc\n\n\nrdp\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nCompile and install R\n\n\n\n\n\n\nr\n\n\ncompile\n\n\ninstall\n\n\n\n\n\n\n\n\n\nOct 7, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to systemd service\n\n\n\n\n\n\nlinux\n\n\nsystemd\n\n\nservice\n\n\n\n\n\n\n\n\n\nOct 6, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nGit syntax basics\n\n\n\n\n\n\ngit\n\n\nsyntax\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nJulia resource list\n\n\n\n\n\n\njulia\n\n\nresource\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nJulia syntax basics\n\n\n\n\n\n\nsyntax\n\n\njulia\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nR resource list\n\n\n\n\n\n\nr\n\n\nresource\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Computer"
    ]
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/introduction_to_systemd_service/index.html",
    "href": "Blogs/Computer/posts/OS/Linux/introduction_to_systemd_service/index.html",
    "title": "Introduction to systemd service",
    "section": "",
    "text": "Systemd 是 Linux 的一个系统和服务管理器。\nSystemd service 脚本一般存放于 /etc/systemd 和 /usr/lib/systemd，前者包含 *.target.wants 文件，后者为安装软件生成 service 的目录。\n前述两个目录下均包含 system 和 user 目录，前者是系统服务，即开机不需要用户登录即可运行的服务，后者是用户服务，是需要用户登录后才能运行的服务。\nService 脚本以 .service 结尾，包含 Unit、Service 和 Install 三个区块，其中\n\n\n\n区块\n描述\n\n\n\n\nUnit 区块\n描述启动依赖关系\n\n\nService 区块\n定义启动行为\n\n\nInstall 区块\n定义服务安装\n\n\n\n\nUnit 区块常见的描述字段\n\n\n服务描述\n\n\n\n\n字段\n描述\n\n\n\n\nDescription\n当前服务的简短描述\n\n\nDocumentation\n文档位置\n\n\n\n\n启动顺序\n\n\n\n\n字段\n描述\n\n\n\n\nAfter\n定义该服务应该在哪些服务（target or service）之后启动\n\n\nBefore\n定义该服务应该在哪些服务（target or service）之前启动\n\n\n\n\n依赖关系\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nWants\n表示该服务与指定的服务存在“弱依赖”关系，即指定的服务启动失败或退出并不影响该服务的运行\n\n\nRequires\n表示“强依赖”关系，即指定的服务启动失败或退出则该服务也会退出\n\n\n\n\nService 区块常见的描述字段\n\n\n启动命令\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nEnvironmentFile\n指定当前服务的环境参数文件\n\n\nEnvironment\n后可接多个不同的 Shell 变量\n\n\nUser\n设置运行服务的用户\n\n\nGroup\n设置运行服务的用户组\n\n\nWorkingDirectory\n设置运行服务的路径\n\n\nExec(Start|Stop|StartPre|StartPost|StopPost|Reload)\n各种与执行相关的命令\n\n\n\n\n启动类型\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nType\n可设置的值如下：\n\nsimple（默认值）：ExecStart 启动的进程为主进程，即直接启动服务进程。\nforking：ExecStart 字段以 fork() 方式启动。\noneshot：类似于 simple，但只执行一次，Systemd 会等它执行完才启动其它服务。\ndbus: 类似于 simple，但会在 D-Bus 信号后启动。\nnotify：类似于 simple，启动结束后会发出通知信号，然后 Systemd 再启动其它服务。\nidle：类似于 simple，但要等到其它任务都执行完，才会启动服务。\n\n\n\nRemainAfterExit\nyes 表示进程退出后，服务仍然保持执行\n\n\n\n\n重启行为\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nKillMode\n可设置的值如下：\n\ncontrol-group（默认值）：当前控制组里面的所有子进程都会被杀掉。\nprocess：只杀主进程。\nmixed：主进程收到 SIGTERM 信号，子进程收到 SIGKILL 信号。\nnone：没有进程会被杀掉，只是执行服务的 stop 命令。\n\n\n\nRestart\n定义服务退出后 Systemd 的重启方式。可设置的值如下：\n\nno（默认值）：退出后不会重启。\non-success：只有正常退出时才会重启。\non-failure：非正常退出时，包括被信号终止和超时才会重启。\non-abnormal：只有被信号终止和超时才会重启。\non-abort：只有在收到没有被捕捉到的终止信号才会重启。\non-watchdog：超时退出才会重启。\nalways：总是重启。\n\n\n\nRestartSec\nSystemd 重启前需等待的秒数\n\n\n\n\nInstall 区块\n\n\n\n\n字段\n描述\n\n\n\n\nWantedBy\n表示该服务所在的 Target\n\n\n\nTarget 的含义是服务组，如 WantedBy=multi-user.target 指的是该服务属于 multi-user.target。当执行 systemctl enable xxx.servive 时，xxx.service 的符号链接就会被创建在 /etc/systemd/system/multi-user.target.wants 目录下。可以通过 systemctl get-default 查看系统默认启动的 target。在配置好相应的 WantedBy 字段后，可以实现服务的开机自启动。\n假设我们已经编写好服务脚本 /etc/systemd/system/clash_galaxy.service，其内容如下：\n[Unit]\nDescription=The internet proxy clash for galaxy.\nAfter=network.target\n\n[Service]\nType=simple\nUser=galaxy\nRestart=on-abort\nExecStart=/home/galaxy/.config/mihomo/clash\n\n[Install]\nWantedBy=graphical.target\n首先，利用 sudo systemctl daemon-reload 重新加载所有 Systemd 服务，否则会找不到 clash_galaxy.service 服务。\n接着就可以使用下述命令来控制服务：\n# 自启动|启动|停止|重启|查看状态\nsudo systemctl [enable|start|stop|restart|status] clash_galaxy.service\n执行启动命令：sudo systemctl start clash_galaxy.service。\n如果希望服务在系统启动时自启动，则需执行命令：sudo systemctl enable clash_galaxy.service。"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Linux/introduction_to_systemd_service/index.html#introduction",
    "href": "Blogs/Computer/posts/OS/Linux/introduction_to_systemd_service/index.html#introduction",
    "title": "Introduction to systemd service",
    "section": "",
    "text": "Systemd 是 Linux 的一个系统和服务管理器。\nSystemd service 脚本一般存放于 /etc/systemd 和 /usr/lib/systemd，前者包含 *.target.wants 文件，后者为安装软件生成 service 的目录。\n前述两个目录下均包含 system 和 user 目录，前者是系统服务，即开机不需要用户登录即可运行的服务，后者是用户服务，是需要用户登录后才能运行的服务。\nService 脚本以 .service 结尾，包含 Unit、Service 和 Install 三个区块，其中\n\n\n\n区块\n描述\n\n\n\n\nUnit 区块\n描述启动依赖关系\n\n\nService 区块\n定义启动行为\n\n\nInstall 区块\n定义服务安装\n\n\n\n\nUnit 区块常见的描述字段\n\n\n服务描述\n\n\n\n\n字段\n描述\n\n\n\n\nDescription\n当前服务的简短描述\n\n\nDocumentation\n文档位置\n\n\n\n\n启动顺序\n\n\n\n\n字段\n描述\n\n\n\n\nAfter\n定义该服务应该在哪些服务（target or service）之后启动\n\n\nBefore\n定义该服务应该在哪些服务（target or service）之前启动\n\n\n\n\n依赖关系\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nWants\n表示该服务与指定的服务存在“弱依赖”关系，即指定的服务启动失败或退出并不影响该服务的运行\n\n\nRequires\n表示“强依赖”关系，即指定的服务启动失败或退出则该服务也会退出\n\n\n\n\nService 区块常见的描述字段\n\n\n启动命令\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nEnvironmentFile\n指定当前服务的环境参数文件\n\n\nEnvironment\n后可接多个不同的 Shell 变量\n\n\nUser\n设置运行服务的用户\n\n\nGroup\n设置运行服务的用户组\n\n\nWorkingDirectory\n设置运行服务的路径\n\n\nExec(Start|Stop|StartPre|StartPost|StopPost|Reload)\n各种与执行相关的命令\n\n\n\n\n启动类型\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nType\n可设置的值如下：\n\nsimple（默认值）：ExecStart 启动的进程为主进程，即直接启动服务进程。\nforking：ExecStart 字段以 fork() 方式启动。\noneshot：类似于 simple，但只执行一次，Systemd 会等它执行完才启动其它服务。\ndbus: 类似于 simple，但会在 D-Bus 信号后启动。\nnotify：类似于 simple，启动结束后会发出通知信号，然后 Systemd 再启动其它服务。\nidle：类似于 simple，但要等到其它任务都执行完，才会启动服务。\n\n\n\nRemainAfterExit\nyes 表示进程退出后，服务仍然保持执行\n\n\n\n\n重启行为\n\n\n\n\n\n\n\n\n字段\n描述\n\n\n\n\nKillMode\n可设置的值如下：\n\ncontrol-group（默认值）：当前控制组里面的所有子进程都会被杀掉。\nprocess：只杀主进程。\nmixed：主进程收到 SIGTERM 信号，子进程收到 SIGKILL 信号。\nnone：没有进程会被杀掉，只是执行服务的 stop 命令。\n\n\n\nRestart\n定义服务退出后 Systemd 的重启方式。可设置的值如下：\n\nno（默认值）：退出后不会重启。\non-success：只有正常退出时才会重启。\non-failure：非正常退出时，包括被信号终止和超时才会重启。\non-abnormal：只有被信号终止和超时才会重启。\non-abort：只有在收到没有被捕捉到的终止信号才会重启。\non-watchdog：超时退出才会重启。\nalways：总是重启。\n\n\n\nRestartSec\nSystemd 重启前需等待的秒数\n\n\n\n\nInstall 区块\n\n\n\n\n字段\n描述\n\n\n\n\nWantedBy\n表示该服务所在的 Target\n\n\n\nTarget 的含义是服务组，如 WantedBy=multi-user.target 指的是该服务属于 multi-user.target。当执行 systemctl enable xxx.servive 时，xxx.service 的符号链接就会被创建在 /etc/systemd/system/multi-user.target.wants 目录下。可以通过 systemctl get-default 查看系统默认启动的 target。在配置好相应的 WantedBy 字段后，可以实现服务的开机自启动。\n假设我们已经编写好服务脚本 /etc/systemd/system/clash_galaxy.service，其内容如下：\n[Unit]\nDescription=The internet proxy clash for galaxy.\nAfter=network.target\n\n[Service]\nType=simple\nUser=galaxy\nRestart=on-abort\nExecStart=/home/galaxy/.config/mihomo/clash\n\n[Install]\nWantedBy=graphical.target\n首先，利用 sudo systemctl daemon-reload 重新加载所有 Systemd 服务，否则会找不到 clash_galaxy.service 服务。\n接着就可以使用下述命令来控制服务：\n# 自启动|启动|停止|重启|查看状态\nsudo systemctl [enable|start|stop|restart|status] clash_galaxy.service\n执行启动命令：sudo systemctl start clash_galaxy.service。\n如果希望服务在系统启动时自启动，则需执行命令：sudo systemctl enable clash_galaxy.service。"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Windows/configure_mstsc_rdp/index.html",
    "href": "Blogs/Computer/posts/OS/Windows/configure_mstsc_rdp/index.html",
    "title": "Configure mstsc RDP",
    "section": "",
    "text": "Once you have created a RDP file from mstsc or anywhere else, you can save your user name and password in your RDP file to achieve one-step login by adding or modifying the following two lines:\nusername:s:&lt;type your user name here&gt;\npassword 51:b:&lt;type your encrypted password here&gt;\nThe encrypted password can be generated from your literal password in PowerShell by typing the command:\n(\"&lt;type your literal password here&gt;\" | ConvertTo-SecureString -AsPlainText -Force) | ConvertFrom-SecureString"
  },
  {
    "objectID": "Blogs/Computer/posts/OS/Windows/configure_mstsc_rdp/index.html#save-user-name-and-password-in-rdp-file",
    "href": "Blogs/Computer/posts/OS/Windows/configure_mstsc_rdp/index.html#save-user-name-and-password-in-rdp-file",
    "title": "Configure mstsc RDP",
    "section": "",
    "text": "Once you have created a RDP file from mstsc or anywhere else, you can save your user name and password in your RDP file to achieve one-step login by adding or modifying the following two lines:\nusername:s:&lt;type your user name here&gt;\npassword 51:b:&lt;type your encrypted password here&gt;\nThe encrypted password can be generated from your literal password in PowerShell by typing the command:\n(\"&lt;type your literal password here&gt;\" | ConvertTo-SecureString -AsPlainText -Force) | ConvertFrom-SecureString"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html",
    "title": "Git syntax basics",
    "section": "",
    "text": "Set the user name and email address\n\ngit config --global user.name \"Rui Yang\"\ngit config --global user.email \"neurospace@petalmail.com\"\n\nImprove the output readability\n\ngit config --global color.ui auto\nThese settings will be stored in the file ~/.gitconfig in the following form:\n[user]\n        name = Rui Yang\n        email = neurospace@petalmail.com\n[color]\n        ui = auto"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#git-initial-settings",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#git-initial-settings",
    "title": "Git syntax basics",
    "section": "",
    "text": "Set the user name and email address\n\ngit config --global user.name \"Rui Yang\"\ngit config --global user.email \"neurospace@petalmail.com\"\n\nImprove the output readability\n\ngit config --global color.ui auto\nThese settings will be stored in the file ~/.gitconfig in the following form:\n[user]\n        name = Rui Yang\n        email = neurospace@petalmail.com\n[color]\n        ui = auto"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#github-initial-settings",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#github-initial-settings",
    "title": "Git syntax basics",
    "section": "2 GitHub initial settings",
    "text": "2 GitHub initial settings\n\nSet SSH key\n\n# don't forget to enter passphrase\nssh-keygen -t rsa -C \"neurospace@petalmail.com\"\nThis command will create two files id_rsa (private key), and id_rsa.pub (public key) under ~/.ssh.\n\nAdd id_rsa.pub to GitHub\nValidate your settings\n\nssh -T git@github.com\n\nThen you can use SSH key to clone and push a repository from and to GitHub\n\ngit clone git@github.com:yr-neurospace/YRUtils.jl.git"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#git-operations",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#git-operations",
    "title": "Git syntax basics",
    "section": "3 Git operations",
    "text": "3 Git operations\n\n3.1 Basic operations\n\ngit init —— initialize a repository\n\n# create a directory first\nmkdir git-tutorial\ncd git-tutorial\n# initialize a repository\n# this will create a .git directory in the curent working directory\n# it records all needed for managing this repository\ngit init\n\n# 提示：使用 'master' 作为初始分支的名称。这个默认分支名称可能会更改。要在新仓库中\n# 提示：配置使用初始分支名，并消除这条警告，请执行：\n# 提示：\n# 提示：  git config --global init.defaultBranch &lt;名称&gt;\n# 提示：\n# 提示：除了 'master' 之外，通常选定的名字有 'main'、'trunk' 和 'development'。\n# 提示：可以通过以下命令重命名刚创建的分支：\n# 提示：\n# 提示：  git branch -m &lt;name&gt;\n# 已初始化空的 Git 仓库于 /home/yangrui/temp/git-tutorial/.git/\n\ngit status —— check the status of a repository\n\ngit status\n\n# 位于分支 master\n\n# 尚无提交\n\n# 无文件要提交（创建/拷贝文件并使用 \"git add\" 建立跟踪）\n\n# we have not any files to be committed yet\n# let's create a README.md file for the first commit\ntouch README.md\ngit status\n\n# 位于分支 master\n\n# 尚无提交\n\n# 未跟踪的文件:\n#   （使用 \"git add &lt;文件&gt;...\" 以包含要提交的内容）\n#         README.md\n\n# 提交为空，但是存在尚未跟踪的文件（使用 \"git add\" 建立跟踪）\n\ngit add —— add files to staging area\n\n# when we create a file, it won't be managed automatically by git\n# we can see that README.md created above is listed as \"Untracted files\"\n# before adding README.md to our repository formally, we first need to add it to the staging area\n# which can be considered as a buffer area\ngit add README.md\ngit status\n\n# 位于分支 master\n\n# 尚无提交\n\n# 要提交的变更：\n#   （使用 \"git rm --cached &lt;文件&gt;...\" 以取消暂存）\n#         新文件：   README.md\n\ngit commit —— add files in the staging area to our repository formally\n\n# content after -m is a summary of the current commit\n# if you want to write more about the commit\n# you can run \"git commit\" without -m directly\n# this will open a text editor\n# and then you can enter more about the commit\n# by convention, the content you enter should be in the format:\n# line 1: a brief summary about the commit\n# line 2: blank line\n# line 3 and after: detailed info about the commit\n# note: if you want to abort the commit, don't enter any, and then close the editor directly\ngit commit -m \"First commit\"\n\n# [master（根提交） 7f9f90e] First commit\n#  1 file changed, 0 insertions(+), 0 deletions(-)\n#  create mode 100644 README.md\n\ngit log —— check commit log\n\n# the string \"7f9f90ef2c6d56862f096f3e0e288af5c3024097\" by \"commit\" in the console output is a hash value\n# which can be used to locate this commit uniquely\ngit log\n\n# commit 7f9f90ef2c6d56862f096f3e0e288af5c3024097 (HEAD -&gt; master)\n# Author: Rui Yang &lt;neurospace@petalmail.com&gt;\n# Date:   Tue May 14 13:26:07 2024 +0800\n\n#     First commit\n\n# to show log in graph\ngit log --graph\n\n# to show more concise info each commit\ngit log --pretty=short\n\n# to show log associated with a specific dir or file\ngit log README.md\n\n# to show modifications added by the last commit\ngit log -p\n\n# to show modifications associated with a specific dir or file added by the last commit\ngit log -p README.md\n\ngit diff —— check modifications\n\n# now let's add \"# Git Guide\" to the README.md\n# this modifies README.md in the working tree\n# the following command displays the difference between the working tree and the staging area\n# due to nothing existed in the staging area\n# it turns to display the difference between the working tree and the last commit\ngit diff\n\n# diff --git a/README.md b/README.md\n# index e69de29..0a49ef6 100644\n# --- a/README.md\n# +++ b/README.md\n# @@ -0,0 +1 @@\n# +# Git Guide\n\ngit add README.md\n\n# if you run \"git diff\" now, it will print nothing\n# because the status between the working tree and the staging area is of no diffrence\n# to check the difference between the working tree and the last commit\n# using\ngit diff HEAD\n\n# diff --git a/README.md b/README.md\n# index e69de29..0a49ef6 100644\n# --- a/README.md\n# +++ b/README.md\n# @@ -0,0 +1 @@\n# +# Git Guide\n\ngit commit -m \"Add index\"\n\n\n3.2 Operations on branches\nThe master branch is created by Git by default.\nWe can create multiple branches from the master branch or other branches.\nEach branch can be developed at the same time and is usually dedicated to a specific feature development (including fixing some bugs). Once the development of some branch is done, we can merge it with the master branch (in most cases).\n\ngit branch —— list all branches\n\n# the active branch is indicated by an * beside the branch name\ngit branch\n\n# * master\n\ngit checkout -b —— create and switch to a new branch\n\n# create and switch to the new branch feature-A from the master branch\n# if this is done\n# then all modifications take effect to the feature-A branch, not the master branch\ngit checkout -b feature-A\n# the above command can also be achieved by combining the two\ngit branch feature-A  # create\ngit checkout feature-A  # switch to\n\n# 切换到一个新分支 'feature-A'\n\ngit branch\n\n# * feature-A\n#   master\n\n# add a new line to README.md like\n# # Git Guide\n\n#   - feature-A\n\ngit add README.md\ngit commit -m \"Add feature-A\"\n\ngit merge —— merge branches\n\n# switch to the master branch\ngit checkout master\n# merge feature-A to master\n# create a merge commit in all cases using --no-ff\ngit merge --no-ff feature-A\n\n\n3.3 Modify commits\n\ngit reset --hard —— back to a specific status\n\n# purpose:\n# back to the status before creating the feature-A branch\n# then create a new branch named fix-B\n# you need to provide the hash value of some commit, to which you want to back\n# you can query it using \"git log\"\n# because we want to back to the status before creating the feature-A branch\n# we back to the commit \"Add index\", which is the last commit before creating the feature-A branch\ngit reset --hard e4fbb2140a7a25d8fa03a53bbf2129c5d8d56aaa\n\n# HEAD 现在位于 e4fbb21 Add index\n\ngit checkout -b fix-B\n\n# add a new line to README.md like\n# # Git Guide\n\n#   - fix-B\n\ngit add README.md\ngit commit -m \"Fix B\"\n\n# forward to the status after merging feature-A\n# because \"git log\" can only query the history up to the current status\n# we use \"git reflog\" to query the repository log to retrieve the hash value we want\ngit checkout master\ngit reset --hard c61dab8\n\n# 切换到分支 'master'\n# HEAD 现在位于 c61dab8 Merge branch 'feature-A'\n\n# merge fix-B to master\ngit merge --no-ff fix-B\n\n# 自动合并 README.md\n# 冲突（内容）：合并冲突于 README.md\n# 自动合并失败，修正冲突然后提交修正的结果。\n\n# this tells us that there is a conflict in README.md between feature-A and fix-B\n# now we need to open the README.md\n# then we will see the following\n\n# # Git Guide\n\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n#   - feature-A\n# =======\n#   - fix-B\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; fix-B\n\n# the content above ======= is the content in HEAD\n# below is the content in fix-B\n# now we modify it to what we want after merging manually\n\n# # Git Guide\n\n#   - feature-A\n#   - fix-B\n\n# now just add and commit it\ngit add README.md\ngit commit -m \"Fix conflict\"\n\ngit commit --amend —— modify the last commit\n\n# this will open a text editor\n# you can modify the last commit summary and then save and exit\ngit commit --amend\n\ngit rebase -i —— change/merge history\n\n# create and switch to the branch feature-C\ngit checkout -b feature-C\n\n# modify README.md like\n# which contains a small typo error\n# # Git Guide\n\n#   - feature-A\n#   - fix-B\n#   - faeture-C\n\n# -a option automatically stages modified and deleted files, not new files\ngit commit -am \"Add feature-C\"\n\n# now fix the typo error\n# # Git Guide\n\n#   - feature-A\n#   - fix-B\n#   - feature-C\n\ngit commit -am \"Fix typo\"\n\n# in fact, we almost don't expect such a commit to be recorded in our commit history at any time\n# so we hope to merge it into the last commit instead of creating a new commit for it\n# here we use \"git rebase -i\" to do this\n# HEAD~2 means that we pick the last two commits and open them in a text editor\ngit rebase -i HEAD~2\n\n# replace \"pick\" with \"fixup\" in the line of \"Fix typo\"\n\n# 成功变基并更新 refs/heads/feature-C。\n\n# now we can see that the commit \"Fix typo\" is not existed in our log\ngit log\n\ngit checkout master\ngit merge --no-ff feature-C\n\n\n3.4 Push to remote repository\n# git@github.com:&lt;account name&gt;/&lt;repository name&gt;.git\n# add a remote repository for our local repository\n# Git will set \"origin\" as the name of the remote repository\ngit remote add origin git@github.com:yr-neurospace/git-tutorial.git\n\n# push the content of the current branch to the master branch of the remote repository origin\n# when using -u option, it will set the master branch of the remote repository origin as the upstream of the local branch used now\n# this will let the local branch used now automatically pull content from the master branch of the remote repository origin when using \"git pull\" command without adding extra options\ngit push -u origin master\n\n\n3.5 Pull from remote repository\n\ngit clone —— pull a remote repository\n\n# by default, we are in the master branch\n# at the same time, Git will set origin as the identifier of the remote repository\n# in other words, the local master branch and the remote master branch are identical\ngit clone git@github.com:yr-neurospace/git-tutorial.git\n\n# show branch info for both local and remote repositories\ngit branch -a\n\n# create the feature-D branch in local\n# the content of which is from the feature-D branch of the remore origin repository\ngit checkout -b feature-D origin/feature-D\n\n# pull the latest feature-D branch to local from the remote repository origin\ngit pull origin feature-D"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#some-github-functions",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#some-github-functions",
    "title": "Git syntax basics",
    "section": "4 Some GitHub functions",
    "text": "4 Some GitHub functions\n\n4.1 Issue\nThe system managing Issue is called Bug Tracking System (BTS).\nIn GitHub, you can use Issue to\n\nreport BUG\ncommunicate among developers\nmake to-do list\n\nIn Issue, you can use the GitHub Flavored Markdown (GFM) syntax to organize your content. This will make your content clearer. Especially, you can use the Tasklist syntax to make a to-do list:\n# To do\n\n- [] add an attractive logo\n- [x] finish deployment\n- [] add sampling tool\nTask marked by [x] is done.\nIn addition, as we all have seen that every Issue has been assigned an unique number, say “#24”. We can associate a commit to one or more commits by adding those Issue’s numbers to our commit log, e.g. Add feature #24.\nWe can also use this way to close an Issue by describing a commit in one of the following formats:\nfix #24\nfixes #24\nfixed #24\n\nclose $24\ncloses #24\nclosed #24\n\nresolve #24\nresolves #24\nresolved #24\nWe may also convert an Issue to a Pull Request, because the numbers of Issue and Pull Request are interoperable."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#send-pull-request",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#send-pull-request",
    "title": "Git syntax basics",
    "section": "5 Send Pull Request",
    "text": "5 Send Pull Request\n\nFork the repository you want to develop in GitHub.\nClone the forked repository to our local development environment using git clone command.\nCreate a feature branch from the target branch.\n\nGenerally, we first create a feature branch and then develop new functions or fix some bugs in this feature branch.\n\nDeveloping in the feature branch.\nadd and then commit our modifications.\nCreate the remote feature branch and then push the content of the local feature branch to it.\n\ne.g., git push origin work will create a remote feature branch called work and then push the local content to it.\n\nSend Pull Request in GitHub (don’t forget to switch to the feature branch).\n\nIn addition, to keep up-to-date with the original repository, not the forked repository under our account, we can set the original repository as our “upstream” repository with the command git remote add upstream &lt;the original repository&gt;.\nOnce set, we can fetch the latest source code from the original repository, and then merge it with the corresponding branch in our local repository, so we can perform development based the latest version.\ngit fetch upstream\ngit merge upstream/master"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#accept-pull-request",
    "href": "Blogs/Computer/posts/Programming/Git/git_syntax_basics/index.html#accept-pull-request",
    "title": "Git syntax basics",
    "section": "6 Accept Pull Request",
    "text": "6 Accept Pull Request\n\nClone the remote repository accepting pull request to local.\nSet the remote repository sending pull request as the remote repository of our cloned local repository with specific name e.g. “PRRepo”, and then fetch it to local.\nNow we have both the repository accepting pull request and the repository sending pull request. Next, we create a feature branch used to test merging from the repository accepting pull request. And then merge the pull request in the merging test branch. Once check passed, we can delete the merging test branch by using git branch -D &lt;merging test branch name&gt;.\nFormally merge the pull request by one of the two methods:\n\nClick Merge pull request in GitHub.\nMerge pull request in the local and then push it to the remote."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html",
    "title": "Julia syntax basics",
    "section": "",
    "text": "Interactive programming\n\nJulia is a dynamically typed language, in contrast with statically typed languages.\n\nHigh performance\n\nJulia uses just-in-time compilation (JIT), compilation at run time.\nTypically, JIT continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code.\nTherefore, JIT combines advantages of ahead-of-time compilation (AOT, compilation before execution) and interpretation.\nDue to the ecosystem of packages, Julia is really suitable for scientific computing, but it can also be used as a general-purpose programming language.\n\n\n\nJulia starts more slowly than Python, R, etc. but begins to run faster once the JIT compiler has converted critical parts of the code to machine code; thus it’s not suitable for:\n\nProgramming small, short-running scripts.\nReal-time systems (Julia implements automatic garbage collection, which tends to introduce small random delays).\nSystem programming (it needs detailed control of resource usage).\nEmbedded systems with limited memory."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#julia-pros-and-cons",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#julia-pros-and-cons",
    "title": "Julia syntax basics",
    "section": "",
    "text": "Interactive programming\n\nJulia is a dynamically typed language, in contrast with statically typed languages.\n\nHigh performance\n\nJulia uses just-in-time compilation (JIT), compilation at run time.\nTypically, JIT continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code.\nTherefore, JIT combines advantages of ahead-of-time compilation (AOT, compilation before execution) and interpretation.\nDue to the ecosystem of packages, Julia is really suitable for scientific computing, but it can also be used as a general-purpose programming language.\n\n\n\nJulia starts more slowly than Python, R, etc. but begins to run faster once the JIT compiler has converted critical parts of the code to machine code; thus it’s not suitable for:\n\nProgramming small, short-running scripts.\nReal-time systems (Julia implements automatic garbage collection, which tends to introduce small random delays).\nSystem programming (it needs detailed control of resource usage).\nEmbedded systems with limited memory."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#basics",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#basics",
    "title": "Julia syntax basics",
    "section": "2 Basics",
    "text": "2 Basics\n\n2.1 Arithmetic operations and number types\n\n2.1.1 Arithmetic operations\nAddition, subtraction, multiplication, division, and power: + - * / ^.\n\n\n2.1.2 Number types\n\nSigned integers: Int8, Int16, Int32, Int64 (default), Int128, BigInt.\nUnsigned integers: UInt8, UInt16, UInt32, UInt64, UInt128.\n\nYou can check the minimum and maximum values of a certain integer type with typemin() and typemax().\nYou can check the type of the input argument with typeof().\nJulia defaults to showing all signed integers in decimal format, and all unsigned integers in hexadecimal format.\nIn fact, what is stored in memory is no difference. The only difference is how to interpret it. You can use the reinterpret() function to see how the exactly same bits in memory can be interpreted differently.\n\nFloating-point numbers: Float16, Float32, Float64 (default).\n\nYou can type a Float32 number by suffixing f0: 3.14f0.\n\nRational type: 2 // 5 represents a rational number \\(\\frac{2}{5}\\).\nComplex type: 1 + 2im.\n\n\n\n2.1.3 Arithmetic operations for integers\n\n/ always gives floating-point number\n\n\n4 / 2\n\n2.0\n\n\n\n÷ or div() gives the quotient\n\n\n5 ÷ 3\n\n1\n\n\n\ndiv(5, 3)\n\n1\n\n\n\n% or rem() gives the remainder\n\n\n5 % 3\n\n2\n\n\n\nrem(5, 3)\n\n2\n\n\n\ndivrem() gives both quotient and remainder\n\n\ndivrem(5, 3)\n\n(1, 2)\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTypically, operations on the same type of values always give the same type of value, even though overflow may occur.\nEven though overflow will occur, Julia won’t give any prompt.\n\n\n\n\n\n2.2 Variables\nIn julia, identifiers can be used to give names to constants, variables, types, and functions.\nVariables defining memory addresses where values are stored, are only references to values, because Julia allocates memory based on values, not variables.\nIn comparison with this, statically typed languages allocate memory based on variables, so you must first decalre the type of a variable (e.g., int) before using it, which will allocate a predefined size (which depends on the type of the variable) in a predefined location in memory to this variable. As a consequence, you should never attempt to assign a value that cannot fit inside the memory slot set aside for the variable to this variable.\n\nThe equal sign (=) operator is used to assign values to variables (i.e., let a variable point to a value):\n\n\nx = 1\n\n1\n\n\n\nAllowed variable names:\n\n\nLeading characters: letters, underscore, Unicode code points greater than 00A0.\nSubsequent characters: other Unicode code points.\nVariable names containing only underscores can only be assigned values, which are immediately discarded.\nExplicitly disallowed variable names: built-in keywords.\n\n\n\n\n\n\n\nTip\n\n\n\nTo type many special characters, like Unicode math symbols, you can type the backslashed LaTeX symbol name followed by tab.\nIf you find a symbol elsewhere, which you don’t know how to type, the REPL help will tell you: just type ? and then paste the symbol.\n\n\n\nMy own rules for clarity:\n\n\nCan only contain letters, underscore, and numbers.\nCan only start with letters.\n\n\nTwo special variables:\n\n\nConstants: defined with the const keyword.\n\n\nconst my_pi = 3.14\n\n3.14\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can still assign a new value with the same type as the original one to a constant, but a warning is printed.\n\n\n\nThe ans variable: in interactive mode, Julia REPL assigns the value of the last expression to the ans (answer) variable.\n\n\nLiteral coefficient\n\nIn mathematics, 3×x + 2×y may be written as 3x + 2y. Julia lets you write a multiplication in the same manner. We refer to this as literal coefficient, which is a shorthand for multiplication between a number literal and a constant or variable:\n\nx = 3\n2x\n\n2*(3+2)\n2(3+2)\n\n2*π\n2π\n\n\n\n2.3 Relation and logical operations\n\n2.3.1 Relation operations\n==, != or ≠, &lt;, &gt;, &lt;= or ≤, &gt;= or ≥\nThe operation result is true or false, which is Bool type.\n\n\n2.3.2 Logical operations\n&&, ||, !\nThe logical evaluation is lazy.\nSuppose i = 10, and then 1 &lt;= i &lt;= 100 is equivalent to i &gt;= 1 && i &lt;= 100.\n\n\n\n2.4 Control flow\n\n2.4.1 Comment\nIn Julial, you can give an inline comment by using #, or multiline comment by using #=...=#.\n\n\n2.4.2 Compound expressions\nTo have a single expression which evaluates several subexpressions in order, returning the value of the last subexpression as its value.\n\n; chain\n\nPut all subexpressions separated by ; inside parentheses.\n\nz = (x = 1; y = 2; x + y)\n\n# or\nz = (x = 1;\n     y = 2;\n     x + y)\n\nz\n\n3\n\n\n\nbegin block\n\nPut all subexpressions separated by a newline character between begin and end keywords.\nYou can also put all subexpressions in one line by separating them with ;.\n\nz = begin\n    x = 1\n    y = 2\n    x + y\n    end\n\n# or\nz = begin x = 1; y = 2; x + y end\n\nz\n\n3\n\n\nThis is quite useful for the inline function definition.\n\n\n\n\n\n\nNote\n\n\n\nFor multiple statements, you can put them in one line and separate them with ;, which is not the same thing as compound expressions:\n\nx = 1 + 2; println(\"x=$x\")\n\nx=3\n\n\n\n\n\n\n2.4.3 Short-circuit evaluation\n\ncond && expr: evaluate expr if and only if cond is true.\ncond || expr: evaluate expr if and only if cond is false.\n\n\n\n2.4.4 Conditional evaluation\n\nif cond1\n    statements\nelseif cond2\n    statements\n...\nelse\n    statements\nend\n\n\n\n\n\n\n\nNote\n\n\n\nTernary operator: cond ? expr1 : expr2, which is closely equivalent to if cond expr1 else expr2.\n\n\n\n\n2.4.5 Looping\n\nwhile\n\n\nwhile cond\n    statements\nend\n\n\n‘for’\n\n\nfor var in iterable\n    statements\nend\n\nFor for loop, var in iterable, var ∈ iterable, and var = iterable are equivalent to one another!\n\n\n\n\n\n\nNote 1: Member operator in or ∈\n\n\n\n\nin(collection) or ∈(collection) creates a function which checks whether its argument is in collection:\n\n\nf = in(1:10)\n\nf(1)\n\ntrue\n\n\nNote: start:stop will generate a number sequence with step 1; start:step:stop with step step.\n\nin(item, collection) or ∈(item, collection) determines whether an item is in the given collection:\n\n\nin(1, 1:10)\n\ntrue\n\n\n\nSets check whether the item is equal to one of the elements:\n\n\n1 in Set(1:10)\n\ntrue\n\n\n\nDicts look for key=&gt;value pairs:\n\n\n(1=&gt;10) in Dict(1=&gt;10, 2=&gt;20)\n\ntrue\n\n\n\nin.(items, collection) or items .∈ collection checks whether each value in items and each value in collection at the corresponding position are the same one:\n\nIf either items or collection contains only one element, it will be broadcasted to the same length as the longer.\n\nin.([1, 3, 2], [1, 4, 2])\n\n3-element BitVector:\n 1\n 0\n 1\n\n\n\nin.(items, Ref(collection)) or items .∈ Ref(collection) checks whether each value in items is in collection:\n\nRef(collection) can also be written as (collection,) (i.e. wrap collection in a tuple or a Ref).\nNote: create a tuple containing only one element with (1,).\n\nin.([1, 3, 2], Ref([8, 6, 1, 4, 3, 2]))\n\n3-element BitVector:\n 1\n 1\n 1\n\n\nin. does not support infix form!\nin, ∈, and .∈ support both forms!\nIn contrary to ∈ (\\in&lt;tab&gt;), ∋ (\\ni&lt;tab&gt;), and .∈, we have, ∉ (\\notin&lt;tab&gt;), ∌ (\\nni&lt;tab&gt;), and .∉.\n\n\n\n\n2.4.6 Jump out of loops\n\nbreak: jump out of the loop in which break is.\ncontinue: stop an iteration and move on to the next one.\n@goto name and @label name: @goto name unconditionally jumps to the statement at the location @label name.\n\n\n\n\n2.5 Functions\n\n2.5.1 Inline functions\n&lt;function name&gt;(&lt;parameters&gt;) = &lt;expression&gt;:\n\ncylinder_volume(r, h) = π*r^2*h\n\ncylinder_volume(5, 3)\n\n235.61944901923448\n\n\n\n\n2.5.2 Multiline functions\n\nfunction &lt;function name&gt;(parameters)\n    ...\nend\n\nIn Julia, return &lt;value&gt; is not necessary. It is only used when you need to exit a function early; otherwise the value of the last expression will always be returned.\n\n\n\n\n\n\nNote\n\n\n\nFunctions are central to Julia! Various interfaces are achieved by functions even though they don’t look like functions.\n\nInfix form\n\n\n5 + 3\n\n8\n\n\n\nPrefix form\n\n\n+(5 + 3 + 5)\n\n13\n\n\nIf a function with a symbol name takes two arguments, we can use it by infix form:\n\n↔(x, y) = x^2 + y^2\n\n6 ↔ 6\n\n72\n\n\n\n\n\n\n2.5.3 Argument passing behaviour\nPass-by-sharing!\n\n\n2.5.4 Specify the type of return value\nYou can specify the type of return value of a function in the form FuncName(parameters)::ReturnType.\nIf the type of return value is not the given type, a conversion is attempted with convert().\n\nfoo(x::Int64) :: Int32 = 2x\n\ntypeof(foo(6))\n\nInt32\n\n\n\n\n2.5.5 Multiple assignments and multiple return values\n\nMultiple assignments\n\nAchieved by using (named) tuples.\n\n(a, b, c) = 1:3  # Assign each variable a value; parentheses are optional\n\n_, _, a = 1:3  # Use _ to discard unwanted values\n\na, b..., c = 1:6  # a -&gt; 1, b -&gt; 2:5, c -&gt; 6; b... indicates that b is a collection (b doesn't need to be the final one)\n\n(; b, a) = (a=1, b=2, c=3)  # Assign values to variables based on names\n\n\nMultiple return values\n\n\n\n2.5.6 Parameter types\n\nPositional parameters: non-optional; optional with defaults.\nKeyword parameters: non-optional; optional with defaults.\n\n\n(a, b = 1; c, d = 2)  # Keyword arguments are defined after ;\n\n# Positional arguments: a, b (optional)\n# Keyword arguments: c, d (optional)\n\n# When you pass arguments, either will be fine:\n(1, 2; c = 3, d = 4)  # Separated by ;\n(1, 2, c = 3, d = 4)  # Separated by ,\n\n\n\n\n\n\n\nImportant\n\n\n\nMultiple dispatch only considers positional arguments.\n\n\n\n\n2.5.7 Anonymous functions\nAnonymous functions play an important role in functional programming.\nAn anonymous function can be defined in two ways:\n\nInline style: (&lt;parameters&gt;) -&gt; &lt;expression&gt; (() can be omitted if it only has a single parameter).\nMultiline style:\n\n\nfunction (&lt;parameters&gt;)\n    ...\nend\n\n\n2.5.7.1 do blocks\nWe can use do blocks to create mutiline anonymous functions.\nThe following two statements are equivalent:\n\nmap(x -&gt; begin\n              if x &lt; 0 && iseven(x)\n                  return 0\n              elseif x == 0\n                  return 1\n              else\n                  return x\n              end\n         end,\n    [-2, 0, 2])\n\n3-element Vector{Int64}:\n 0\n 1\n 2\n\n\n\nmap([-2, 0, 2]) do x\n    if x &lt; 0 && iseven(x)\n        return 0\n    elseif x == 0\n        return 1\n    else\n        return x\n    end\nend\n\n3-element Vector{Int64}:\n 0\n 1\n 2\n\n\nIn the above example, the do x syntax creates an anonymous function with argument x and passes it as the first argument to map().\nSimilarly, do x, y will create a two-argument anonymous function but do (x, y) will create a one-argument anonymous function, whose argument is a tuple.\nIn a word, we can use do blocks to create anonymous functions which are passed as the first argument to some higher-order functions, the first argument of which must be the Function type.\n\n\n\n2.5.8 The splat operator ...\nThe splat operator can be used to turn arrays or tuples into function arguments.\ne.g. foo([1, 2, 3]...) is the same as foo(1, 2, 3).\nYou can define a parameter which accepts a variable number of arguments by using the splat operator:\n\n# All arguments except the 1st will be stored in a tuple, assigned to args\nfunction var_f(x, args...)\n    ...\nend\n\n\n\n2.5.9 Closure\nA closure is a function that has captured some external state not supplied as an argument since the inner scope can use variables defined in an outter scope.\nAnonymous functions are frequently used as closures.\n\nfunction make_pow(n::Real)  # Outer function\n    function (x::Real)  # Inner function\n        x^n  # The inner function uses n defined outside it and n is not passed as an argument to it\n    end\nend\n\npow2 = make_pow(2)  # The returned function with n=2 is assigned to variable pow2\npow3 = make_pow(3)\n\npow2(2), pow3(2)\n\n(4, 8)\n\n\n\n\n\n\n\n\nPerformance of captured variable\n\n\n\nFor the consideration of performance, if the type of a captured variable is already known, you would better add a type annotation to it. In addition, if the value of this captured variable need not be changed after the closure is created, you can indicate it with a let block:\n\nfunction abmult(r::Int)\n    r1::Int = r  # Type annotation\n    if r1 &lt; 0\n        r1 = -r1\n    end\n    f = let r1 = r1  # Fix it\n            x -&gt; x * r1\n        end\n    return f\nend\n\nf = abmult(10)\nf(10)\n\n100\n\n\n\n\n\n\n2.5.10 Partial function application\nPartial function application refers to the process of fixing a number of arguments to a function, producing another function accepting fewer arguments.\nObviously, closure is a way to achieve the partial function application.\n\n\n2.5.11 Function composition, vectorization and piping\n\n2.5.11.1 Function composition\nThe concept of function composition in Julia is the very concept of function composition in mathematics and the operation symbol is the same one: ∘, typed using \\circ&lt;tab&gt; (e.g. (f ∘ g)(args...) is the same as f(g(args...))).\n\n(sqrt ∘ +)(3, 6)  # Equivalent to sqrt(+(3, 6))\n\n3.0\n\n\n\n\n2.5.11.2 Dot syntax for vectorizing functions\nIn Julia, vectorized functions are not required for performance, and indeed it is often beneficial to write your own loops, but they can still be convenient.\nYou can add a dot . after regular function names (e.g. f) or before special operators (e.g. +) to get their vectorized versions.\n\nOperating on a single array:\n\n\nA = 1:3\n\nsin.(A)  # Which is equivalent to map(sin, A) or broadcast(sin, A)\n\n3-element Vector{Float64}:\n 0.8414709848078965\n 0.9092974268256817\n 0.1411200080598672\n\n\n\nOperating on multiple arrays (even of different shapes), or a mix of arrays and scalars:\n\n\nfp(x, y) = 3x + 4y\n\nA = 1:3\nB = 4:6\n\nfp.(pi, A)\n\n3-element Vector{Float64}:\n 13.42477796076938\n 17.42477796076938\n 21.42477796076938\n\n\n\nfp.(A, B)\n\n3-element Vector{Int64}:\n 19\n 26\n 33\n\n\n\nKeyword arguments are not broadcasted over, but are simply passed through to each of the function.\nNested f.(args...) calls are fused into a single broadcast loop.\n\n\nX = 1:6\n\nsin.(cos.(X))  # Equivalent to broadcast(x -&gt; sin(cos(x)), X)\n\n6-element Vector{Float64}:\n  0.5143952585235492\n -0.4042391538522658\n -0.8360218615377305\n -0.6080830096407656\n  0.2798733507685274\n  0.819289219220601\n\n\nHowever, the fusion stops as soon as a “non-dot” function call is encountered (e.g. sin.(sort(cos.(X)))).\n\nThe maximum efficiency is typically achieved when the output array of a vectorized operation is pre-alllocated.\n\n\nX = 1:10000\n\n@time sin.(X)\n\n  0.000052 seconds (3 allocations: 78.203 KiB)\n\n\n10000-element Vector{Float64}:\n  0.8414709848078965\n  0.9092974268256817\n  0.1411200080598672\n -0.7568024953079282\n -0.9589242746631385\n -0.27941549819892586\n  0.6569865987187891\n  0.9893582466233818\n  0.4121184852417566\n -0.5440211108893698\n -0.9999902065507035\n -0.5365729180004349\n  0.4201670368266409\n  ⋮\n -0.9534986003597155\n -0.26156028858731495\n  0.6708553462651908\n  0.9864896695694187\n  0.39514994010172155\n -0.5594888219681838\n -0.9997361413354392\n -0.5208306628783247\n  0.4369241250954582\n  0.9929728874353159\n  0.6360869563962336\n -0.30561438888825215\n\n\n\nY = Vector{Float64}(undef, 10000)  # Construct an uninitialized (undef) Vector{Float64} of length 10000\n\n@time Y .= sin.(X)  # Overwrite Y with sin.(X) in-place\n\n  0.007350 seconds (9.57 k allocations: 711.680 KiB, 99.11% compilation time)\n\n\n10000-element Vector{Float64}:\n  0.8414709848078965\n  0.9092974268256817\n  0.1411200080598672\n -0.7568024953079282\n -0.9589242746631385\n -0.27941549819892586\n  0.6569865987187891\n  0.9893582466233818\n  0.4121184852417566\n -0.5440211108893698\n -0.9999902065507035\n -0.5365729180004349\n  0.4201670368266409\n  ⋮\n -0.9534986003597155\n -0.26156028858731495\n  0.6708553462651908\n  0.9864896695694187\n  0.39514994010172155\n -0.5594888219681838\n -0.9997361413354392\n -0.5208306628783247\n  0.4369241250954582\n  0.9929728874353159\n  0.6360869563962336\n -0.30561438888825215\n\n\n\nUsing the @. macro to convert every function call, operation, and assignment in an expression into the “dotted” version.\n\n\nX = [1.0, 2.0, 3.0]\n\nY = similar(X)  # Pre-allocate the output array\n\n@. Y = sin(cos(X))\n\n3-element Vector{Float64}:\n  0.5143952585235492\n -0.4042391538522658\n -0.8360218615377305\n\n\n\nUsing vectorized piping operator.\n\n\n1:6 .|&gt; [x -&gt; x-1, inv, x -&gt; 2*x, -, isodd, iseven]\n\n6-element Vector{Real}:\n    0\n    0.5\n    6\n   -4\n true\n true\n\n\n\n\n\n\n\n\nbroadcast(f, As...)\n\n\n\nBroadcast the function f over the arrays, tuples, collections, Refs, and/or scalars As.\n\n\n\n\n\n\n\n\nPre-allocating outputs\n\n\n\n\nVector{Int}(undef, 10)  # Construct an uninitialized Vector{Int} of length 10\n\n10-element Vector{Int64}:\n               0\n               0\n               0\n               0\n               0\n 140265967167120\n 140262402246488\n              32\n          563204\n              32\n\n\n\nMatrix{Float64}(undef, 3, 3)  # Construct an uninitialized Matrix{Float64} of 3 by 3\n\n3×3 Matrix{Float64}:\n 2.3e-322  5.0e-324     5.0e-324\n 7.4e-323  2.7826e-318  6.92988e-310\n 7.4e-323  5.0e-324     0.0\n\n\n\n\n\n\n2.5.11.3 Function piping\nThe pipe operator is |&gt;, which is used to chain together functions taking single arguments as inputs.\n\n1:10 |&gt; sum |&gt; sqrt\n\n7.416198487095663\n\n\n\n\n\n\n2.6 Exception\nUsage:\n\ntry\n    &lt;some code which may raise some errors&gt;\ncatch &lt;exception variable&gt;\n    &lt;some code dealing with exceptions&gt;\nelse\n    &lt;some code to be executed when no error occurs&gt;\nfinally\n    &lt;some code to be executed anyway&gt;\nend\n\nYou can use throw() to raise a given type of exception or use error() to raise an ErrorException directly.\nThen you can use isa() to check whether the error type raised is the expected.\ne.g.\n\nx = [2, -2, 'a']\n\nfor i in x\n    try\n        y = sqrt(i)\n        println(\"√\", i, \" = \", y)\n    catch e\n        if isa(e, DomainError)\n            println(\"√\", i, \": $(i) is out of domain\")\n        else\n            println(\"√\", i, \": $(i) is an unsopported type\")\n        end\n    end\nend\n\n√2 = 1.4142135623730951\n√-2: -2 is out of domain\n√a: a is an unsopported type\n\n\n\n\n2.7 Metaprogramming\n\n\n\n\n\n\nKey concepts\n\n\n\n\nAbstract Syntax Tree (AST): a data structure used in computer science to represent the structure of a program or code snippet.\nHigher-order functions: functions taking one or more functions as arguments and returning a function. All other functions are called first-order functions.\nClosure: a function that has captured some external state not supplied as arguments to it since the inner scope can refer to variables defined in its outer scopes.\nReflection: the ability of a process to examine, introspect, and modify its own structure and behavior.\n\n\n\n\n2.7.1 Program representation\nIn a word, each Julia program starts its life as a string, which then is parsed into an object called expression of type Expr. The key point is that Julia code is internally represented as a data structure that is accessible from the language itself. It means that we can generate, examine, and modify Julia code like manipulating ordinary Julia objects within Julia.\n\n\n2.7.2 Expressions and evaluation\nThe next questions are how to construct expressions of type Expr, and how to execute (evaluate) them?\n\n2.7.2.1 Expressions\nThere are several ways to construct expressions:\n\nFrom strings via Meta.parse().\n\n\nprog = \"1 + 1\"\nex1 = Meta.parse(prog)\n\n:(1 + 1)\n\n\n\ntypeof(ex1)\n\nExpr\n\n\n\n\n\n\n\n\nNote\n\n\n\nExpr objects contain two fields:\n\nhead: a Symbol identifying the kind of expression.\nargs: the expression arguments, which may be symbols, expressions, or literal values.\n\n\n\n\nUse Expr() constructor.\n\n\nex2 = Expr(:call, :+, 1, 1)\n\n:(1 + 1)\n\n\n\nex1 == ex2\n\ntrue\n\n\n\nQuoting single/multiple statements of Julia code.\n\nThe usual representation of a quote form in an AST is an Expr with head :quote.\n\nQuoting single statement of Julia code using : character, followed by paired parentheses:\n\n\n:(a + b * c + 1) |&gt; typeof\n\nExpr\n\n\n\nQuoting multiple statements of Julia code using quote ... end blocks:\n\n\nex = quote\n    x = 1\n    y = 2\n    x + y\nend\ntypeof(ex)\n\nExpr\n\n\n\n\n\n\n\n\nInterpolation\n\n\n\nIn contrast with expressions constructed using Meta.parse() or Expr(), expressions constructed by quoting single/multiple statements of Julia code allow us to interpolate literals or expressions into, quite similar with string interpolation:\n\na = 1\n:($a + b)  # literals\n\n:(1 + b)\n\n\n\n:(a in $:((1,2,3)))  # expressions\n\n:(a in (1, 2, 3))\n\n\nSplatting interpolation: you have an array of expressions and need them all to become arguments of the surrounding expression. This can be done with the syntax $(xs...):\n\nargs = [:x, :y, :z]\n:(f(1, $(args...)))\n\n:(f(1, x, y, z))\n\n\n\n\n\nNested quote and interpolation:\n\nNaturally, it is possible for quote expressions to contain other quote expressions.\nUnderstanding how interpolation works in these cases can be a bit tricky.\nThe basic principle is that $x works similarly to eval(:x).\n\njulia&gt; x = 100\n# 100\n\njulia&gt; quote $x end  # x will be evaluated in a non-nested quote (this should be natrual for interpolation introduced above)\n# quote\n#     #= REPL[13]:1 =#\n#     100\n# end\n\njulia&gt; quote quote $x end end  # x won't be evaluated yet, because it belongs to the inner quote, not the outer quote\n# quote\n#     #= REPL[14]:1 =#\n#     $(Expr(:quote, quote\n#     #= REPL[14]:1 =#\n#     $(Expr(:$, :x))\n# end))\n# end\n\njulia&gt; quote quote $x end end |&gt; eval  # the inner quote will be evaluated and x will too as a consequence\n# quote\n#     #= REPL[15]:1 =#\n#     100\n# end\n\njulia&gt; quote quote $$x end end  # the outer quote can interpolate values inside $ in the inner quote with multiple $s, which means x will be evaluated in this case\n# quote\n#     #= REPL[16]:1 =#\n#     $(Expr(:quote, quote\n#     #= REPL[16]:1 =#\n#     $(Expr(:$, 100))\n# end))\n# end\n\njulia&gt; quote quote quote $$x end end end  # x won't be evaluated here, because the outer $ belongs to the innermost quote, and the inner $ belongs to the second quote\n# quote\n#     #= REPL[17]:1 =#\n#     $(Expr(:quote, quote\n#     #= REPL[17]:1 =#\n#     $(Expr(:quote, quote\n#     #= REPL[17]:1 =#\n#     $(Expr(:$, :($(Expr(:$, :x)))))\n# end))\n# end))\n# end\n\n\nQuoteNode:\n\nIn some situations, it is necessary to quote code without performing interpolation. This kind of quoting does not yet have syntax, but is represented internally as an object of type QuoteNode:\n\njulia&gt; quote quote $x end end |&gt; eval  # with interpolation\n# quote\n#     #= REPL[34]:1 =#\n#     100\n# end\n\njulia&gt; quote quote $x end end |&gt; QuoteNode |&gt; eval  # wihout interpolation\n# quote\n#     #= REPL[36]:1 =#\n#     $(Expr(:quote, quote\n#     #= REPL[36]:1 =#\n#     $(Expr(:$, :x))\n# end))\n# end\n\nNote: the parser yields QuoteNodes for simple quoted items like symbols:\n\ndump(Meta.parse(\":x\"))\n\nQuoteNode\n  value: Symbol x\n\n\n\n\n\n\n\n\nShow expressions elegantly\n\n\n\n\ndump(Meta.parse(\"1 + 1\"))\n\nMeta.show_sexpr(Meta.parse(\"(4 + 4) / 2\"))  # shows that Expr objects can be nested\n\nExpr\n  head: Symbol call\n  args: Array{Any}((3,))\n    1: Symbol +\n    2: Int64 1\n    3: Int64 1\n(:call, :/, (:call, :+, 4, 4), 2)\n\n\n\n\n\n\n\n\n\n\nSymbols\n\n\n\nA Symbol is an interned string, used as one building block of expressions.\nA Symbol can be constructed in two ways:\n\n# using : character from valid identifiers\ns = :foo\ntypeof(s)\n\nSymbol\n\n\n\n# using Symbol() constructor from any number of arguments by concatenating their string representations together\nSymbol(:var, \"_\", \"sym\")\n\n:var_sym\n\n\n\n# sometimes extra parentheses around the argument to : are needed to avoid ambiguity in parsing\n:(::)\n\n:(::)\n\n\nNote: in the context of an expression, symbols are used to indicate access to variables; when an expression is evaluated, a symbol is replaced with the value bound to that symbol in the appropriate scope.\n\n\n\n\n2.7.2.2 Evaluation\nGiven an expression object, one can cause Julia to evaluate (execute) it at global scope using eval() (for code block, use @eval begin ... end).\nEvery module has its own eval() function that evaluates expressions in its global scope.\nNote the behaviors of variable a and symbol :b in the following code:\n\na = 1\nex = Expr(:call, :+, a, :b)  # The value of the variable a at expression construction time is uesd as an immediate value in the expression; on the other hand, the symbol :b is used in the expression construction, so the value of the variable b at that time is irrelevant. Only when the expression is evaluated is the symbol :b resolved by looking up the value of the variable b.\na, b = 0, 2\neval(ex)\n\n3\n\n\n\n\n\n2.7.3 Code generation\nBy means of expressions along with its interpolation, and evaluation, one extremely useful feature of Julia is the capability to generate and manipulate Julia code within Julia itself. Such as defining functions returning Expr objects, defining methods programmatically, etc.\n\nstruct MyNumber\n    x::Float64\nend\n\nfor op = (:sin, :cos, :tan, :log, :exp)\n    eval(quote\n        Base.$op(a::MyNumber) = MyNumber($op(a.x))\n    end)\nend\n\nx = MyNumber(π)\nprintln(sin(x))\nprintln(cos(x))\n\nMyNumber(1.2246467991473532e-16)\nMyNumber(-1.0)\n\n\n\n\n2.7.4 Macros\nMacros provide a mechanism to include generated code in the final body of a program.\nA macro maps a tuple of arguments (including symbols, literal values, and expressions, which hints that all the other arguments passed to a macro are considered as expressions, except symbols and literal values) to a returned expression, which is compiled directly rather than requiring a runtime eval() call. This means that the returned expression is compiled at parse time. This is why we can include generated code in the final body of a program using macros.\n\nDefining macros:\n\n\nmacro &lt;NAME&gt;(&lt;arguments&gt;)\n    body  # return an expression at last\nend\n\nFor example,\n\nmacro sayhello(name)\n    return :(println(\"Hello, \", $name))\nend\n\n@sayhello(\"human\")\n\nHello, human\n\n\nWhen @sayhello is encountered, the quoted expression is expanded to interpolate the value of the argument into the final expression. Then, the compiler will replace all instances of @sayhello with :(Main.println(\"Hello, \", \"human\")). When @sayhello is entered in the REPL, the expression executes immediately, thus we only see the evaluation result. We can view the returned expression using the function macroexpand() or macro @macroexpand:\n\n@macroexpand @sayhello(\"human\")  # equivalent to macroexpand(Main, :(@sayhello(\"human\")))\n\n:(Main.println(\"Hello, \", \"human\"))\n\n\n\nWhy macros?\n\nMacros are necessary because they execute when code is parsed; therefore, macros allow the programmer to generate and include fragments of customized code before the full program is run.\n\nmacro twostep(arg)\n    println(\"I execute at parse time. The argument is: \", arg)\n    return :(println(\"I execute at runtime. The argument is: \", $arg))\nend\n\nex = @macroexpand @twostep :(1, 2, 3)\nprintln(typeof(ex))\nprintln(repr(ex))  # equivalent to show(ex), because repr() actually calls show() and then returns a string\neval(ex)\n\nI execute at parse time. The argument is: :((1, 2, 3))\nExpr\n:(Main.println(\"I execute at runtime. The argument is: \", $(Expr(:copyast, :($(QuoteNode(:((1, 2, 3)))))))))\nI execute at runtime. The argument is: (1, 2, 3)\n\n\n\nMacro invocation\n\n\n# separated by white space\n@name expr1 expr2 ...\n# separated by ,\n@name(expr1, expr2, ...)\n\nNote:\n\n# there is only an argument here - a tuple\n@name (expr1, expr2, ...)\n\n@name[a b] * c  # no space and parenthesis between the macro name and the argument, which is the unique argument to this macro\n# is equivalent to\n@name([a b]) * c\n\nNote: again, macros receive their arguments as expressions, literals, and symbols. You can explore the macro arguments using the show() function within the macro body.\nNote: in addition to the given argument list, every macro is passed extra two arguments named __source__, and __module__.\n\n__source__ argument provides information if the form of a LineNumberNode object about the parser location of the @ sign from the macro invocation. The location information can be accessed by referencing __source__.line, and __source__.file. It can also be used for other useful purposes, such as implementing the @__LINE__, @__FILE__, and @__DIR__ macros.\n__module__ argument provides information in the form of a Module object about the expansion context of the macro invocation. This allows macros to look up contextual information, such as existing bindings.\n\n\nHygiene\n\nHow to resolve variables within a macro result in an appropriate scope?\nIn short, we have several concerns:\n\nMacros must ensure that the variables they introduce in their returned expressions do not accidentally clash with existing variables in the surrounding code they expand into.\nConversely, the expressions that are passed into a macro as arguments are often expected to evaluate in the context of the surrounding code, interacting with and modifying the existing variables.\nIn addition, a macro may be called in a different module from where it was defined. In this case we need to ensure that all global variables are resolved in the correct module.\n\nJulia’s macro expander solves these problems in the following way:\n\nFirst, variables within a macro result are classified as either local or global. A variable is considered local if and only if it is assigned to declared local, or used as a function argument name. Otherwise, it is considered global. Local variables are then renamed to be unique via gensym() function, and global variables are resolved within the macro definition environment.\n\nThe above rules can meet the following expectations:\n\n# here, we want t0, t1, and val to be private temporary variables,\n# and we want time_ns() and println() refer to the time_ns() and println() functions in Julia Base,\n# not to any time_ns() and println() functions the user might have\nmacro time(ex)\n    return quote\n        local t0 = time_ns()\n        local val = $ex\n        local t1 = time_ns()\n        println(\"elapsed time: \", (t1-t0)/1e9, \" seconds\")\n        val\n    end\nend\n\n\nBut sometimes, we want some variables in the user expression to be resolved in the macro call environment. To achieve this goal, we can put the user expression in the esc() function, which means “escaping”. An expression wrapped in this manner is left alone by the macro expander and simply pasted into the output verbatim. Therefore it will be resolved in the macro call environment.\n\nThe above rules can meet the following expectations:\n\n# suppose that the user has already defined a time_ns() function, different from the time_ns() function in the Julia Base,\n# and he call @time in this way:\n\n@time time_ns()\n\n# obviously, we just want time_ns() contained in the user expression to be resolved in the macro call environment, instead of the macro definition environment.\n# so this is why we need esc().\n\n\nMacro dispatch\n\nMacro dispatch is based on the types of AST that are handed to the macro, not the types that AST evaluates to at runtime.\nFor example:\n\nExpr: contains many different heads.\nSymbol\nLiteral values: Int64, Float64, String, Char, etc.\nQuoteNode\nLineNumberNode\n\nand so on.\n\n\n2.7.5 Non-standard string and command literals\n\nStandard string literals\n\nFor example, \"abc\", \"\"\"abc\"\"\".\n\nNon-standard string literals\n\nTo provide some convenient methods to generate some special objects using non-standard string literals.\n\nmacro r_str(pattern, flags...)\n    Regex(pattern, flags...)\nend\n\np = r\"^http\"  # equivalent to call @r_str \"^http\" to produce a regular expression object rather than a string\n\n# how to define a non-standard string literal\nmacro &lt;name&gt;_str(str)  # affixing _str after the formal macro name\n    ...\nend\n\n# add a flag\nmacro &lt;name&gt;_str(str, flag)  # flag is also a String type\n    ...  # the return value may depend on the flag content (different flags with different return values)\nend\n\n# how to call\nname\"str\"flag\n\n\nStandard command literals\n\nFor example, `echo hello, world`.\n\n# generate a Cmd from the str string which represents the shell command(s) to be executed\nmacro cmd(str)\n    cmd_ex = shell_parse(str, special=shell_special, filename=String(__source__.file))[1]\n    return :(cmd_gen($(esc(cmd_ex))))\nend\n\n# if you want to call shell_parse() and cmd_gen(), you need do it in the forms of Base.shell_parse() and Base.cmd_gen(), respectively\n\n\nNon-standard command literals\n\n\nmacro echo_cmd(str)\n    cmd_str = string(\"echo \", str)\n    return :(@cmd $cmd_str)\nend\n\nc = echo`hello, world`\ntypeof(c)\nshow(c)\n\n`echo hello, world`\n\n\n\n\n2.7.6 Generated functions\nHow to generate specialized code depending on only the types of their arguments using generated functions (argument names refer to types, and the code should return an expression)?\nThe capability of multiple dispatch can also be achieved by using generated functions, which is defined by prefixing @generated before a normal function definition, but we’d better obey some rules when defining generated functions.\nOf course, we can define an optionally-generated function containing a generated version and a normal version by using if @generated ... else ... in a normal function body. Statements after if @generated is the generated one and after else the normal one. The compiler may use the generated one if convenient; otherwise it may choose to use the normal implementation instead.\n\n\n\n2.8 Types\n\n2.8.1 Basics\nIn Julia, all are objects having a type, and types are first-class objects.\n\nYou can use typeof() to get the type of any object.\nYou can find the supertype of any type with supertype(): the root of type hierarchy is Any.\nYou can find the subtypes of any type with subtypes(): if there is no subtype for a given type, it will return Type[].\nYou can check whether a type is a subtype of the other with the &lt;: operator (e.g. String &lt;: Any).\nSeeing that you created an empty array with the type Integer, then you can only add elements with the type Integer or its subtypes to this array.\n\n\n\n\n\n\n\nPrimitive and composite types\n\n\n\nWe can roughly divide all types into primitive types (concrete types whose data consists of plain old bits) and composite types (derived from primitive types or other composite types). On the other hand, we can also devide all types into abstract types (with zero fields) and concrete types (with fields).\nIn Julia, there are three primitive types: integers, floating-point numbers and characters. You can use the function isprimitivetype() to check whether a type is a primitive type (e.g. isprimitivetype(Int8)).\nIt’s possible to define new primitive types in Julia by using primitive type ... end.\n\n\nYou can create composite types from primitive types or composite types:\n\nDefinition of an immutable composite type:\n\n\nstruct TypeName\n    # Defining typed fields here\nend\n\ne.g.\n\nstruct Archer\n    name::String\n    health::Int\n    arrows::Int\nend\n\n# Once the composite type Archer is defined, you can instantiate the Archer object\nwilliam = Archer(\"William Tell\", 30, 24)\n\n# Then access the values of fileds by using dot operator\nwilliam.name, william.health, william.arrows\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, :: is used to annotate variables and expression with types.\nx::T means variable x should have type T.\n\n\n\nDefinition of a mutable composite type:\n\n\nmutable struct TypeName\n    # Defining typed fields here\nend\n\n\nDefinition of abstract type: abstract type TypeName end.\n\nObviously, the type created by using struct is a concrete type.\nYou can create objects of a concrete type but not of an abstract type.\nAn abstract type cannot have any fields. Only concrete types can have fields or a value.\nThe purpose of abstract types is to facilitate the construction of type hierarchy.\nA composite type is a concrete type with fields; a primitive type is a concrete type with a single value.\n\nYou can use the subtype operator &lt;: to create a concrete or abstract subtype of an abstract type.\n\n\nabstract type Warrior end\n\n# Archer is a subtype of Warrior\nstruct ArcherSoldier &lt;: Warrior\n    name::String\n    health::Int\n    arrows::Int\nend\n\nsupertype(ArcherSoldier)\n\nWarrior\n\n\n\n\n\n\n\n\nNote\n\n\n\nDifferent with object-oriented languages, composite types in Julia can only have fields, and cannot have methods bound to them.\n\n\nAfter creating concrete types, you can make objects of them (i.e. instantiate them) with arguments.\n\n\n\n\n\n\nNote\n\n\n\nYou can only make objects of concrete types!\n\n\ne.g.\n\nmutable struct TestType\n    a::Int64\n    b::Float64\nend\n\nt1 = TestType(1, 10.5)\n\nTestType(1, 10.5)\n\n\nYou can instantiate objects of TestType in this way t1 = TestType(1, 10.5), because Julia automatically creates a special function called constructor with the same name as your type. A constructor is responsible for making an instance (object) of the type it is associated with. Julia adds two methods to the constructor function, which takes the same number of arguments as you have fields. One method uses type annotations for its arguments, as specified for each field in the struct. The other takes arguments of Any type.\n\nmethods(TestType)\n\n# 2 methods for type constructor: TestType(a::Int64, b::Float64) in Main at In[62]:2  TestType(a, b) in Main at In[62]:2 \n\n\n\nfunction TestType(a::Int64)\n    TestType(a, a)\nend\n\nmethods(TestType)\n\n# 3 methods for type constructor: TestType(a::Int64) in Main at In[64]:1  TestType(a::Int64, b::Float64) in Main at In[62]:2  TestType(a, b) in Main at In[62]:2 \n\n\n\nTestType(100)\n\nTestType(100, 100.0)\n\n\nSurely, you can add methods to this constructor function outside of struct in the same manner as any other fucntion, called outer constructor.\nIn addition, you can define accessors (getters and setters) as well as other functions accepted arguments of this type to achieve some tasks.\nYou can only provide types without concrete parameters to define a function tied to types (this type of function are usually used to get some properties of a type, independent of its objects):\n\ntoy(::TestType) = 100\n\nt = TestType(100)\ntoy(t)\n\n100\n\n\nIn functions (including outer constructors) you defined outside of struct, you can easily check whether user-provided arguments are valid or not. But how can we check this when instantiating objects of a concrete type by using constructors Julia created?\nTo solve this problem, we need to define the constructor inside of struct, called inner constructor. Once you do this, you tell Julia that you don’t want it to create constructor methods automatically (i.e. disable this manner). Then, users can only use the constructor you defined to instantiate objects of a concrete type.\n\nmutable struct TempType\n    a::Int64\n    b::Float64\n    diff::Float64\n\n    function TempType(a::Int64, b::Float64)\n        new(a, b, b - a)  # We don't want users to provide the value of diff, which is defined as the difference of b and a\n    end\nend\n\n\n\n\n\n\n\nNote\n\n\n\nIn inner constructor, you need use new() (which is only available inside an inner constructor) to instantiate objects of a concrete type, which accepts zero or more arguments but never more aguments than the number of fields in your composite type, because creating an inner constructor removes all constructor methods created by Julia. Feilds with missing values will be set to random values.\n\n\n\nmethods(TempType)\n\n# 1 method for type constructor: TempType(a::Int64, b::Float64) in Main at In[67]:6 \n\n\n\nTempType(1, 10.5)\n\nTempType(1, 10.5, 9.5)\n\n\n\n# This will raise an error\nTempType(1, 10.5, 9.5)\n\n\nMethodError: no method matching TempType(::Int64, ::Float64, ::Float64)\n\nClosest candidates are:\n  TempType(::Int64, ::Float64)\n   @ Main In[67]:6\n\n\nStacktrace:\n [1] top-level scope\n   @ In[70]:3\n\n\n\n\n\n2.8.2 Multiple dispatch\n\n2.8.2.1 How does multiple dispatch work\n\nfunction myadd(x::Int, y::Int)\n    print(\"The sum is: \")\n    printstyled(x + y, \"\\n\", bold = true, color = :red)\nend\n\nfunction myadd(x::String, y::String)\n    print(\"The concatenated string is: \")\n    printstyled(join([x, y]), \"\\n\", bold = true, color = :red)\nend\n\nfunction myadd(x::Char, y::Char)\n    print(\"The character is: \")\n    printstyled(Char(Int(x) + Int(y)), \"\\n\", bold = true, color = :red)\nend\n\nmyadd(1, 1)\nmyadd(\"abc\", \"def\")\nmyadd('W', 'Y')\n\nThe sum is: 2\nThe concatenated string is: abcdef\nThe character is: °\n\n\nHow does Julia know which function should be called in this situation?\n\nIn fact, we defined three methods, attached to the function myadd, instead of three functions above.\nIn Julia, functions are just names. Without attached methods, they cannot do anything. Code is always stored inside methods. The type of arguments determines which method will get executed at runtime.\nYou can use methods() to check how many methods a function contains (e.g. methods(myadd)).\nIf some parameters without types specified, the type will be Any (i.e. accept all types of values).\nYou can only define functions without methods:\n\n\nfunction func_no_method end\n\nfunc_no_method(1, 1)  # Attempt to call a function with no methods\n\nLoadError: MethodError: no method matching func_no_method(::Int64, ::Int64)\nMethodError: no method matching func_no_method(::Int64, ::Int64)\n\nStacktrace:\n [1] top-level scope\n   @ In[72]:3\n\n\n\nfunc_not_defined(1, 1)  # Attempt to call a function not defined\n\nLoadError: UndefVarError: `func_not_defined` not defined\nUndefVarError: `func_not_defined` not defined\n\nStacktrace:\n [1] top-level scope\n   @ In[73]:2\n\n\n\n\n2.8.2.2 The way Julia selects the correct method of a function for each situation\nInternally, Julia has a list of functions. Every function enters another list containing the methods, which deals with different argument type combinations.\n\nFirst, Julia matches the function name (i.e. the called function should be defined).\nThen, Julia matches the type combination of arguments and parameters (i.e. the combination of types of arguments passed = the combination of types of parameters defined in a method).\n\nIn contrast with multiple dispatch, what method is used is decided only by the type of the first argument in single dispatch or object-oriented languages (i.e. in a.join(b), the function (method) used is only decided by the object a, not decided by both a and b, because in object-oriented languages, various attributes and fuctions (methods) are bound to objects of a class). If you defined a function multiple times with arguments of different types in object-oriented languages, the previous will be overwritten by the latter.\nIn statically typed languages which allows you to define a function multiple times with arguments of different types, when the code gts compiled, the compiler will pick the right function. But the selection process can only be done during compilation, it cannot be done during execution, which Julia can do.\ni.e. statically typed languages cannot deal with such a situation:\n\nfunction f1(a::Warrior, b::Warrior)\n    f2(a, b)\n    # Some other statements\nend\n\nIn the function f1, defined above, a and b must be subtypes of the Warrior type. Suppose that the function f1 is designed to allow accepting and dealing with these a and b with differnt subtypes of Warrior. When compiling the method f1, it only knows that a and b must be subtypes of Warrior but cannot know what concrete types they have. Then it won’t pick up the right method of f2 (suppose f2 has at least two methods bound to it).\n\n\n\n2.8.3 Conversion and promotion\n\n2.8.3.1 Why do we need type promotion\nInside a microprocessor, mathematical operations are always performed between identical types of numbers.\nThus, when dealing with expressions composed of different number types, all higher-level programming languages have to convert all arguments in the expression to the same number type.\nBut what should this common number type be? Figuring out this common type if what promotion is all about.\nIn most mainstream languages, the mechanisms and rules governing number promotion are hardwired into the language and detaild in the specifications of the language.\nBut Julia promotion rules are defined in the standard library, not in the internals of the Julia JIT compiler. This allows you to extend the existing system, not modifying it.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the @edit macro to explore the Julia source code.\nBy prefixing with the @edit macro, Julia jumps to the definition of the function called to handled the expression (e.g. @edit 1+1).\nBefore using this, you may need to set the environment variable JULIA_EDITOR in your OS.\n\n\n\n\n2.8.3.2 How does type promotion work\nJulia performs type promotion by calling the promote() function, which promotes all arguments to a least common denominator.\ne.g. every arithmetic operation on some Number in Julia first calls promote() before performing the actual arithmetic operation.\ne.g. here, promote() promotes an integer and a floating-point number to floating-point numbers.\n\npromote(1, 2.5)  # It returns a tuple\n\n(1.0, 2.5)\n\n\n\n\n2.8.3.3 How does conversion work\n\n\n\n\n\n\nCaution\n\n\n\nConversion means converting from one type to another related type.\nThis is totally different from parsing a text string to produce a number, because a string and a number are not related types.\n\n\nFor number type conversion, it is recommended to use the constructor of the type you want to convert to.\n\nInt8(32)  # Convert a number of Int64 to Int8\n\n32\n\n\nDifferent from using type constructors, Julia calls the convert() function to achieve this.\n\nconvert(Int8, 32)\n\n32\n\n\nThe first argument of convert() is a type object (we know that all are objects in Julia).\nActually, the type of Int64 is Type{Int64}.\n\nInt64 isa Type{Int64}\n\ntrue\n\n\nYou can regard Type as a function, accepting a type argument T, and then returning the type of T - Type{T}.\n\n\n2.8.3.4 An example extending the type system\nHere we give an example of defining units for angles (redian/degree) and related operations.\n\n2.8.3.4.1 Defining unit types and constructors\n\nabstract type Angle end  # The super type of Radian and Degree\n\nstruct Radian &lt;: Angle\n    radians::Float64\n\n    # Defining customized constructor\n    function Radian(radians::Number=0.0)\n        new(radians)\n    end\nend\n\n# 1 degree = 60 minutes\n# 1 minute = 60 seconds\n# degrees, minutes, seconds (DMS)\nstruct DMS &lt;: Angle\n    seconds::Int\n\n    # Defining customized constructor\n    function DMS(degrees::Integer=0, minutes::Integer=0, seconds::Integer=0)\n        new(degrees * 60 * 60 + minutes * 60 + seconds)\n    end\nend\n\n\n\n2.8.3.4.2 Defining accessors\n\nradians(radian::Radian) = radian.radians\n\nseconds(dms::DMS) = dms.seconds % 60\n\nminutes(dms::DMS) = (dms.seconds ÷ 60) % 60\n\ndegrees(dms::DMS) = (dms.seconds ÷ 60) ÷ 60\n\ndegrees (generic function with 1 method)\n\n\n\n\n2.8.3.4.3 Displaying angles\nThe Julia REPL environment uses the show(io::IO, data) to display data of some specific type to the user.\n\nimport Base: show\n\nfunction show(io::IO, radian::Radian)\n    print(io, radians(radian), \"rad\")\nend\n\nfunction show(io::IO, dms::DMS)\n    print(io, degrees(dms), \"° \", minutes(dms), \"' \", seconds(dms), \"''\")\nend\n\nshow (generic function with 610 methods)\n\n\n\n\n\n\n\n\nCaution\n\n\n\nHere, we only want to attach new methods to the show() function, which is already defined in the Base package.\nSo we need to first import the show() function from the Base package; otherwise, it will automatically create a new function named show, which belongs to the namespace of Main, instead of Base, and then attach the newly defined method to this function.\n\n\n\n\n2.8.3.4.4 Defining type conversions\n\nimport Base: convert\n\nRadian(dms::DMS) = Radian(deg2rad(dms.seconds / 3600))\nDMS(radian::Radian) = DMS(floor(Int, rad2deg(radian.radians) * 3600))\n\nconvert(::Type{Radian}, dms::DMS) = Radian(dms)\nconvert(::Type{DMS}, radian::Radian) = DMS(radian)\n\nconvert (generic function with 673 methods)\n\n\n\n\n2.8.3.4.5 Defining type promotions\nIn fact, promote() does its job by calling the promote_rule() function.\n\nimport Base: promote_rule\n\n# If an expression contains both Radian and DMS, convert DMS into Radian\npromote_rule(::Type{Radian}, ::Type{DMS}) = Radian\n\npromote_rule (generic function with 210 methods)\n\n\n\n\n2.8.3.4.6 Defining arithmetic operations\n\nimport Base: +, -\n\n# If an expression contains both Radian and DMS, convert DMS into Radian, and then perform arithmetic operations of Radian\n+(θ::Angle, α::Angle) = +(promote(θ, α)...)\n-(θ::Angle, α::Angle) = -(promote(θ, α)...)\n\n+(θ::Radian, α::Radian) = Radian(θ.radians + α.radians)\n-(θ::Radian, α::Radian) = Radian(θ.radians - α.radians)\n\n+(θ::DMS, α::DMS) = DMS(θ.seconds + α.seconds)\n-(θ::DMS, α::DMS) = DMS(θ.seconds - α.seconds)\n\n- (generic function with 335 methods)\n\n\n\n\n2.8.3.4.7 Making pretty literals by using literal coefficients\n\nimport Base: *, /\n\n*(coeff::Number, dms::DMS) = DMS(0, 0, coeff * dms.seconds)\n*(dms::DMS, coeff::Number) = coeff * dms\n/(dms::DMS, denom::Number) = DMS(0, 0, dms.seconds / denom)\n\n*(coeff::Number, radian::Radian) = Radian(coeff * radian.radians)\n*(radian::Radian, coeff::Number) = coeff * radian\n/(radian::Radian, denom::Number) = Radian(radian.radians / denom)\n\nconst ° = DMS(1)\nconst rad = Radian(1.0)\n\n1.0rad\n\n\n\n\n2.8.3.4.8 Overriding standard sin() and cos() functions to only accept DMS and Radian\n\n\n\n\n\n\nCaution\n\n\n\nIn the following code snippet, we do not import sin() and cos() from the Base package, instead of overriding them (i.e. create a function and then attach the newly defined method to it).\n\n\n\n# The standard sin() and cos() only accept numbers regarded as the radian\nsin(rad::Radian) = Base.sin(rad.radians)\ncos(rad::Radian) = Base.cos(rad.radians)\n\nsin(dms::DMS) = sin(Radian(dms))\ncos(dms::DMS) = cos(Radian(dms))\n\n\n\n\n\n2.8.4 Representing unknown values\n\nnothing: indicates something not existed.\n\nThe nothing object is an instance of the type Nothing, which is a composite type without any fields.\n\n\n\n\n\n\nNote\n\n\n\nEvery instance of a composite type with zero fields is the same obeject.\n\nstruct MyNothing\n    # No fields defined here\nend\n\nobj1 = MyNothing()\nobj2 = MyNothing()\n\nobj1 == obj2\n\ntrue\n\n\nInstances of different composite types with zero fields are different.\n\nstruct AgainNothing\n    # No fields defined here\nend\n\nobj1 = MyNothing()\nobj2 = AgainNothing()\n\nobj1 == obj2\n\nfalse\n\n\n\n\n\nmissing: indicates something, which should have existed, but missing due to some reason (i.e. unlike nothing, missing data actually exists in the real world, but we don’t know what it is).\n\nThe concept of missing, which is of type Missing, a composite type with zero fields, is the same as that in statistics.\n\n\n\n\n\n\nCaution\n\n\n\nAny expression containing missing will be evaluated to missing!\nYou can use skipmissing() to filter missing out.\n\n\n\nNaN: indicates something, which is Not a Number.\n\nSimilarly, NaN also propagates through all calculations.\nThe only difference of the propagation behaviour between NaN and missing is that NaN always returns false when NaN is used in a comparison expression, where missing always returns missing:\n\nmissing &lt; 10, NaN &lt; 10\n\n(missing, false)\n\n\n\n\n\n\n\n\nCaution\n\n\n\n0/0 returns NaN.\nIn other words, 0/0 may be a valid number somewhere else, but now it doesn’t belong to any number we have already defined; thus it is regarded as NaN.\n\n\n\n#undef: indicates something undefined (i.e. a variable was not instantiated to a known value).\n\ne.g. Julia allows the construction of composite objects with uninitialized fields; however, it will throw an exception if you try to access an uninitialized field:\n\n\n\n\n\n\nNote\n\n\n\nBoth firstname and lastname in the type Person have no type annotations. If you define them with type annotations, Julia will automatically instantiate them to some values based on their types.\nIn other words, if some fields have no type annotations, then Julia has no way of guessing what the fields should be initialized to.\n\n\n\nstruct Person\n    firstname\n    lastname\n    Person(firstname::String, lastname::String) = new(firstname, lastname)  # This allows you to instantiate instances of Person with arguments\n    Person() = new()  # This allows you to instantiate instances of Person without arguments\nend\n\nfriend = Person()\n\nfriend\n\nPerson(#undef, #undef)\n\n\n\nfriend.firstname\n\nLoadError: UndefRefError: access to undefined reference\nUndefRefError: access to undefined reference\n\nStacktrace:\n [1] getproperty(x::Person, f::Symbol)\n   @ Base ./Base.jl:37\n [2] top-level scope\n   @ In[89]:2\n\n\n\n2.8.4.1 To solve infinite chain of initialization using parametric type\nA parametric type can be regarded as a function which accepts type parameters, and then returns a new type.\ne.g. if P is a parametric type, and T is a type, then P{T} returns a new type.\nYou can think of a parametric type as a template to make an actual type:\n\ntypeof(1:3)  # Equivalent to UnitRange(1, 3)\n\nUnitRange{Int64}\n\n\n\nFloatRange = UnitRange{Float64}\n\nUnitRange{Float64}\n\n\n\nFloatRange(1, 3)\n\n1.0:3.0\n\n\nWe can use the Union parametric type to solve infinite chain of initialization. Union accetps one or more type parameters, and then return a new type which can serve as placeholders for any of the types listed as type parameters.\n\nf1(x::Union{Int, String}) = x^3\n\nf1 (generic function with 1 method)\n\n\n\nf1(3)\n\n27\n\n\n\nf1(\"hello\")\n\n\"hellohellohello\"\n\n\n\nf1(1.1)\n\n\nMethodError: no method matching f1(::Float64)\n\nClosest candidates are:\n  f1(::Union{Int64, String})\n   @ Main In[93]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[96]:2\n\n\n\nNow let’s solve the problem of infinite chain of initialization using parametric type:\n\nstruct Wagon\n    cargo::Float64\n    next::Union{Wagon, Nothing}  # next can be an object of either Wagon or Nothing\nend\n\n# Calculate the total tons of cargo in the train\ncargo(w::Wagon) = w.cargo + cargo(w.next)\ncargo(::Nothing) = 0.0\n\ntrain = Wagon(6, Wagon(8, Wagon(10, nothing)))\n\ncargo(train)\n\n24.0\n\n\n\n\n\n\n2.9 Collections\nCollections are objects that store and organize other objects.\n\n2.9.1 Strings\nIn computer memory, everything is a number, including characters.\n\nA character (Char type) is quoted by ''.\n\n\nInt8('A')\n\n65\n\n\n\nChar(65)\n\n'A': ASCII/Unicode U+0041 (category Lu: Letter, uppercase)\n\n\nYou can add a number to a character, which returns a new character corresponding to the sum:\n\n'A' + 3\n\n'D': ASCII/Unicode U+0044 (category Lu: Letter, uppercase)\n\n\n\nA string is quoted by \"\" or `\"\"\"\"\"\".\n\nLong lines in strings can be broken up by preceding the newline with a backslash (\\):\n\n\"This is a long \\\nline\"\n\n\"This is a long line\"\n\n\nMerging elements into a string by join():\n\nchars = 'A':'Z'\n\njoin(chars)\n\n\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\n\nSplitting a string into characters by collect():\n\ncollect(\"HELLO\")\n\n5-element Vector{Char}:\n 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n 'E': ASCII/Unicode U+0045 (category Lu: Letter, uppercase)\n 'L': ASCII/Unicode U+004C (category Lu: Letter, uppercase)\n 'L': ASCII/Unicode U+004C (category Lu: Letter, uppercase)\n 'O': ASCII/Unicode U+004F (category Lu: Letter, uppercase)\n\n\nIn fact, you can collect() any iterable objects into an array.\n\n2.9.1.1 Unicode and UTF-8\nText strings in Julia are Unicode, encoded in UTF-8 format.\nIn Unicode, each character is given a number (code point), encoded by several bytes (code units) in computer.\nUTF-8 is the current Unicode scheme used, which uses a variable number of bytes (1-4 bytes) per character to encode characters in computer.\nYou can use codepoint() to get the code point of a character, and ncodeunits() to get the code units of a character.\nIn addition, UTF-8 is backward compatible with ASCII (encoding each character with 1 byte). You can use isascii() to check whether a character is a ASCII character.\n\ncodepoint('A'), ncodeunits('A'), isascii('A')\n\n(0x00000041, 1, true)\n\n\nAs a consequence, you can type a character by typing either the character itself or its code point.\n\n'A', '\\U41', Char(0x00000041)\n\n('A', 'A', 'A')\n\n\n\n\n2.9.1.2 String indexing\nYou can use subscript index to index each character in a string, but the step between indices is not always 1. It may be an integer greater than 1.\nYou can combine the following functions to get correct indices for each character in a string:\n\nfirstindex(): return the first index in a string.\nlastindex(): return the last index in a string.\nnextind(s, i): return the next index of the element following index i in s.\neachindex(): return the indices of each element.\nUsing for loop to iterate a string.\n\n\ns = \"123一二三\"\n\ni = firstindex(s)\nwhile i &lt;= lastindex(s)\n    println((i, s[i]))\n    i = nextind(s, i)\nend\n\n(1, '1')\n(2, '2')\n(3, '3')\n(4, '一')\n(7, '二')\n(10, '三')\n\n\n\nfor i in s\n    println(i)\nend\n\n1\n2\n3\n一\n二\n三\n\n\n\nfor i in eachindex(s)\n    println((i, s[i]))\nend\n\n(1, '1')\n(2, '2')\n(3, '3')\n(4, '一')\n(7, '二')\n(10, '三')\n\n\n\n\n2.9.1.3 String operations\n\nSplitting strings\n\n\nsplit(\"abc_def_ghi\", \"_\")\n\n3-element Vector{SubString{String}}:\n \"abc\"\n \"def\"\n \"ghi\"\n\n\n\nsplit(\"abcAdefBghi\", isuppercase)\n\n3-element Vector{SubString{String}}:\n \"abc\"\n \"def\"\n \"ghi\"\n\n\n\nConverting letters between uppercases and lowercases\n\n\nmap(uppercasefirst, split(\"abc_def_ghi\", \"_\"))\n\n3-element Vector{String}:\n \"Abc\"\n \"Def\"\n \"Ghi\"\n\n\n\nisuppercase('A')  # Check whether a single letter is in the form of uppercase\n\ntrue\n\n\n\nJoining substrings\n\n\njoin([\"abc\", \"def\", \"ghi\"], \"_\")\n\n\"abc_def_ghi\"\n\n\n\nReading from and writing to the clipboard\n\n\n# Write to the clipboard\nclipboard(\"Hello, world!\")\n\n# Read from the clipboard\nclipboard()\n\n\"Hello, world!\\n\"\n\n\nOn Linux, clipboard() works only when you have installed the xsel or xclip commands.\n\nFinding whether a substring is existed in a string by using find* functions\n\n\nfindall(\"abc\", \"abc_def_abc\")\n\n2-element Vector{UnitRange{Int64}}:\n 1:3\n 9:11\n\n\n\nfindall(isuppercase, \"AaBbCc\")\n\n3-element Vector{Int64}:\n 1\n 3\n 5\n\n\n\nConverting between numbers and strings\n\n\nparse(Float64, \"3.14\")  # The default base is 10\n\n3.14\n\n\n\nparse(Int, \"1010101\", base = 2)\n\n85\n\n\n\nstring(100)  # The default base is 10\n\n\"100\"\n\n\n\nstring(100, base = 2)\n\n\"1100100\"\n\n\n\nString concatenation\n\n\nfruit = \"apple\"\n\nstring(\"This is a(an) \", fruit, \", made in China.\")\n\n\"This is a(an) apple, made in China.\"\n\n\n\n\"This is a(an) \" * fruit * \", made in China.\"\n\n\"This is a(an) apple, made in China.\"\n\n\n\nString interpolation\n\n\n\"This is a(an) $fruit, made in China.\"\n\n\"This is a(an) apple, made in China.\"\n\n\n\n\"This is a(an) $(fruit), made in China.\"\n\n\"This is a(an) apple, made in China.\"\n\n\n\nString formatting\n\nYou can use macros @printf and @sprintf to perform string formatting. These two macros are defined in the Printf module.\nIn Julia, macros are distinguished from functions with the @ prefix.\nA macro is akin to a code generator; the call site of a macro gets replaced with other code.\n\n@printf outputs the result to the console:\n\n\nusing Printf\n\n@printf(\"π = %0.2f\", pi)  # Output pi (floating-point number) with two digits\n\nπ = 3.14\n\n\n\n@sprintf returns the result as a string.\n\n\n@sprintf(\"π = %0.2f\", pi)\n\n\"π = 3.14\"\n\n\nFor a systematic specification of the format, see here.\n\n\n2.9.1.4 Nonstandard string literals\nIn Julia, you cannot express very large numbers as number literals, so you have to express them as strings that get parsed later.\ne.g.\n\n3.14e600\n\n\nParseError:\n# Error @ ]8;;file:///home/yangrui/mywd/NeuroBorder/Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/In[127]#2:1\\In[127]:2:1]8;;\\\n\n3.14e600\n└──────┘ ── overflow in floating point literal\n\nStacktrace:\n [1] top-level scope\n   @ In[127]:2\n\n\n\n\nx = parse(BigFloat, \"3.14e600\")\n\n3.140000000000000000000000000000000000000000000000000000000000000000000000000003e+600\n\n\n\ntypeof(x)\n\nBigFloat\n\n\nIf you put such a expression into a loop, then it will be run at least once in each loop:\n\nfor i in 1:4\n    x = parse(BigFloat, \"3.14e600\")\n    println(x)\nend\n\n3.140000000000000000000000000000000000000000000000000000000000000000000000000003e+600\n3.140000000000000000000000000000000000000000000000000000000000000000000000000003e+600\n3.140000000000000000000000000000000000000000000000000000000000000000000000000003e+600\n3.140000000000000000000000000000000000000000000000000000000000000000000000000003e+600\n\n\nThis will damage the performance of your program.\nTo avoid having to parse strings to create objects such as BigFloat in each loop, Julia provides special string literals such as big\"3.14e600\".\nJulia will parse such a string literal only once for a for loop in your program, but run them many times (i.e. it won’t be parsed in each loop).\nIn other words, these objects such as BigFloat are created at parse time, rather than runtime.\n\nDateFormat strings\n\n\nIn the following code, the DateFormat object will be created in each loop:\n\n\nusing Dates\n\ndates = [\"21/7\", \"8/12\", \"28/2\"]\n\nfor s in dates\n    date = Date(s, DateFormat(\"dd/mm\"))  # Convert a date string into a date object\n    date_str = Dates.format(date, DateFormat(\"E-u\"))  # Convert a date object into a date string with given date format\n    println(date_str)\nend\n\nSaturday-Jul\nSaturday-Dec\nWednesday-Feb\n\n\n\nIn the following code, the DateFormat object will be created once, but the code becomes less clear at the first glance:\n\n\nusing Dates\n\ninformat = DateFormat(\"dd/mm\")\noutformat = DateFormat(\"E-u\")\n\ndates = [\"21/7\", \"8/12\", \"28/2\"]\n\nfor s in dates\n    date = Date(s, informat)  # Convert a date string into a date object\n    date_str = Dates.format(date, outformat)  # Convert a date object into a date string with given date format\n    println(date_str)\nend\n\nSaturday-Jul\nSaturday-Dec\nWednesday-Feb\n\n\n\nWe can use the dateformat literal to solve this problem:\n\n\nusing Dates\n\ndates = [\"21/7\", \"8/12\", \"28/2\"]\n\nfor s in dates\n    date = Date(s, dateformat\"dd/mm\")  # Convert a date string into a date object\n    date_str = Dates.format(date, dateformat\"E-u\")  # Convert a date object into a date string with given date format\n    println(date_str)\nend\n\nSaturday-Jul\nSaturday-Dec\nWednesday-Feb\n\n\nFor detailed date format specifications, see ?DateFormat.\n\nRaw strings\n\nIn regular Julia strings, characters such as $ and \\n have special meaning.\nIf you just want every character in a string to be literal, you need to prefix special characters with a \\ to escape them.\nBut the more convenient way is to prefix a string with raw to tell Julia that this is a raw string, which means that every character in it is literal.\n\nnum = 100\n\nraw\"What? $(num)?\"  # num won't be replaced by its actual value\n\n\"What? \\$(num)?\"\n\n\n\nRegular expressions\n\nIn Julia, you can create a Regex object by prefixing your regular expression string with a r.\n\ns = \"E-mail address: 123456@qq.com\"\n\nreplace(s, r\"\\d+(?=@)\" =&gt; \"abcdef\")  # Replace matched part with the pair value\n\n\"E-mail address: abcdef@qq.com\"\n\n\nIn the following code, match(r, s) will search for the first match of the regular expression r in s and return a RegexMatch object containing the match, or nothing if the match failed.\n\nrx = r\"\\d+:\\d+\"\n\nm = match(rx, \"11:30 in the morning; 12:00 in the noon\")\n\nm\n\nRegexMatch(\"11:30\")\n\n\nIf some parts of the regular expression are contained within parentheses, then these matched parts will be extracted out alone from the matched string, and you can retrieve these parts by indices:\n\nrx = r\"(\\d+):(\\d+)\"\n\nm = match(rx, \"11:30 in the morning; 12:00 in the noon\")\n\nm\n\nRegexMatch(\"11:30\", 1=\"11\", 2=\"30\")\n\n\n\nm[1], m[2]\n\n(\"11\", \"30\")\n\n\nFurther, you can give these parts names (?&lt;name&gt;) so you can retrieve them by names instead of indices:\n\nrx = r\"(?&lt;hour&gt;\\d+):(?&lt;minute&gt;\\d+)\"\n\nm = match(rx, \"11:30 in the morning; 12:00 in the noon\")\n\nm\n\nRegexMatch(\"11:30\", hour=\"11\", minute=\"30\")\n\n\n\nm[\"hour\"], m[\"minute\"]\n\n(\"11\", \"30\")\n\n\nIn addition, you can also iterate over a RegexMatch object, and many functions applicable to dictionaries also works with the RegexMatch object.\n\nNumber literals with big\n\nYou can use the big number literal to create extremely large numbers:\n\ntypeof(big\"100\")  # BigInt\n\nBigInt\n\n\n\ntypeof(big\"1e600\")  # BigFloat\n\nBigFloat\n\n\n\nDefining your own number literals with macros\n\n\nmacro int8_str(s)  # For a string literal with the prefix foo, such as foo\"100\", write foo_str\n    println(\"hello\")  # You can check how many times the \"hello\" will be printed when you call this macro in a loop\n    parse(Int8, s)  # Parse the number string and return an 8-bit number\nend\n\n@int8_str (macro with 1 method)\n\n\n\ntotal = 0\n\n# The \"hello\" will be printed only once,\n# which indicates that the 8-bit integer is created when the program is parsed,\n# not each time it is run\nfor _ in 1:4\n    total += int8\"10\"\nend\n\nhello\n\n\n\ntotal\n\n40\n\n\n\nMIME types\n\nMIME means Multipurpose Internet Mail Extensions, which is used as a standard to identify the file types across devices because Windows usually uses a filename extension to indicate the type of a file, while Unix-like system stores the file type in special attributes.\nIn Julia, you can create a MIME type object in the following way:\n\nMIME(\"text/html\")  # This denotes that the type of this file is a HTML page\n\nMIME type text/html\n\n\n\ntypeof(ans)  # The above MIME object with the type of MIME{Symbol(\"text/html\")}.\n\nMIME{Symbol(\"text/html\")}\n\n\nNow we know that MIME type is a parametric type. When you pass \"text/html\" to its constructor, the concrete type of the object is MIME{Symbol(\"text/html\")}. This is long and cumbersome to write so this is why Julia offers the shortcut MIME\"text/html\", which is a concrete MIME type, not an object.\n\nsay_hello(::MIME\"text/plain\") = \"hello world\"\nsay_hello(::MIME\"text/html\") = \"&lt;h1&gt;hello world&lt;/h1&gt;\"\n\nsay_hello (generic function with 2 methods)\n\n\n\nsay_hello(MIME(\"text/plain\"))\n\n\"hello world\"\n\n\n\nsay_hello(MIME(\"text/html\"))\n\n\"&lt;h1&gt;hello world&lt;/h1&gt;\"\n\n\n\n\n\n2.9.2 Arrays\n\n2.9.2.1 Types of arrays\n\n1D array\n\n\nColumn vector (type Vector)\n\nElements are separated by , inside [].\nCreating a column vector with default data type:\n\ncolumn_vector = [1, 2, 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nCreating a column vector with given data type:\n\ncolumn_vector = Int8[1, 2, 3]\n\n3-element Vector{Int8}:\n 1\n 2\n 3\n\n\nYou can check what type each element in an array is by using the eltype() function. If an array contains different types of elements, it will return Any.\n\nRow vector (1 by n matrix, type Matrix)\n\nElements are separated by space.\n\nrow_vector = [1 2 3]\n\n1×3 Matrix{Int64}:\n 1  2  3\n\n\n\n2D array (type Matrix)\n\nRows are separated by ;.\n\nmatrix = [1 2 3;\n          4 5 6;\n          7 8 9]\n\n# or\nmatrix = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\nColumns are separated by space:\n\nmatrix = [[1, 2, 3] [4, 5, 6] [7, 8, 9]]\n\n3×3 Matrix{Int64}:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nArray (type Array)\n\nThe dimension of an array is greater than 2.\n\nzeros(Int64, 2, 3, 4)  # two rows, three columns, and four slices\n\n2×3×4 Array{Int64, 3}:\n[:, :, 1] =\n 0  0  0\n 0  0  0\n\n[:, :, 2] =\n 0  0  0\n 0  0  0\n\n[:, :, 3] =\n 0  0  0\n 0  0  0\n\n[:, :, 4] =\n 0  0  0\n 0  0  0\n\n\n\n\n2.9.2.2 Creating arrays by specific functions\nzeros(), ones(), fill(), rand().\n\n\n\n\n\n\nNote\n\n\n\nArrays can contain any type of element.\nYou can check the type of an object by using either typeof(), which reports the types of the object itself and its elements; or eltype(), which only reports the type of its elements.\nJulia will guess the type of elements in an array if it’s not given explicitly when an array is created.\nIf an array contains different types of elements, then the type of elements in this array will be Any, which means that you can store any type of values.\nWhen you add elements to an array by using push!(), it will check whether the type of elements to be added is consistent with the type of elements in this array, or whether the type of elements to be added can be converted to the type of elements in this array. If both failed, Julia will raise an error!\n\n\n\n\n2.9.2.3 Accessing array attributes\n\nsize(): the size of each dimension of an array.\neltype(): the type of elements in an array.\ntypeof(): the type of the object itself and its elements.\nndims(): the dimension of an array.\nlength(): total number of elements in an array.\nreshape(): change the shape of an array.\nnorm(): magnitude of a vector, calculated by the following formula (this function comes from the package LinearAlgebra).\n\n\\[\n\\|A\\|_p = \\left(\\sum_{i=1}^n |a_i|^p \\right)^{1/p}\n\\]\n\n\n2.9.2.4 Operartions on arrays\nSuppose we have:\n\namounts = [4, 2, 5, 8, 1, 10]\n\n6-element Vector{Int64}:\n  4\n  2\n  5\n  8\n  1\n 10\n\n\n\nprices = [15.0, 2.5, 3.8, 9.0, 10.5, 8.5]\n\n6-element Vector{Float64}:\n 15.0\n  2.5\n  3.8\n  9.0\n 10.5\n  8.5\n\n\nNote: both amounts and prices are column vectors.\n\nsum()\n\n\nsum(amounts)\n\n30\n\n\n\npush!(): insert one or more items into a collection.\nsort() or sort!()\n\n\n# Not modify input in place\nsort(amounts)\n\n6-element Vector{Int64}:\n  1\n  2\n  4\n  5\n  8\n 10\n\n\n\namounts\n\n6-element Vector{Int64}:\n  4\n  2\n  5\n  8\n  1\n 10\n\n\n\n# Modify input in place\nsort!(amounts)\n\n6-element Vector{Int64}:\n  1\n  2\n  4\n  5\n  8\n 10\n\n\n\namounts\n\n6-element Vector{Int64}:\n  1\n  2\n  4\n  5\n  8\n 10\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy convention, Julia functions never modify any of their inputs in place.\nIf it is necessary to modify inputs in place, Julia has established the convention of tacking on an exclamation mark (!) to the name of any function which modifies its input in place instead of returning a modified version.\n\n\n\nElement-wise operations: .+, .-, .*, ./.\n\n\namounts .* prices\n\n6-element Vector{Float64}:\n 15.0\n  5.0\n 15.2\n 45.0\n 84.0\n 85.0\n\n\n\nPerforming statistics by using Statistics.\nPerforming operations of linear algebra by using LinearAlgebra.\n\n\n\n2.9.2.5 Slicing and dicing an array\nElements in a Julia array are numbered starting from 1 (i.e. 1-based indexing)!\n\nvec = [1, 2, 3, 4, 5, 6]\n\n6-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n\n\n\nAccessing elements by using [index].\n\nFor arrays with dimension greater than 1, you can use [dim1, dim2, ...].\n\nvec[3]\n\n3\n\n\nOf course, subsetting and then assignment is supported:\n\nvec[3] = 100\n\n100\n\n\n\nUsing begin and end to access the first and last element.\n\n\nvec[begin], vec[end]\n\n(1, 6)\n\n\n\nUsing : to access all elements of some dimension.\n\n\nvec[:]  # Access the whole vector\n\n6-element Vector{Int64}:\n   1\n   2\n 100\n   4\n   5\n   6\n\n\n\nA = rand(Int64, 3, 3)\n\nA[:, 1]  # Access the 1st column\n\n3-element Vector{Int64}:\n  2750603897720011220\n -8230328028304376042\n  -626162070422440743\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll slice operations return copies of data.\n\nA = collect(1:6)\n\nB = A[4:end]\n\nB[1] = 100\n\nB\n\n3-element Vector{Int64}:\n 100\n   5\n   6\n\n\n\nA\n\n6-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n\n\nInstead, to avoid copying data during slicing an array, you can prefix the @view macro to the slice operations, since it will only return a view of subset of the array.\n\nA = collect(1:6)\n\nB = @view A[4:end]\n\nB[1] = 100\n\nB\n\n3-element view(::Vector{Int64}, 4:6) with eltype Int64:\n 100\n   5\n   6\n\n\n\nA\n\n6-element Vector{Int64}:\n   1\n   2\n   3\n 100\n   5\n   6\n\n\n\n\n\n\n2.9.2.6 Combining arrays\ncat(), hcat(), and vcat().\n\n\n\n2.9.3 Tuples\nElements are separated by , inside ().\n\nt = (1, 2, 3)\n\n(1, 2, 3)\n\n\n\n\n\n\n\n\nNote\n\n\n\nCreating a tuple containing only one element with (1,) (i.e. adding a , after the element).\nTuples are immutable once created.\n\n\n\n2.9.3.1 Named tuples\n\nstudent = (name = \"Bob\", score  = 99, height = 2)\n\n# Index by Symbol or dot\nstudent[:name], student.name\n\n(\"Bob\", \"Bob\")\n\n\n\n# Symbol &lt;==&gt; String\nSymbol(\"price\"), string(:price)\n\n(:price, \"price\")\n\n\n\n\n\n2.9.4 Dictionaries\nA dictionary is made up of a number of pairs of key =&gt; value, where key and value can be any type of values.\n\n2.9.4.1 Creating a dictionary\n\nCreating a pair with the arrow operator =&gt;:\n\n\np = 'a' =&gt; 1  # This is a pair with type Pair\n\ntypeof(p)\n\nPair{Char, Int64}\n\n\n\ndump(p)  # You can use dump() to look at the fields of any value\n\nPair{Char, Int64}\n  first: Char 'a'\n  second: Int64 1\n\n\n\n# From the output of dump(), we can easily see how to get values of a pair\n# This will generate a tuple by putting several values in one line by separating them with a comma\n# the functions first() and last() are versatile for ordered collections\np.first, p.second, first(p), last(p), p[1], p[2]\n\n('a', 1, 'a', 1, 'a', 1)\n\n\n\nYou can provide a list of pairs to create a dictionary:\n\n\nd = Dict('a' =&gt; 1, 'b' =&gt; 2, 'c' =&gt; 3)\n\ntypeof(d)\n\nDict{Char, Int64}\n\n\n\ndump(d)  # Checking the fields of a dictionary\n\nDict{Char, Int64}\n  slots: Array{UInt8}((16,)) UInt8[0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xf4, 0xad, 0x00, 0x00, 0x00, 0x00, 0x00, 0xe3, 0x00]\n  keys: Array{Char}((16,))\n    1: Char '\\0'\n    2: Char '\\0'\n    3: Char '\\0'\n    4: Char '\\0'\n    5: Char '\\0'\n    ...\n    12: Char '\\0'\n    13: Char '\\0'\n    14: Char '\\0'\n    15: Char 'b'\n    16: Char '\\0'\n  vals: Array{Int64}((16,)) [2478498643202684469, 8751724873810518572, 7311705192714626416, 8099005258945295715, 7310315409782241644, 2466321564960910962, 2479642204066177401, 1, 3, 3475145254212088365, 3546363903228459569, 7598542776403373090, 3687943565983510127, 7598543875864669218, 2, 7220446003176039012]\n  ndel: Int64 0\n  count: Int64 3\n  age: UInt64 0x0000000000000003\n  idxfloor: Int64 8\n  maxprobe: Int64 1\n\n\n\nPassing an array of pairs to the dictionary constructor:\n\n\na = ['a' =&gt; 1, 'b' =&gt; 2, 'c' =&gt; 3]\n\nDict(a)\n\nDict{Char, Int64} with 3 entries:\n  'a' =&gt; 1\n  'c' =&gt; 3\n  'b' =&gt; 2\n\n\n\nPassing an array of tuples containing only two elements to Dict():\n\n\na = [('a' =&gt; 1), ('b' =&gt; 2), ('c' =&gt; 3)]\n\nDict(a)\n\nDict{Char, Int64} with 3 entries:\n  'a' =&gt; 1\n  'c' =&gt; 3\n  'b' =&gt; 2\n\n\n\nCreating an empty dictionary:\n\n\nDict()\n\nDict{Any, Any}()\n\n\n\nCreating an empty dictionary with given types of keys and values:\n\n\nd = Dict{String, Int64}()\n\nDict{String, Int64}()\n\n\nIn the above case, you must provide the keys and values with matched types as set above:\n\nd[\"a\"] = 1\n\n1\n\n\n\nd['b'] = 2  # This will raise an error, because the type of 'b' is Char, not String\n\n\nMethodError: Cannot `convert` an object of type Char to an object of type String\n\nClosest candidates are:\n  convert(::Type{String}, ::JuliaSyntax.Kind)\n   @ JuliaSyntax ~/.julia/packages/JuliaSyntax/q0tWf/src/kinds.jl:975\n  convert(::Type{String}, ::Base.JuliaSyntax.Kind)\n   @ Base /cache/build/builder-amdci4-0/julialang/julia-release-1-dot-10/base/JuliaSyntax/src/kinds.jl:975\n  convert(::Type{String}, ::String)\n   @ Base essentials.jl:321\n  ...\n\n\nStacktrace:\n [1] setindex!(h::Dict{String, Int64}, v0::Int64, key0::Char)\n   @ Base ./dict.jl:367\n [2] top-level scope\n   @ In[188]:1\n\n\n\n\nCreating a dictionary from two separate arrays zipped by zip() function:\n\n\nDict(zip('a':'c', 1:3))\n\nDict{Char, Int64} with 3 entries:\n  'a' =&gt; 1\n  'c' =&gt; 3\n  'b' =&gt; 2\n\n\n\n\n\n\n\n\nNote\n\n\n\nzip() function can zip the corresponding values in a list of arrays into paired tuples, until any of them is exhausted.\n\ncollect(zip('a':'c', 1:3, 'A':'C'))\n\n3-element Vector{Tuple{Char, Int64, Char}}:\n ('a', 1, 'A')\n ('b', 2, 'B')\n ('c', 3, 'C')\n\n\n\n\n\n\n2.9.4.2 Accessing elements\n\nd = Dict(i =&gt; j for (i, j) in zip('A':'F', 'a':'f'))\n\nDict{Char, Char} with 6 entries:\n  'C' =&gt; 'c'\n  'D' =&gt; 'd'\n  'A' =&gt; 'a'\n  'E' =&gt; 'e'\n  'F' =&gt; 'f'\n  'B' =&gt; 'b'\n\n\n\nBy key:\n\n\nd['F']\n\n'f': ASCII/Unicode U+0066 (category Ll: Letter, lowercase)\n\n\n-By get(dict, key, default): if the key is not in the dict, it will return the default, instead of raising an error.\n\nget(d, 'Z', -1)  #\n\n-1\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can use keys() and values() to get all keys and values, respectively.\nYou can check whether a dictionary contains a key by using haskey(dict, key).\n\n\n\n\n\n2.9.5 Sets\n\nCreating sets\n\n\nfruits = Set([\"apple\", \"banana\", \"peach\", \"pear\", \"orange\"])\n\nSet{String} with 5 elements:\n  \"peach\"\n  \"pear\"\n  \"orange\"\n  \"banana\"\n  \"apple\"\n\n\n\nProperties of sets\n\nThe set in Julia is the very set in mathematics.\nFor a given set S, the following hold:\n\nEach element x is either in S or not in S.\nElements are unordered in S.\nThere are no duplicate elements in S.\n\n\nSet-specific operations\n\n\nUnion: ∪ or union().\nIntersection: ∩ or intersect().\nDifference: setdiff().\n\nCertainly, you can check whether an element belongs to a set or not (see Note 1), as well as whether a set is a (proper) subset of the other (see Note 2).\n\n\n\n\n\n\nNote 2: Subset operator ⊆\n\n\n\nYou can use issubset(), ⊆, ⊇, or ⊈ to judge the relationship between any two sets.\n\n\n\n\n2.9.6 Collection comprehension\nAn example in terms of an array: [F(x, y, ...) for x = rx, y = ry, ...], where the latter for is nested within the former one, and generated values can be filtered using the if keyword.\n\n[i for i in 1:10 if i%2 == 0]\n\n5-element Vector{Int64}:\n  2\n  4\n  6\n  8\n 10\n\n\n\n[(i, j, k) for (i, j, k) in zip('A':'F', 1:6, 'a':'f')]  # For (i, j, k), () is mandatory\n\n6-element Vector{Tuple{Char, Int64, Char}}:\n ('A', 1, 'a')\n ('B', 2, 'b')\n ('C', 3, 'c')\n ('D', 4, 'd')\n ('E', 5, 'e')\n ('F', 6, 'f')\n\n\n\nDict('A'+i =&gt; i+1 for i in 0:10)\n\nDict{Char, Int64} with 11 entries:\n  'K' =&gt; 11\n  'J' =&gt; 10\n  'I' =&gt; 9\n  'H' =&gt; 8\n  'E' =&gt; 5\n  'B' =&gt; 2\n  'C' =&gt; 3\n  'D' =&gt; 4\n  'A' =&gt; 1\n  'G' =&gt; 7\n  'F' =&gt; 6\n\n\n\n[[j for j in 1:6] for i in 1:3]\n\n3-element Vector{Vector{Int64}}:\n [1, 2, 3, 4, 5, 6]\n [1, 2, 3, 4, 5, 6]\n [1, 2, 3, 4, 5, 6]\n\n\nYou can specify the type of elements generated by prefixing with a wanted type:\n\nVector{Float64}[[j for j in 1:6] for i in 1:3]\n\n3-element Vector{Vector{Float64}}:\n [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n\n\n\n\n2.9.7 Generator\nCollection comprehensions can also be written without the enclosing brackets, producing an object known as a generator.\n\nsum(1/n^2 for n = 1:1000)\n\n1.6439345666815615\n\n\nNote: when writing a generator expression with multiple dimensions inside an argument list, parentheses are needed to separate the generator from subsequent arguments.\n\nmap(tuple, (1/(i+j) for i=1:2, j=1:2), [1 3; 2 4])\n\n2×2 Matrix{Tuple{Float64, Int64}}:\n (0.5, 1)       (0.333333, 3)\n (0.333333, 2)  (0.25, 4)\n\n\nGenerating a matrix:\n\n[100i + j for i=1:3, j=1:3]\n\n3×3 Matrix{Int64}:\n 101  102  103\n 201  202  203\n 301  302  303\n\n\nThe above code is equivalent to:\n\nA = zeros(Float64, 3, 3)\n\nfor i in 1:3\n    for j in 1:3\n        A[i,j] = 100i + j\n    end\nend\n\nA\n\n3×3 Matrix{Float64}:\n 101.0  102.0  103.0\n 201.0  202.0  203.0\n 301.0  302.0  303.0\n\n\n\n\n2.9.8 Enumerating values and indices\n\ncollect(enumerate('A':'F'))\n\n6-element Vector{Tuple{Int64, Char}}:\n (1, 'A')\n (2, 'B')\n (3, 'C')\n (4, 'D')\n (5, 'E')\n (6, 'F')\n\n\n\n[(i, val) for (i, val) in enumerate('A':'F')]\n\n6-element Vector{Tuple{Int64, Char}}:\n (1, 'A')\n (2, 'B')\n (3, 'C')\n (4, 'D')\n (5, 'E')\n (6, 'F')\n\n\n\n\n2.9.9 Creating an enum type with @enum macro\n\n@enum Fruit apple peach pear banana orange\n\nFruit\n\nEnum Fruit:\napple = 0\npeach = 1\npear = 2\nbanana = 3\norange = 4\n\n\n\nFruit(0), Fruit(3)  # Access by index\n\n(apple, banana)\n\n\n\ninstances(Fruit)  # Return all possible values\n\n(apple, peach, pear, banana, orange)\n\n\n\n\n2.9.10 Understanding Julia collections\nTwo key questions:\n\nWhat makes something a collection?\nWhat are the differences and similarities between different collection types?\n\n\n2.9.10.1 What makes something a collection\nAt a minimum, you are expected to extend the iterate() function for your data type with the following methods to make your data type a collection:\n\n\n\n\n\n\n\nMethod\nPurpose\n\n\n\n\niterate(iter)\nReturn the first item and the next state (e.g. the index of the next item)\n\n\niterate(iter, state)\nReturn the current item and the next state\n\n\n\nAn index-based iteration example:\n\nDefine the Cluster type to be iterated:\n\n\n# Define the Engine type\nabstract type Engine end\n\n# Define valid engine models\nstruct Panda &lt;: Engine\n    count::Integer\nend\nstruct Bear &lt;: Engine\n    count::Integer\nend\nstruct Dog &lt;: Engine\n    count::Integer\nend\n\n# Define the Cluster type, which can consist of many engine models\nstruct Cluster &lt;: Engine\n    engines::Vector{Engine}  # A vector with elements of Engine type\nend\n\n\nengine_type(::Panda) = \"Panda\"\nengine_type(::Bear) = \"Bear\"\nengine_type(::Dog) = \"Dog\"\n\nengine_count(engine::Union{Panda, Bear, Dog}) = engine.count\n\nengine_count (generic function with 1 method)\n\n\n\nExtend the iterate() function:\n\n\nimport Base: iterate\n\n# Start the iteration\nfunction iterate(cluster::Cluster)\n    cluster.engines[1], 2  # Return the first element and the index of the next element\nend\n\n# Get the next element\nfunction iterate(cluster::Cluster, i::Integer)\n    if i &gt; length(cluster.engines)\n        nothing  # Return nothing to indicate you reached the end\n    else\n        cluster.engines[i], i+1  # Don't forget to return the index of the next element\n    end\nend\n\niterate (generic function with 488 methods)\n\n\n\nIterate the Cluster instance:\n\n\ncluster = Cluster([Panda(1), Bear(5), Dog(10)])\n\nCluster(Engine[Panda(1), Bear(5), Dog(10)])\n\n\n\nfor engine in cluster\n    println(engine_type(engine), \": \", engine_count(engine))\nend\n\nPanda: 1\nBear: 5\nDog: 10\n\n\nInternally, the Julia JIT compiler will convert this for loop into a lower-level while loop, which looks like the following code:\n\nnext = iterate(cluster)  # Begin iteration\nwhile next != nothing  # Check if you reached the end of the iteration\n    (engine, i) = next\n    println(engine_type(engine), \": \", engine_count(engine))\n    next = iterate(cluster, i)  # Advance to the next element\nend\n\nPanda: 1\nBear: 5\nDog: 10\n\n\nA linked list example:\n\nimport Base: iterate\n\nstruct MyLinkedList\n    id::Int\n    name::String\n    next::Union{MyLinkedList, Nothing}\nend\n\n# First, Julia uses the instance of MyLinkedList as the unique argument to retrieve the first element and the flag of the next element\niterate(first::MyLinkedList) = ((first.id, first.name), first.next)  # The first value is what you want to retrieve; the second value is used to tell where the next element is\n# Then, Julia uses the instance of MyLinkedList and the flag of the next element, returned by the previous one to retrieve the next element and the flag of the next element, in contrast with the current one\niterate(prev::MyLinkedList, current::MyLinkedList) = ((current.id, current.name), current.next)\n# Finally, iteration-supported function needs a nothing to indicate that the iteration is done\niterate(::MyLinkedList, ::Nothing) = nothing  # Return nothing if the iteration is done\n\nx = MyLinkedList(1, \"1st\", MyLinkedList(2, \"2nd\", MyLinkedList(3, \"3rd\", nothing)))\n\nfor (id, name) in x  # The parentheses are essential\n    println(id, \": \", name)\nend\n\n1: 1st\n2: 2nd\n3: 3rd\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor multiple assignment, parentheses are mandatory in for loop; otherwise it’s trivial.\n\n\nA similar while counterpart of for:\n\nnext = iterate(x)\nwhile next != nothing\n    current, next = next\n    println(current[1], \": \", current[2])\n    next = iterate(x, next)\nend\n\n1: 1st\n2: 2nd\n3: 3rd\n\n\n\nAdding support for map() and collect()\n\nIf you run collect() on x, you will get the following error:\n\ncollect(x)\n\n\nMethodError: no method matching length(::MyLinkedList)\n\nClosest candidates are:\n  length(::Automa.ByteSet)\n   @ Automa ~/.julia/packages/Automa/Aq8Mq/src/byteset.jl:7\n  length(::Tables.DictRowTable)\n   @ Tables ~/.julia/packages/Tables/8p03y/src/dicts.jl:118\n  length(::Core.Compiler.InstructionStream)\n   @ Base show.jl:2777\n  ...\n\n\nStacktrace:\n [1] _similar_shape(itr::MyLinkedList, ::Base.HasLength)\n   @ Base ./array.jl:710\n [2] _collect(cont::UnitRange{Int64}, itr::MyLinkedList, ::Base.HasEltype, isz::Base.HasLength)\n   @ Base ./array.jl:765\n [3] collect(itr::MyLinkedList)\n   @ Base ./array.jl:759\n [4] top-level scope\n   @ In[217]:2\n\n\n\nOf course, you can simply define a length() method for MyLinkedList type like the following:\n\nimport Base: length\n\nlength(::Nothing) = 0\nlength(x::MyLinkedList) = 1 + length(x.next)\n\nlength(x)\n\n3\n\n\nHowever, the time it takes to calculate the length of MyLinkedList is proportional to its length. Such algorithms are referred to as linear or \\(O(n)\\) in big-O notation.\nInstead, we will implement an IteratorSize() method:\n\nimport Base: IteratorSize\n\nIteratorSize(::Type{MyLinkedList}) = Base.SizeUnknown()\n\nIteratorSize\n\n\nBy default, IteratorSize() is defined like the following:\n\nIteratorSize(x) = IteratorSize(typeof(x))\nIteratorSize(::Type) = HasLength()\n\n\n\n\n\n\n\nNote\n\n\n\nHere, IteratorSize() is a trait of Julia collections. It is used to indicate whether a collection has a known length.\nIn Julia, traits are defined as abstract types. The values a trait can have are determined by a concrete subtype.\nFor example, the trait IteratorSize() has subtypes SizeUnknown(), HasLength(), and so on.\nIf the IteratorSize() trait is defined as HasLength(), then Julia will call length() to determine the size of the result array produced from collect(). Instead, when you define this trait as SizeUnknown(), Julia will use an empty array for output that grows as needed.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nfoo(::Type{Integer}) = 'A'  # Only accepting the type Integer as a valid argument\n\nfoo(Integer)\n\n'A': ASCII/Unicode U+0041 (category Lu: Letter, uppercase)\n\n\n\nfoo(Int64)\n\n\nMethodError: no method matching foo(::Type{Int64})\n\nClosest candidates are:\n  foo(::Int64)\n   @ Main In[23]:1\n  foo(::Type{Integer})\n   @ Main In[220]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[221]:2\n\n\n\n\nfb(::Type{&lt;:Integer}) = 'B'  # Integer as well as its all subtypes are valid arguments\n\nfb(Integer)\n\n'B': ASCII/Unicode U+0042 (category Lu: Letter, uppercase)\n\n\n\nfb(Int64)\n\n'B': ASCII/Unicode U+0042 (category Lu: Letter, uppercase)\n\n\n\n\n\nAdding more interfaces to your data type\n\nTo make your data type more versatile, you may add more interfaces to your data type.\nFor example, as a collection, your data type should support getting, setting, adding, and removing elements, which are achieved by the following methods:\n\ngetindex(): this makes it possible to access elements with [].\nsetindex!(): this makes it possible to set elements with [].\npush!(): adding elements to the back of a collection.\npushfirst!(): adding elements to the front of a collection.\npop!(): removing the last element.\npopfirst!(): removing the first element.\n\nIn a word, some interfaces to a collection are achieved by implicitly calling some methods by Julia itself (e.g. looping a collection); some other interfaces to a collection are achieved by explicitly calling some methods by users (e.g. adding elements).\n\n\n\n\n2.10 Functional programming\n\n2.10.1 Higher order functions\nThese are funtions that take other functions as arguments and/or return functions.\n\nmap(f, iterable): apply f to each element of iterable.\n\n\nmap(uppercase, 'a':'z')\n\n26-element Vector{Char}:\n 'A': ASCII/Unicode U+0041 (category Lu: Letter, uppercase)\n 'B': ASCII/Unicode U+0042 (category Lu: Letter, uppercase)\n 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n 'D': ASCII/Unicode U+0044 (category Lu: Letter, uppercase)\n 'E': ASCII/Unicode U+0045 (category Lu: Letter, uppercase)\n 'F': ASCII/Unicode U+0046 (category Lu: Letter, uppercase)\n 'G': ASCII/Unicode U+0047 (category Lu: Letter, uppercase)\n 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n 'I': ASCII/Unicode U+0049 (category Lu: Letter, uppercase)\n 'J': ASCII/Unicode U+004A (category Lu: Letter, uppercase)\n 'K': ASCII/Unicode U+004B (category Lu: Letter, uppercase)\n 'L': ASCII/Unicode U+004C (category Lu: Letter, uppercase)\n 'M': ASCII/Unicode U+004D (category Lu: Letter, uppercase)\n 'N': ASCII/Unicode U+004E (category Lu: Letter, uppercase)\n 'O': ASCII/Unicode U+004F (category Lu: Letter, uppercase)\n 'P': ASCII/Unicode U+0050 (category Lu: Letter, uppercase)\n 'Q': ASCII/Unicode U+0051 (category Lu: Letter, uppercase)\n 'R': ASCII/Unicode U+0052 (category Lu: Letter, uppercase)\n 'S': ASCII/Unicode U+0053 (category Lu: Letter, uppercase)\n 'T': ASCII/Unicode U+0054 (category Lu: Letter, uppercase)\n 'U': ASCII/Unicode U+0055 (category Lu: Letter, uppercase)\n 'V': ASCII/Unicode U+0056 (category Lu: Letter, uppercase)\n 'W': ASCII/Unicode U+0057 (category Lu: Letter, uppercase)\n 'X': ASCII/Unicode U+0058 (category Lu: Letter, uppercase)\n 'Y': ASCII/Unicode U+0059 (category Lu: Letter, uppercase)\n 'Z': ASCII/Unicode U+005A (category Lu: Letter, uppercase)\n\n\n\nreduce(f, iterable): apply f to the element of iterable in an iterable way.\n\n\nreduce(+, 1:100)\n\n5050\n\n\n\nfilter(predicate, iterable): return a subset of iterable based on predicate.\n\nNote: a predicate is a function that takes an element of iterable and always returns a Boolean value.\n\nfilter(isuppercase, ['A', 'b', 'C', 'd'])\n\n2-element Vector{Char}:\n 'A': ASCII/Unicode U+0041 (category Lu: Letter, uppercase)\n 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n\n\n\n\n\n2.11 I/O (Networking and Streams)\n\n2.11.1 I/O types\nThe Julia I/O system is centered on the abstract type IO, which has several concrete types, such as IOStream, IOBuffer, Process and TCPSocket. Each type allows you to read and write data from different I/O devices, such as files, in-memory buffers, running processes, or network connections.\n\n\n2.11.2 Stream I/O\nAll Julia streams expose at least a read() and a write() method, taking the stream as their first argument.\nThe write() method operates on binary streams, which means that values do not get converted to any canonical text representation but are written out as is.\nwrite() takes the data to write as its second argument:\n\nwrite(stdout, \"Hello World\")  # return 11, the number of bytes written to stdout\n\nHello World\n\n\n11\n\n\n\nwrite(stdout, \"Hello World\");  # supress return value 11 with ;\n\nHello World\n\n\nread() takes the type of data to be read as its second argument:\n\njulia&gt; read(stdin, Char)\nA\n# 'A': ASCII/Unicode U+0041 (category Lu: Letter, uppercase)\n\nTo read a simple byte array:\n\njulia&gt; x = zeros(UInt8, 6)\n# 6-element Vector{UInt8}:\n#  0x00\n#  0x00\n#  0x00\n#  0x00\n#  0x00\n#  0x00\n\njulia&gt; read!(stdin, x)  # read from stdin and store them in x\nabcdef\n# 6-element Vector{UInt8}:\n#  0x61\n#  0x62\n#  0x63\n#  0x64\n#  0x65\n#  0x66\n\njulia&gt; x\n# 6-element Vector{UInt8}:\n#  0x61\n#  0x62\n#  0x63\n#  0x64\n#  0x65\n#  0x66\n\nThe above is equivalent to:\n\njulia&gt; x = read(stdin, 6)\nabcdef\n# 6-element Vector{UInt8}:\n#  0x61\n#  0x62\n#  0x63\n#  0x64\n#  0x65\n#  0x66\n\njulia&gt; x\n# 6-element Vector{UInt8}:\n#  0x61\n#  0x62\n#  0x63\n#  0x64\n#  0x65\n#  0x66\n\nTo read the entire line:\n\njulia&gt; readline(stdin)\n1234567890\n# \"1234567890\"\n\nTo read all lines of an I/O stream or a file as a vector of strings using redalines(io).\nTo read every line from stdin you can use eachline(io):\n\n# you can use Ctrl + D to terminate the input (play the role of EOF) \njulia&gt; for line in eachline(stdin)\n           println(\"Found $line\")\n       end\n123456\n# Found 123456\nabcdef\n# Found abcdef\n\nRead by character:\n\njulia&gt; while !eof(stdin)\n       x = read(stdin, Char)\n       println(\"Found: $x\")\n       end\nabcdef\n# Found: a\n# Found: b\n# Found: c\n# Found: d\n# Found: e\n# Found: f\n\n\n\n2.11.3 Text I/O\nFor text I/O, using the print() or show() methods, taking the stream as their first argument, which is a mandatory convention.\nprint() is used to write a canonical text representation of a value to the output stream. If a canonical text representation exists for the value, it is printed without any adornments. If no canonical text representation exists, print() calls the show() function to display the value.\nprint() is more about customizing the output for specific messages, while show() is about displaying complex objects in a readable format. The choice between print() and show() depends on the context and the desired output format. For simple text output, print() is often sufficient, but for displaying the structure and content of complex objects, show() is the preferred choice.\nFor custom pretty-printing of your own types, define show() (which calls print() to customize the output content and style of your own type) instead of print() for it.\nOf course, for more pretty-printing, Julia also provides functions such as println() (with trailing newline), printstyled() (support some rich displays, such as colors), etc.\n\n\n2.11.4 I/O output contextual properties\nSometimes I/O output can benefit from the ability to pass contextual information into show methods. The IOContext object provides this framework for associating arbitrary metadata with an I/O object.\n\n\n2.11.5 Working with files\n\n# 1. Write content to a file with the write(filename::String, content) method\n# 2. Read the contents of a file with the read(filename::String) method, or read(filename::String, String) to the contents as a string\njulia&gt; write(\"hello.txt\", \"Hello, World!\")  # return the number of bytes written\n# 13\n\njulia&gt; read(\"hello.txt\")  # return bytes\n# 13-element Vector{UInt8}:\n#  0x48\n#  0x65\n#  0x6c\n#  0x6c\n#  0x6f\n#  0x2c\n#  0x20\n#  0x57\n#  0x6f\n#  0x72\n#  0x6c\n#  0x64\n#  0x21\n\njulia&gt; read(\"hello.txt\", String)  # return the contents as a string\n# \"Hello, World!\"\n\nInstead of directly passing a string as the file name, you can first open a file with open(filename::AbstractString, [mode::AbstractString]; lock = true) -&gt; IOStream, which returns an IOStream object that you can use to read/write things from/to the file.\n\njulia&gt; f = open(\"hello.txt\")  # open a file\n# IOStream(&lt;file hello.txt&gt;)\n\njulia&gt; readlines(f)  # do something (read/write)\n# 1-element Vector{String}:\n#  \"Hello, World!\"\n\njulia&gt; close(f)  # close the file\n\nInstead of closing the file manually, you can pass a function (accepting the IOStream returned by open() as its first argument) as the first argument of open() method, which will close the file upon completion for you.\n\njulia&gt; open(\"hello.txt\") do io\n       uppercase(read(io, String))\n       end\n# \"HELLO, WORLD!\"\n\n\n\n2.11.6 Working with networking\n\n\n\n\n\n\n\nA rough scheme of five network layers\n\n\n\n2.11.6.1 TCP (Transmission Control Protocol)\nIn a word, TCP provides highly reliable data transmission services with these features: connection-oriented, reliable, flow control, congestion control, error checking, slower than UDP due to providing such features.\n\nusing Sockets\n\n## server side\nerrormonitor(@async begin\n        server = listen(2000)  # 1. listen on a given port on a specified address; create a server waiting for incoming connections on the specified port 2000 in this case; a TCPServer socket is returned; in computer networking, a socket is a software structure that provides a bidirectional communication channel between two processes, where one process acts as a server and the other as a client\n        while true\n            sock = accept(server)  # 2. retrieve a connection to the client that is trying to connect to the server we just created\n            @async while isopen(sock)  # 3. if connected, do something between the server and the client\n                write(sock, string(\"The server has received the message from the client: \", readline(sock, keep = true)))  # 4. read something from the client and then write something to the client; keep = true means that these trailing newline characters are also returned (instead of removing them from the line before it is returned) as part of the line\n            end\n        end\nend)\n\n## client side\nclient = connect(2000)  # 1. connect to a host on a given port; return a TCPSocket socket\n\nerrormonitor(@async while isopen(client)  # 2. if connected, do something\n    write(stdout, readline(client, keep = true))  # 3. read something from the server and then print them to the termimal (stdout)\nend)\n\nprintln(client, \"Hello world from the client\")  # 3. write something to the server\n# The server has received the message from the client: Hello world from the client\n\n## finally, use close() to disconnect the socket\nclose(client)\n\nNote: some details about listen() and connect():\n\n## 1. connect([host], port::Integer) -&gt; TCPSocket  #  Connect to the host `host` on port `port` (TCPServer)\nlisten(2000)  # listen on localhost:2000 (IPv4)\nlisten(ip\"127.0.0.1\", 2000)  # equivalent to the above (IPv4)\nlisten(ip\"::1\", 2000)  # equivalent to the above (IPv6)\nlisten(IPv4(0), 2000)  # listen on port 2000 on all IPv4 interfaces\nlisten(IPv6(0), 2000)  # listen on port 2000 on all IPv6 interfaces\n\n## 2. connect(path::AbstractString) -&gt; PipeEndpoint  # connect to the named pipe (Windows) / UNIX domain socket at `path` (PipeServer)\nlisten(\"testsocket\")  # listen on a UNIX domain socket\nlisten(\"\\\\\\\\.\\\\pipe\\\\testsocket\")  # listen on a Windows named pipe (\\\\.\\pipe\\)\n\nThe difference between TCP and named pipes or UNIX domain sockets is subtle and has to do with the accept() and connect() methods:\n\naccept(server[, client])  # Accepts a connection on the given server and returns a connection to the client. An uninitialized client stream may be provided, in which case it will be used instead of creating a new stream.\n\nconnect([host], port::Integer) -&gt; TCPSocket  # Connect to the host `host` on port `port`.\nconnect(path::AbstractString) -&gt; PipeEndpoint  # Connect to the named pipe / UNIX domain socket at path.\n\nResolving IP addresses:\n\njulia&gt; getaddrinfo(\"google.com\")\n# ip\"59.24.3.174\"\n\n\n\n2.11.6.2 UDP (User Datagram Protocol)\nUDP provides no such features as provided by TCP.\nA common use for UDP is in multicast applications.\n\n## receiver\nusing Sockets\n\ngroup = ip\"226.6.8.8\"  # Choose a valid IP address for multicast: for IPv4, the multicast address range is from 224.0.0.0 to 239.255.255.255. Any address within this range is designated for multicast use. For IPv6, the multicast range begins with ff, such as ff05::5:6:7.\nsocket = UDPSocket()  # Open a UDP socket.\nbind(socket, ip\"0.0.0.0\", 6688)  # Bind socket to the given host:port. Note that 0.0.0.0 (IPv4) / :: (IPv6) will listen on all devices (listen on all available network interfaces and all IPv4 / IPv6 addresses associated with the host machine. When binding to a port, make sure that the port number is not in use by another application and that it's not a well-known or registered port that has a specific protocol associated with it.\njoin_multicast_group(socket, group)  # Join a socket to a particular multicast group.\nprintln(String(recv(socket)))  # For recv():  read a UDP packet from the specified socket, and return the bytes received. This call blocks.\n# Hello over IPv4\nleave_multicast_group(socket, group)  #  Remove a socket from a particular multicast group.\nclose(socket)  # Close the socket.\n\n## sender\nusing Sockets\n\ngroup = ip\"226.6.8.8\"\nsocket = UDPSocket()\nsend(socket, group, 6688, \"Hello over IPv4\")  #  Send msg over socket to host:port. It is not necessary for a sender to join the multicast group.\nclose(socket)\n\n\n\n\n\n2.12 Parametric types\nYou can think of the expression S=P{T} as parametric type P taking a type parameter T and returning a new concrete type S. Both T and S are concrete types, while P is just a template for making types.\n\n2.12.1 Defining parametric methods\n\nfunction linearsearch(haystack::AbstractVector{T}, needle::T) where T\n    for (i, x) in enumerate(haystack)\n        if needle == x\n            return i\n        end\n    end\nend\n\nlinearsearch([1, 4, 6, 8], 6)\n\n3\n\n\n\nlinearsearch([1, 4, 6, 8], \"six\")\n\n\nMethodError: no method matching linearsearch(::Vector{Int64}, ::String)\n\nClosest candidates are:\n  linearsearch(::AbstractVector{T}, ::T) where T\n   @ Main In[229]:1\n\n\nStacktrace:\n [1] top-level scope\n   @ In[230]:2\n\n\n\nIn this example, the linearsearch() is a parametric method, which takes a type parameter T, defined in the where T clause. You can define more than one type parameter in the where clause (e.g. where {T, S}).\n\n\n2.12.2 Defining parametric types\n\n\"A point at coordinate (x, y)\"\nstruct Point{T}\n    x::T\n    y::T\nend\n\nPoint\n\n\nYou can impose constraints on the type parameter T with subtype operator &lt;::\n\nstruct RPoint{T&lt;:Number}\n    x::T\n    y::T\nend\n\nWhen creating a point with Point, you can let Julia to infer the type parameter from arguments or explicitly set the type parameter:\n\nPoint(1, 2), Point{Int}(3, 4)\n\n(Point{Int64}(1, 2), Point{Int64}(3, 4))\n\n\nIn fact, sum(xs::Vector) is the same as sum(xs::Vector{T}) where T.\nIn summary, parametric types can improve the type safety (stricter type checking), performance (more type restrictions, less type-related jobs), and memory usage (more type restrictions, more precise assignment of memory).\n\n\n\n2.13 Scope of variables\n\n2.13.1 Introduction\nThe scope of a variable is the region of code within which a variable is accessible. Variable scoping helps avoid variable naming conflicts.\nThere are two main types of scopes in programming languages: lexical scope (also called static scope) and dynamic scope.\nIn languages with lexical scope, the name resolution depends on the location in the source code and the lexical context, where the named variable is defined. In contrast, in languages with dynamic scope, the name resolution depends on the program state and the runtime context when the name is encountered.\nIn a word, with lexical scope a name is resolved by searching the local lexical context, then if that fails, by searching the outer lexical context, and so on; with dynamic scope, a name is resolved by searching the local execution context, then if that fails, by searching the outer execution context, and so on, progressing up the call stack.\nJulia uses lexical scope. Further, there are two main types of scopes in Julia, global scope and local scope. The latter can be nested.\nIn Julia, different constructs may introduce different types of scopes.\n\n\n2.13.2 Scope constructs\nThe constructs introducing scopes are:\n\n\n\n\n\n\n\n\nConstruct\nScope type\nAllowed within\n\n\n\n\nmodule, baremodule\nglobal\nglobal\n\n\nstruct\nsoft local\nglobal\n\n\nfor, while, try\nsoft local\nglobal, local\n\n\nmacro\nhard local\nglobal\n\n\nfunctions, do blocks, let blocks, comprehensions, generators\nhard local\nglobal, local\n\n\n\nNote: begin blocks and if blocks do not introduce scopes.\n\n\n2.13.3 Global scope\nEach module introduces a global scope.\nModules can introduce variables of other modules into their scopes through the using or import statement, or through qualified access using the dot notation.\n\nmodule A\n    a = 1  # a is a global variable in A's scope\nend\n\nmodule B\n    module C\n        c = 2\n    end\n    b = C.c  # can access the namespace of a nested global scope through a qualified access\n\n    import ..A  # makes module A available\n    d = A.a\nend\n\nIf a top-level expression (e.g. a begin or if block) contains a variable declared with keyword local, then that variable is not accessible outside that expression.\n\nx = 1\nbegin\n    local x = 0\n    @show x\nend\n@show x\n\nx = 0\nx = 1\n\n\n1\n\n\nNote: the REPL is in the global scope of the module Main.\n\n\n2.13.4 Local scope\nA local scope nested inside another local/global scope can see variables in all the outer scopes in which it is contained. Outer scopes, on the other hand, cannot see variables in inner scopes.\nWhen x = &lt;value&gt; occurs in a local scope, Julia will apply the following rules to decide what the expression means:\n\nExisting local: if x is already a local variable, then the existing local x is assigned.\nHard scope: if x is not already a local variable and this assignment occurs inside of any hard scope construct, then a new local variable named x is created in the scope of the assignment.\nSoft scope: if x is not already a local variable and all of the scope constructs containing the assignment are soft scopes, the behavior depends on whether the global variable x is defined:\n\nIf global x is undefined, a new local variable named x is created in the scope of the assignment;\nIf global x is defined, then the following rules are applied:\n\nIn interactive mode, the global variable x is assigned;\nIn non-interactive mode, an ambiguity warning is printed and a new local variable named x is created in the scope of the assignment.\n\n\n\nTherefore, in non-interactive mode, the soft scope and hard scope behaves identically except that a warning is printed when an implicitly local variable shadows a global variable in the soft scope.\nNote: in Julia, a variable cannot be a non-local variable, meaning that it is either a local variable or a global variable, which is determined regardless of the order of expressions. As a consequence, if you assign to an existing local, it always updates that existing local; therefore, you can only shadow a local by explicitly declaring a new local in a nested scope with the local keyword.\n\nfunction outer_foo()\n    x = 99  # x is a local variable in the outer_foo's scope\n    @show x\n    let\n        x = 100  # updates the local variable x defined in the outer_foo's scope\n    end\n    @show x\n    return nothing\nend\n\nouter_foo (generic function with 1 method)\n\n\n\ncode = \"\"\"\ns = 0 # global\nfor i = 1:10\n    t = s + i # new local t\n    s = t # new local s with warning\nend\ns, # global; should be 0\n@isdefined(t) # global; should be false\n\"\"\";\n\ninclude_string(Main, code)\n\nLoadError: LoadError: UndefVarError: `s` not defined\nin expression starting at string:2\nLoadError: UndefVarError: `s` not defined\nin expression starting at string:2\n\nStacktrace:\n [1] top-level scope\n   @ ./string:3\n [2] eval\n   @ ./boot.jl:385 [inlined]\n [3] include_string(mapexpr::typeof(identity), mod::Module, code::String, filename::String)\n   @ Base ./loading.jl:2076\n [4] include_string\n   @ ./loading.jl:2086 [inlined]\n [5] include_string(m::Module, txt::String)\n   @ Base ./loading.jl:2086\n [6] top-level scope\n   @ In[236]:12\n\n\n\n\n\n\n\n\nCaution\n\n\n\nSo don’t forget to use the global keyword to declare a variable x if you want to use a global x instead of a local x in seeing a for loop in non-interactive mode:\n\ncode = \"\"\"\ns = 0\nfor i in 1:100\n    global s = s + i\nend\n@show s\n\"\"\"\n\ninclude_string(Main, code)\n\ns = 5050\n\n\n5050\n\n\n\n\n\n\n2.13.5 let blocks\nlet blocks create a new hard local scope and introduce new variable bindings each time they run. The variable need not be immediately assigned. The value evaluated from the last expression is returned.\n\nlet x  # x need not be immediately assigned\n    x = 1\nend\n\n1\n\n\nThe let syntax accepts a comma-separated series of assignments and variable names.\n\nx, y = 1, 2\n\nlet x = x, y = 20\n    @show x, y\nend\n\n(x, y) = (1, 20)\n\n\n(1, 20)\n\n\nNote: in the above example, x = x is possible, since the assignment is evaluated from the right to the left. x in the right-hand side is global; x in the left-hand side is local.\n\n\n2.13.6 Loops\nA for loop iteration variable is always a new local variable; otherwise you declare it using the outer keyword.\n\nfunction for_f1()\n    i = 0\n    for i = 1:3  # i is local\n    end\n    return i\nend\n\nfor_f1()\n\n0\n\n\n\nfunction for_f2()\n    i = 0\n    for outer i = 1:3  # i is global\n    end\n    return i\nend\n\nfor_f2()\n\n3\n\n\nA noteworthy fact is that you must declare i using the global keyword in the following code or an error will be raised when you run it in non-interactive mode:\n\ncode = \"\"\"\n    i = 10\n    while i &lt;= 12\n        i = i + 1  # i is regarded as a local instead of a global since this is determined regardless of the order of expressions\n        @show i\n    end\n    @show i\n\"\"\"\n\ninclude_string(Main, code)\n\nLoadError: LoadError: UndefVarError: `i` not defined\nin expression starting at string:2\nLoadError: UndefVarError: `i` not defined\nin expression starting at string:2\n\nStacktrace:\n [1] top-level scope\n   @ ./string:3\n [2] eval\n   @ ./boot.jl:385 [inlined]\n [3] include_string(mapexpr::typeof(identity), mod::Module, code::String, filename::String)\n   @ Base ./loading.jl:2076\n [4] include_string\n   @ ./loading.jl:2086 [inlined]\n [5] include_string(m::Module, txt::String)\n   @ Base ./loading.jl:2086\n [6] top-level scope\n   @ In[242]:11\n\n\n\ncode = \"\"\"\n    i = 10\n    while i &lt;= 12\n        global i = i + 1  # i is global\n        @show i\n    end\n    @show i\n\"\"\"\n\ninclude_string(Main, code)\n\ni = 11\ni = 12\ni = 13\ni = 13\n\n\n13\n\n\n\n\n2.13.7 Constants\nThe const declaration should only be used in global scope on globals. It is difficult for the compiler to optimize code involving global variables, since their values (or even their types) might change at almost any time. If a global variable will not change, adding a const declaration solves this performance problem.\nLocal constants are quite different. The compiler is able to determine automatically when a local variable is constant, so local constant declarations are not necessary, and in fact are currently not supported.\n\n\n2.13.8 Typed globals\nA global can be declared to always be of a constant type by using the syntax global x::T or upon assignment as x::T = 123.\nOnce a global is declared to be of a constant type, it cannot be assigned to values which cannot be converted to the specified type. In addition, a global has either been assigned to or its type has been set, the binding type is not allowed to change.\n\n\n\n2.14 Parallel computing\n\n2.14.1 Asynchronous tasks\nA task has a create-start-run-finish lifecycle, allowing suspending and resuming computations.\n\nCreate a task by calling the Task constructor on a 0-argument function or using the @task macro: Task(() -&gt; x) is equivalent to @task x.\nStart a task by calling schedule(x) (i.e., add it to an internal queue of tasks).\n\nNote: for convenience, you can use @async x to create and start a task at once (equivalent to schedule(@task x)).\n\nYou can then call wait(x) to wait the task to exit.\n\n\nfunction mysleep(seconds)\n    sleep(seconds)\n    println(\"done\")\nend\n\nt = Task(() -&gt; mysleep(5))  # equivalent to `@task mysleep(5)`\nschedule(t)\nwait(t)\n\ndone\n\n\n\nCommunicating with channels.\n\n\nYou can call the Channel{T}(size) constructor to create a channel with an internal buffer that can hold a maximum of size objects of type T (Channel(0) constructs an unbuffered channel).\nDifferent tasks can write to the same channel concurrently via put!(channel, x) calls.\nDifferent tasks can read data concurrently via take!(channel) (remove and return a value from a channel) or fetch() (return the first available value from a channel without removing) calls.\nIf a channel is empty, readers (on a take!() call) will block until data is available.\nIf a channel is full, writers (on a put!() call) will block until space becomes available.\nYou can use isready(channel) to check for the presence of any object in the channel, and use wait(channel) to wait for an object to become available.\nYou can use close(channel) to close a channel. On a closed channel, put!() will fail, but take!() and fetch() can still successfully return any existing values until it is emptied.\nYou can associate a channel with a task using the Channel(f) constructor (f is a function accepting a single argument of type Channel) or the bind(channel, task) function. This means that the lifecycle of the channel is bound to this task (i.e., you don’t have to close the channel explicitly, while the channel will be closed the moment the task exits). In addition, it will not only log any unexpected failures, but also force the associated resources to close and propagate the exception everywhere. Compared with bind(), errormonitor(task) only prints an error log if task fails.\nThe returned channel can be used as an iterable object in a for loop, in which case the loop variable takes on all the produced values. The loop is terminated when the channel is closed.\n\n\njobs = Channel{Int}(32)\nresults = Channel{Tuple}(32)\n\nfunction do_work()\n    for job_id in jobs\n        exec_time = rand()\n        sleep(exec_time)\n\n        put!(results, (job_id, exec_time))\n    end\nend\n\nfunction make_jobs(n)\n    for i in 1:n\n        put!(jobs, i)\n    end\nend\n\nn = 12\n\nerrormonitor(@async make_jobs(n))\n\nfor i in 1:4  # spawn 4 tasks\n    errormonitor(@async do_work())\nend\n\nsum_time = 0\neval_time = @elapsed while n &gt; 0\n    job_id, exec_time = take!(results)\n    println(\"$job_id finished in $(round(exec_time; digits = 2)) seconds\")\n    global n = n - 1\n    global sum_time = sum_time + exec_time\nend\nprintln(\"The evaluated time is $eval_time seconds\")\nprintln(\"The accumulated time is $sum_time seconds\")\n\n1 finished in 0.22 seconds\n4 finished in 0.35 seconds\n6 finished in 0.2 seconds\n2 finished in 0.67 seconds\n3 finished in 0.97 seconds\n7 finished in 0.45 seconds\n8 finished in 0.39 seconds\n5 finished in 0.87 seconds\n12 finished in 0.04 seconds\n9 finished in 0.48 seconds\n11 finished in 0.61 seconds\n10 finished in 0.77 seconds\nThe evaluated time is 1.874634034 seconds\nThe accumulated time is 6.0185192397190255 seconds\n\n\n\nMore task operations\n\nTask operations are built on a low-level primitive called yieldto(task, value), which suspends the current task, switches to the specified task, and causes that task’s last yieldto() call to return the specified value.\nA few other useful functions of tasks:\n\ncurrent_task(): gets a reference to the currently-running task.\nistaskdone(): queries whether a task has exited.\nistaskstarted(): queries whether a task has run yet.\ntask_local_storage(): manipulates a key-value store specific to the current task.\n\n\n\n2.14.2 Multi-threading\nJulia’s multi-threading, provided by the Threads module, a sub-module of Base, provides the ability to schedule tasks simultaneously on more than one thread or CPU core, sharing memory.\n\n2.14.2.1 Starting Julia with multiple threads\nThe number of execution threads is controlled either by using -t/--threads (julia -t 4) command line argument or by using the JULIA_NUM_THREADS (export JULIA_NUM_THREADS=4, which must be done before starting Julia, and setting it in startup.jl file by using ENV is too late) environment variable. When both are specified, the -t/--threads takes precedence. Both options support the auto argument, which let Julia itself infer a useful default number of threads to use.\nNote: The number of threads specified with -t/--threads is propagated to processes that are spawned using the -p/--procs or --machine-file command line option. For example, julia -p 2 -t 2 spawns 1 main process and 2 worker processes, and all three processes have 2 threads enabled. For more fine grained control over worker threads use addprocs() and pass -t/--threads as exeflags.\nNote: The Garbage Collector (GC) can use multiple threads. You can specify it either by using the --gcthreads command line argument or by using the JULIA_NUM_GC_THREADS environment variable.\nAfter starting Julia with multiple threads, you can check it with the following functions:\n\nThreads.nthreads()\n\n1\n\n\n\nThreads.threadid()\n\n1\n\n\n\n\n2.14.2.2 Thread pools\nThere are two types of thread pools: :interactive (often used for interactive tasks) and :default (often used for long duration tasks).\nYou can set the number of execution threads available for each thread pool of the two by: -t 3,1 or JULIA_NUM_THREADS=3,1, which means that there are 3 threads in the :default thread pool, and 1 thread in the :interactive thread pool. Both numbers can be replaced with the word auto.\nCorresponding helper functions:\n\nusing Base.Threads\n\nprintln(nthreadpools())  # the number of thread pools\n\nprintln(threadpool(1))  # which thread pool the thread 1 belongs to\n\nprintln(nthreads(:default))  # the number of threads available for the :default thread pool\n\n2\ndefault\n1\n\n\n\n\n2.14.2.3 Spawning threads\n\n@spawn: you can specify which thread pool should be used by the spawned thread.\n\n\nThreads.@spawn :interactive begin; println(\"task done\"); end\n\ntask done\n\n\nTask (done) @0x00007f915a615aa0\n\n\n\n@threads: this macro is affixed in front of a for loop to indicate to Julia that the loop is a multi-threaded region.\n\n\na = zeros(10)\n\n# the iteration space is plit among the threads\nThreads.@threads for i = 1:10\n    a[i] = Threads.threadid()\nend\n\na\n\n10-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n\n\nNote: after a task starts running on a certain thread it may move to a different thread although the :static schedule option for @threads does freeze the thread id. This means that in most cases threadid() should not be treated as constant within a task.\n\n\n2.14.2.4 Avoiding data race\nBe very careful about reading any data if another thread might write to it!\nInstead, always use the lock pattern when changing data accessed by other threads.\n\nlk = ReentrantLock()\n\n# method 1\nlock(lk) do \n    use(a)\nend\n\n# method 2\nbegin\n    lock(lk)\n    try\n        use(a)\n    finally\n        unlock(lk)  # each lock must be matched by an unlock\n    end\nend\n\nA toy example:\n\nWithout multi-threading:\n\n\n# the correct result\nfunction sum_single(x)\n    s = 0\n    for i = x\n        s += i\n    end\n    s\nend\n\n@time sum_single(1:1_000_000)  # in Julia, the underscore (_) can be used as a separator in literal integers to enhance readability\n\n  0.000001 seconds\n\n\n500000500000\n\n\n\nData race often leads to non-deterministic results:\n\n\n# with data race and the result is non-deterministic\nfunction sum_multi_bad(x)\n    s = 0\n    Threads.@threads for i = x\n        s += i\n    end\n    s\nend\n\nfor i = 1:6\n    println(sum_multi_bad(1:1_000_000))\nend\n\n500000500000\n500000500000\n500000500000\n500000500000\n500000500000\n500000500000\n\n\n\nAdd lock when performing data race operations:\n\n\n# locked version\n# the result is correct\nlk = ReentrantLock()\n\nfunction sum_multi_lock(x)\n    s = 0\n    Threads.@threads for i = x\n        lock(lk) do\n            s += i\n        end\n    end\n    s\nend\n\nfor i = 1:6\n    println(sum_multi_lock(1:1_000_000))\nend\n\n500000500000\n500000500000\n500000500000\n500000500000\n500000500000\n500000500000\n\n\n\nSplit data into chunks –&gt; use its own internal buffer for each thread –&gt; collect all results of chunks:\n\n\n# split the sum into chunks that are race-free\n# collect the result of each chunk\n# add the results together\nfunction sum_multi_chunk(x)\n    chunks = Iterators.partition(x, length(x) ÷ Threads.nthreads())\n    tasks = map(chunks) do chunk\n        Threads.@spawn sum_single(chunk)\n    end\n    chunk_sums = fetch.(tasks)\n    return sum_single(chunk_sums)\nend\n\n@time sum_multi_chunk(1:1_000_000)\n\n  0.031615 seconds (15.59 k allocations: 1.081 MiB, 99.56% compilation time)\n\n\n500000500000\n\n\n\nUse atomic operations:\n\nJulia supports accessing and modifying values atomically, that is, in a thread-safe way to avoid data race.\nA value (which must be of a primitive type) can be wrapped as Threads.Atomic{T}(value) to indicate it must be accessed in this way.\nIn a word, perform atomic operations on atomic values to avoid data race.\n\nfunction sum_multi_atomic(x)\n    s = Threads.Atomic{Int}(0)  # s is an atomic value of type Int\n    Threads.@threads for i = x\n        Threads.atomic_add!(s, i)  # perform atomic operation atomic_add! (add i to s, and return the old value) on atomic value s\n    end\n    s\nend\n\nres = sum_multi_atomic(1:1_000_000)\nres[]  # Atomic objects can be accessed using the [] notation\n\n500000500000\n\n\n\n\n\n2.14.3 Multi-processing and Distributed computing\nDistributed computing provided by module Distributed runs multiple Julia processes with separate memory spaces.\n\n2.14.3.1 Starting and managing multiple processes\nIn Julia, each process has an associated identifier. The process providing the interactive Julia prompt always has an id equal to 1, called the main process.\nBy default, the processes used for parallel operations are referred to as “workers”. When there is only 1 process, process 1 is considered a worker. Otherwise, workers are considered to be all processes other than process 1. As a result, adding 2 or more processes is required to gain benefits from parallel processing methods. Adding a single process is beneficial if you just wish to do other things in the main process while a long computation is running on the worker.\nJulia has built-in support for two types of clusters:\n\nA local cluster specified with the -p/--procs option (implicitly loads module Distributed).\nA cluster spanning machines using the --machine-file option.\n\nThis uses a passwordless ssh login to start Julia worker processes from the same path as the current host on the specified machines.\nEach machine definition takes the form [count*] [user@]host[:port] [bind_addr[:port]]. count is the number of workers to spawn on the node, and defaults to 1; user defaults to the current user; port defaults to the standard ssh port; bind_addr[:port] specifies the IP address and port that other workers should use to connect to this worker.\nNote: in Julia, distribution of code to worker processes relies on Serialization.serialize (the need for data serialization and deserialization arises primarily due to the requirement to convert complex data structures into formats that can be transmitted across a network when different nodes communicate with each other), so it is advised that all workers on all machines use the same version of Julia to ensure compatibility of serialization and deserialization.\nDistributed package provides some useful functions for starting and managing processes within Julia:\n\nusing Distributed  # Module Distributed must be explicitly loaded on the master process before invoking addprocs() and other functions if you want to start distributed computing within Julia, instead of using command line options. It is automaticaly made available on the worker processes.\n\naddprocs()  # launch worker processes using the LocalManager (the same as -p), SSHManager (the same as --machine-file) or other cluster managers of type ClusterManager\n\nprocs()  # return a list of all process identifiers\n\nnprocs()  # return the number of available processes\n\nworkers()\n\nnworkers()\n\nmyid()  # get the id of the current process\n\nNote: workers do not run a ~/.julia/config/startup.jl startup script, nor do they synchronize their global state with any of the other running processes. You may use addprocs(exeflags = \"--project\") to initialize a worker with a particular environment.\n\n\n\n\n\n\nNetwork requirements for LocalManager and SSHManager\n\n\n\n\nThe master process does not listen on any port. It only connects out to the workers.\nEach worker binds to only one of the local interfaces and listens on an ephemeral port number assigned by the OS.\nLocalManager, used by addprocs(N), by default binds only to the loopback interface. An addprocs(4) followed by an addprocs([\"remote_host\"]) will fail. To create a cluster comprising their local system and a few remote systems, it can be done by explicitly requesting LocalManager to bind to an external network interface via restrict keyword argument: addprocs(4; restrict = false).\nSSHManager, used by addprocs(list_of_remote_hosts), launches workers on remote hosts via SSH. By default SSH is only used to launch Julia workers. Subsequent master-worker and worker-worker connections use plain, unencrypted TCP/IP sockets. The remote hosts must have passwordless login enabled. Additional SSH flags or credentials may be specified via keyword argument sshflags.\n\n\n\n\n\n\n\n\n\nCluster cookie\n\n\n\nAll processes in a cluster share the same cookie which, by default, is a randomly generated string on the master process, and can be accessed via cluster_cookie(), while cluster_cookie(cookie) sets it and returns the new cookie. It can also be passed to the workers at startup via --worker=&lt;cookie&gt;.\n\n\n\n\n\n\n\n\nSpecifying network topology\n\n\n\nThe keyword argument topology to addprocs() is used to specify how the workers must be connected to each other. The default is :all_to_all, meaning that all workers are connected to each other.\n\n\n\n\n2.14.3.2 Starting distributed programming\nDistributed programming in Julia is built on two primitives:\n\nRemote references: a remote reference is an object of type RemoteChannel that can be used from any process to refer to an object stored on a particular process. Multiple processes can communicate via RemoteChannel.\nRemote calls: a remote call is a request by one process to call a certain function on certain arguments on another (possibly the same) process. A remote call returns a Future object to its result. Then you can use wait() to wait the function running to finish or use fetch() to get the returned value by the called function.\n\nLaunch remote calls:\n\n@spawn p expr  # Create a closure around an expression and run the closure asynchronously on process p. If p is set to :any, then the system will pick a process to use automatically.\n@fetchfrom p expr  # equivalent to fetch(@spawnat p expr)\n\nremotecall(f, pid, ...)  # Call a function f asynchronously on the given arguments ... on the specified process pid.\nremotecall(f, pool, ...)  # Give a pool of type WorkerPool instead of a pid. It will wait for and take a free worker from pool to use.\nremotecall_fetch()  # equivalent to fetch(remotecall())\nremote_do(f, id, ...)  # Run f on worker id asynchronously. Unlike remotecall, it does not store the result of computation, nor is there a way to wait for its completion.\n\n\nusing Distributed\n\naddprocs(2)  # add 2 wrokers via LocalManager\n\nr = remotecall(rand, 2, 3, 3)  # run rand(3, 3) on process 2\ns = @spawnat 2 1 .+ fetch(r)  # run expr 1 .+ fetch(r) on process 2 (note: this forms a closure () -&gt; 1 .+ fetch(r) which contains the global variable r)\nfetch(s)\n\n3×3 Matrix{Float64}:\n 1.5031   1.409    1.48658\n 1.61592  1.6915   1.6822\n 1.22917  1.57784  1.43686\n\n\nNote: once fetched, a Future will cache its value locally. Further fetch() calls don not entail a network hop. Once all referencing Futures have fetched, the remote stored value is deleted.\n\n\n2.14.3.3 Code and data availability\nBefore spawning a process, you must ensure that your code and data are available on any process that runs it.\n\nCode availability\n\n\nfunction rand2(dims...)\n    return 2 * rand(dims...)\nend\n\nrand2(2, 2)\n\n2×2 Matrix{Float64}:\n 1.95961  1.90016\n 0.80429  1.84709\n\n\n\nusing Distributed\n\naddprocs(2)\n\n\n# rand2 is defined in the main process\n# so process 1 knew it but the others did not\nfetch(@spawnat :any rand2(2, 2))\n\n\nOn worker 2:\nUndefVarError: `#rand2` not defined\nStacktrace:\n  [1] deserialize_datatype\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:1399\n  [2] handle_deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:867\n  [3] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:814\n  [4] handle_deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:874\n  [5] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:814 [inlined]\n  [6] deserialize_global_from_main\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/clusterserialize.jl:160\n  [7] #5\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/clusterserialize.jl:72 [inlined]\n  [8] foreach\n    @ ./abstractarray.jl:3097\n  [9] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/clusterserialize.jl:72\n [10] handle_deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:960\n [11] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:814\n [12] handle_deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:871\n [13] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:814\n [14] handle_deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:874\n [15] deserialize\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Serialization/src/Serialization.jl:814 [inlined]\n [16] deserialize_msg\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/messages.jl:87\n [17] #invokelatest#2\n    @ ./essentials.jl:892 [inlined]\n [18] invokelatest\n    @ ./essentials.jl:889 [inlined]\n [19] message_handler_loop\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:176\n [20] process_tcp_streams\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:133\n [21] #103\n    @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/process_messages.jl:121\n\nStacktrace:\n [1] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID; kwargs::@Kwargs{})\n   @ Distributed ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:465\n [2] remotecall_fetch(f::Function, w::Distributed.Worker, args::Distributed.RRID)\n   @ Distributed ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:454\n [3] remotecall_fetch\n   @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:492 [inlined]\n [4] call_on_owner\n   @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:565 [inlined]\n [5] fetch(r::Future)\n   @ Distributed ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/remotecall.jl:619\n [6] top-level scope\n   @ ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/macros.jl:95\n\n\n\nNote: more commonly you’ll be loading code from files or packages, and you’ll have a considerable amount of flexibility in controlling which processes load code. So if you have defined some functions, types, etc., you’d better organize them into files or packages, which will make things easier.\nConsider a file, DummyModule.jl, containing the following code:\n\nmodule DummyModule\nexport MyType, f\n\nmutable struct MyType\n    a::Int\nend\n\nf(x) = x^2 + 1\n\nprintln(\"DummyModule loaded\")\n\nend\n\nIn order to refer to the code defined in DummyModule.jl across all processes, first, DummyModule.jl needs to be loaded on every process. Calling include(\"DummyModule.jl\") loads it only on a single process. To load it on every process, use the @everywhere [procs()] expr macro, which execute an expression under Main on all procs:\n\n@everywhere include(\"DummyModule.jl\")\n@everywhere using InteractiveUtils\n\n@fetchfrom 2 InteractiveUtils.varinfo()  # show exported global variables in a module\n\nDummyModule loaded\n      From worker 2:    DummyModule loaded\n      From worker 3:    DummyModule loaded\n\n\n\n\n\nname\nsize\nsummary\n\n\n\n\nBase\n\nModule\n\n\nCore\n\nModule\n\n\nDistributed\n1.130 MiB\nModule\n\n\nDummyModule\n266.719 KiB\nModule\n\n\nMain\n\nModule\n\n\nr\n256 bytes\nFuture\n\n\n\n\n\nOnce loaded, we can use code defined in DummyModule.jl across all processes by:\n\n@everywhere using .DummyModule\n\n@fetchfrom 2 f(100)\n\n      From worker 2:    ┌ Warning: Cannot transfer global variable f; it already has a value.\n      From worker 2:    └ @ Distributed ~/softs/julia-1.10.4/share/julia/stdlib/v1.10/Distributed/src/clusterserialize.jl:166\n\n\n10001\n\n\nNote: a file can be preloaded on multiple processes at startup with the -L flag, and a driver script can be used to drive the computation: julia -p &lt;n&gt; -L file1.jl -L file2.jl driver.jl. The Julia process running the driver script given here has an id equal to 1, just like a process providing an interactive prompt.\nIf DummyModule.jl is a package, just use @everywhere using DummyModule, which will make code defined in DummyModule.jl available in every process.\n\nData availability\n\nSending messages and moving data constitute most of the overhead in a distributed program.\nReducing the number of messages and the amount of data sent is critical to achieving performance and scalability.\n\nGlobal variables\nExpressions executed remotely via @spawnat, or closures specified for remote execution using remotecall() may refer to global variables.\nRemote calls with embedded global references (under Main module only) manage globals as follows:\n\nNew global bindings are created on destination workers if they are referenced as part of remote call.\nGlobal constants are declared as constants on remote nodes too.\nGlobals are re-sent to a destination worker only in the context of a remote call, and only if its value has changed.\nThe cluster does not synchronize global bindings across nodes.\n\n\nNote: memory associated with globals may be collected when they are reassigned on the master, while no such action is taken on the workers as the bindings continue to be valid. clear!() can be used to manually reassign specific globals on remote nodes to nothing once they are no longer required.\nOnly when remote calls refer to globals under the Main module are new global bindings created on destination workers, so we can use let blocks to localize global variables when forming closures. This avoids new global bindings’ creating on destination workers:\n\nA = rand(10, 10)\nremotecall_fetch(() -&gt; A, 2)  # A is a global variable under the Main module, so new global binding of A will be created on process 2\n\nB = rand(10, 10)\nlet B = B  # B becomes a local variable, so B won't be created on process 2\n    remotecall_fetch(() -&gt; B, 2)\nend\n@fetchfrom 2 InteractiveUtils.varinfo()\n\n\n\n\nname\nsize\nsummary\n\n\n\n\nA\n840 bytes\n10×10 Matrix{Float64}\n\n\nBase\n\nModule\n\n\nCore\n\nModule\n\n\nDistributed\n1.141 MiB\nModule\n\n\nDummyModule\n267.550 KiB\nModule\n\n\nMain\n\nModule\n\n\nr\n256 bytes\nFuture\n\n\n\n\n\n\nCommunicating with RemoteChannels\nCreate references to remote channels with the following:\n\nRemoteChannel(f, pid)  # Create references to remote channels of a specific size and type. f is a function that when executed on pid (the default is the current process) must return an implementation of an AbstractChannel. e.g., RemoteChannel(() -&gt; Channel{Int}(10), pid).\nRemoteChannel(pid)  # make a reference to a Channel{Any}(1) on process pid\n\n\nA Channel is local to a process, but a RemoteChannel can put and take values across workers.\nA RemoteChannel can be thought of as a handle to a Channel.\nThe process id, pid, associated with a RemoteChannel identifies the process where the backing store, i.e., the backing Channel exists.\nAny process with a reference to a RemoteChannel can put and take items from the channel. Data is automatically sent to or retrieved from the process a RemoteChannel is associated with.\nSerializing a Channel also serializes any data present in the channel. Deserializing it therefore effectively makes a copy of the original object.\nOn the other hand, serializing a RemoteChannel only involves the serialization of an identifier that identifies the location and instance of Channel referred to by the handle. A deserialized RemoteChannel object on any worker, therefore also points to the same backing store as the original.\n\n\n\njobs = RemoteChannel(() -&gt; Channel{Int}(32))\nresults = RemoteChannel(() -&gt; Channel{Tuple}(32))\n\n@everywhere function do_work(jobs, results)  # define work function everywhere\n    while true\n        job_id = take!(jobs)\n        exec_time = rand()\n        sleep(exec_time)  # simulate elpased time doing actual work\n        put!(results, (job_id, exec_time, myid()))\n    end\nend\n\nfunction make_jobs(n)\n    for i in 1:n\n        put!(jobs, i)\n    end\nend\n\nn = 12\n\nerrormonitor(@async make_jobs(n))\n\nfor p in workers()\n    remote_do(do_work, p, jobs, results)\nend\n\n@elapsed while n &gt; 0\n    job_id, exec_time, where = take!(results)\n    println(\"$job_id finished in $(round(exec_time; digits = 2)) seconds on worker $where\")\n    global n = n - 1\nend\n\n1 finished in 0.36 seconds on worker 2\n2 finished in 0.49 seconds on worker 3\n3 finished in 0.61 seconds on worker 2\n4 finished in 0.46 seconds on worker 3\n5 finished in 0.21 seconds on worker 2\n6 finished in 0.46 seconds on worker 3\n7 finished in 0.63 seconds on worker 2\n8 finished in 0.54 seconds on worker 3\n9 finished in 0.73 seconds on worker 2\n10 finished in 0.78 seconds on worker 3\n12 finished in 0.47 seconds on worker 3\n11 finished in 0.77 seconds on worker 2\n\n\n3.646881956\n\n\n\n\n\n\n\n\nLocal invocations\n\n\n\nWhen data is stored on a different node from the execution node, data is necessarily copied over to the remote node for execution. However, when the destination node is the local node, i.e., the calling process id is the same as the remote node id, it is executed as a local call. It is usually (not always) executed in a different task, but there is no serialization/deserialization of data. Consequently, the call refers to the same object instances as passed, i.e., no copies are created.\n\nrc = RemoteChannel(() -&gt; Channel(3))  # RemoteChannel created on local node\n\nv = [0]  # array in Julia has stable memory address\n\nfor i in 1:3\n    v[1] = i  # reusing v\n    put!(rc, v)\nend\n\nres = [take!(rc) for _ in 1:3]\n\nprintln(res)\n\nprintln(map(objectid, res))\n\nprintln(\"Num unique obejcts: \", length(unique(map(objectid, res))))\n\n[[3], [3], [3]]\nUInt64[0xb5f36520df483d44, 0xb5f36520df483d44, 0xb5f36520df483d44]\nNum unique obejcts: 1\n\n\nIn general, this is not an issue. If the local node is also being used as a compute node, and the arguments used post the call, this behavior needs to be factored in and if required deep copies of arguments.\n\n\n\nShared arrays\n\nShared arrays use system shared memory to map the same array across many processes.\nEach “participating” process has access to the entire array, which is totally different from the DArray defined in DistributedArrays.jl, of which each process has local access to just a chunk (i.e., no two processes share the same chunk).\nA SharedArray defined in SharedArrays module is a good choice when you want to have a large amount of data jointly accessible to two or more processes on the same machine.\nIn cases where an algorithm insists on an Array input, the underlying array can be retrieved from a SharedArray by calling sdata(). For other AbstractArray types, sdata() just returns the object itself.\nThe constructor for a shared array is of the form: SharedArray{T, N}(dims::NTuple; init=false, pids=Int[]), by which we can construct an N-dimensional shared array of a bits type (check whether an element is supported using isbits()) T and size dims across the processes specified by pids. If an initialization function of the form f(S::SharedArray) is passed to init, then it is called on all the participating workers. You can specify that each worker runs the init function on a distinct portion of the array, thereby parallelizing initialization.\n\n@everywhere using SharedArrays\n\nS = SharedArray{Int, 2}((3, 4), init = S -&gt; S[localindices(S)] = repeat([myid()], length(localindices(S))))\n\n# localindices(S): return a range describing the \"default\" indices to be handled by the current process.\n# indexpids(S): return the current worker's index (starting from 1, not the same as the actual pid) in the list of workers mapping the SharedArray, or 0 if the SharedArray is not mapped onto the current process.\n# procs(S): return the list of pids mapping the SharedArray.\n\n3×4 SharedMatrix{Int64}:\n 2  2  3  3\n 2  2  3  3\n 2  2  3  3\n\n\nNote: because any process mapping the SharedArray has access to the entire array, you must take consideration on possible operation conflicts.\n\n\n2.14.3.4 Parallel loops and map\n\nLooping and then reducing\n\nMany iterations run independently over several processes, and then their results are combined using some function (the result of each iteration is taken as the value of the last expression inside the loop) . The combination process is called a reduction. In code, this typically looks like the pattern x = f(x, v[i]), where x is the accumulator, f is the reduction function, and v[i] are the elements being reduced. It is desirable for f to be associative, so that it does not matter what order the operations are performed in.\n\n# When reducer is given, it will be blocked and return the final result of reduction process.\n# @distributed [reducer] for var = range\n#     body\n# end\n\n# reducer is optional.\n# If it is omitted, then it will return a Task object immediately without waiting for completion.\n# You can prefix @sync or add wait(t) or fetch(t) (returns nothing) after it to wait for completion.\n # @sync @distributed for var = range\n #    body\n # end\n\nres = @distributed (vcat) for i in 1:6\n    [(myid(), i)]\nend\n\nres\n\n6-element Vector{Tuple{Int64, Int64}}:\n (2, 1)\n (2, 2)\n (2, 3)\n (3, 4)\n (3, 5)\n (3, 6)\n\n\n\nMapping\n\nIf we merely want to apply a function to all elements in some collection, we can use parallelized map, implemented in Julia as the pmap() function.\n\nusing LinearAlgebra\n\nM = Matrix{Float64}[rand(1000, 1000) for _ in 1:10]\npmap(svdvals, M)  # calculate the singular values of several matrices in parallel\n\n10-element Vector{Vector{Float64}}:\n [500.0177759176757, 18.06421174287783, 17.981227621452373, 17.92332726630117, 17.838731068240488, 17.753420331994597, 17.71168733528711, 17.65476735816585, 17.612128360065476, 17.60771882704177  …  0.13840101616328937, 0.13193911862258584, 0.119357747880438, 0.09113289171473231, 0.08174833937081102, 0.06222010852072497, 0.049038422635441, 0.044561563193161305, 0.032449963316072604, 0.004270513230201547]\n [500.30544074594235, 18.208593500785533, 18.065289425851613, 17.933575561586757, 17.876499153629222, 17.79394429110242, 17.743295558948113, 17.71173784722835, 17.636519899933216, 17.568455695453785  …  0.13909473497550548, 0.1258594448249676, 0.10669500571693123, 0.08732721226257344, 0.08142060317423437, 0.0788981154402882, 0.05107719237438204, 0.031088180053545786, 0.026451363097944342, 0.004063611399161055]\n [500.2383144234097, 18.20885652402711, 18.07387962906993, 17.941728535882334, 17.86677133986938, 17.785710542272103, 17.739979733947795, 17.687792755541416, 17.655984575634626, 17.587119441580146  …  0.1451055539983298, 0.11358291164602549, 0.09018720630023444, 0.07432213468827491, 0.0634179113616737, 0.05692325777032984, 0.046625536277500206, 0.041853196581529445, 0.018178386645154875, 0.005094318045634688]\n [500.08794972395094, 18.211169090921466, 18.01872349533582, 17.88317231483914, 17.831103953157847, 17.788623584914873, 17.69556377892961, 17.64227565387745, 17.562609974342866, 17.530957120518234  …  0.1299667395269755, 0.12605384263694233, 0.1199364272934578, 0.0907819888244224, 0.08444632611907225, 0.0645911259983415, 0.04500889195905585, 0.03609595929475514, 0.021457892995438033, 3.911322873167237e-5]\n [500.39799136478507, 18.188496528811577, 17.93885712318831, 17.91617495698291, 17.87925556993448, 17.804217993958584, 17.759407936966824, 17.68514205593617, 17.572076461263453, 17.486672912061415  …  0.1417312756246332, 0.13230348206024667, 0.10808243480402457, 0.09433543844715478, 0.08511001643180341, 0.07146686489631734, 0.05385226061060048, 0.04661522077498004, 0.03626662998958416, 0.002823350266137398]\n [500.0464917909289, 18.113149192600773, 17.97923538558104, 17.892593281819696, 17.88070110324045, 17.826539102019595, 17.795053182243407, 17.709390365345474, 17.636474841339222, 17.57943201808123  …  0.12713221472491243, 0.12043499286655325, 0.11892362431840613, 0.10574598850361529, 0.07040897674634287, 0.06203266394157855, 0.02891899925283343, 0.02324572333043797, 0.008048687735659933, 0.004048970364503665]\n [500.20792958963085, 18.263404607762315, 17.988461798078323, 17.94151044591021, 17.906683661996663, 17.799256070999544, 17.691587113039837, 17.638905291137, 17.592113448939926, 17.524114072141707  …  0.11865220351167223, 0.11324976639200965, 0.09827556395803742, 0.07991930837783091, 0.07332384654639487, 0.06128267369804715, 0.04588312454628313, 0.03396766771447287, 0.016349936424052768, 0.011199176040884285]\n [500.2391541923157, 18.19140580852207, 18.073220181904198, 17.999214486451805, 17.93329076587103, 17.836893095606627, 17.74171259073714, 17.699132546313077, 17.69173191902944, 17.622034367356633  …  0.1290446371721021, 0.11873486270170926, 0.09776060243126537, 0.08638887552109367, 0.06533047631062908, 0.052847250293200455, 0.04289225912421125, 0.02916876608337244, 0.003817842295596054, 0.0007402282451418853]\n [500.402623977743, 18.214065294826792, 18.048992153116778, 17.894189262106632, 17.826211143515497, 17.772187539583324, 17.728088360235752, 17.679718584941128, 17.639346999834785, 17.591389840924748  …  0.13196716329141053, 0.1225266613074704, 0.11380031492083569, 0.10058478965749333, 0.09556531974737391, 0.06989544785771873, 0.043063064029072784, 0.03170637137963487, 0.012399855214929882, 0.0056719089695850345]\n [500.35870301845523, 18.269334272611584, 18.158536673951332, 18.031058325685493, 17.919853937318564, 17.82302393607198, 17.785346597746784, 17.719313041097486, 17.676155655930508, 17.616267744093314  …  0.13204201738915136, 0.12738071898037176, 0.1105672679705761, 0.0868965269591531, 0.07557966901028731, 0.06625255459036215, 0.04305752033514217, 0.025735748362021374, 0.014438080587197949, 0.006864933414932486]\n\n\n\n\n2.14.3.5 Noteworthy external parallel packages\nThere are also other packages implementing parallelism or providing data structures suitable for parallelism in Julia.\nIn addition, we have also several packages used for GPU programming in Julia.\n\n\n\n2.14.4 Running external programs\n\n2.14.4.1 Creating Cmd objects\nThere are two ways to create a Cmd objects:\n\nPut the command between backticks (`):\n\n\n`echo hello, world`\n\n\n`echo hello, world`\n\n\n\n\nUse Cmd() constructor:\n\n\nCmd(`echo hello, world`)  # from an existing Cmd\nCmd([\"echo\", \"hello, world\"])  # from a list of arguments\n\n\n`echo 'hello, world'`\n\n\n\nKeyword arguments of Cmd() allow you to specify several aspects of the Cmd’s execution environment.\nFor example, you can specify a working directory for the command via dir, setting execution environment variables via env, which can also be set by two helper functions setenv() and addenv().\n\n\n2.14.4.2 Running Cmd objects\nThe command is never run with a shell. Instead, Julia will do all of the following processes itself. In fact, the command is run as Julia’s immediate child process, using folk and exec calls.\nJulia provides several ways to run a Cmd object:\n\nrun():\n\n\nrun(`echo hello, world`)\n\nhello, world\n\n\n\nProcess(`echo hello, world`, ProcessExited(0))\n\n\n\n\nread():\n\n\nread(`echo hello, world`, String)  # run the command and return the resulting output as a `String`, or as an array of bytes if `String` is omitted\n\n\"hello, world\\n\"\n\n\nAs can be seen, the resulting string has a single trailing newline. You can use readchomp(), equivalent to chomp(read(x, String)) to remove it (chomp() can be used to remove a single trailing newline from a string).\n\nUse open() to read from or write to an external command:\n\n\n# writes go to the command's standard input (stdio = stdout)\nopen(`sort -n`, \"w\", stdout) do io\n    for i = 6:-1:1\n        println(io, i)\n    end\nend\n\n# reads from the command's standard output (stdio = stdin)\nopen(`echo \"hello, world\"`, \"r\", stdin) do io\n    readchomp(io)\nend\n\n1\n2\n3\n4\n5\n6\n\n\n\"hello, world\"\n\n\nNote: the program name and individual arguments in a command can be accessed and iterated over as if the command were an array of strings:\n\ncollect(`cut -f 1,3,5 test.txt`)\n\n4-element Vector{String}:\n \"cut\"\n \"-f\"\n \"1,3,5\"\n \"test.txt\"\n\n\n\n`cut -f 1,3,5 test.txt`[2]\n\n\"-f\"\n\n\n\n\n2.14.4.3 Command interpolation\nYou can use $ for interpolation much as you would in a string literal, and Julia will know when the inserted string needs to be quoted:\n\npath = \"/Volumes/External HD\"\nname = \"data\"\next = \"csv\"\n`sort $path/$name.$ext`  # due to the command is never interpreted by a shell, there's no need for actual quoting, which is only for presentation to the user\n\n\n`sort '/Volumes/External HD/data.csv'`\n\n\n\nIf you want to interpolate multiple words, just using an iterable container:\n\nfiles = [\"/etc/passwd\", \"/Volumes/External HD/data.csv\"]\n`grep foo $files`\n\n\n`grep foo /etc/passwd '/Volumes/External HD/data.csv'`\n\n\n\nIf you interpolate an array as part of a shell word, the shell’s Cartesian product generation is simulated:\n\nnames = [\"foo\", \"bar\", \"baz\"]\n`cat $names.txt`\n\n\n`cat foo.txt bar.txt baz.txt`\n\n\n\nSince you can interpolate literal arrays, no need to create temporary array objects first:\n\n`cat $[\"foo\", \"bar\"].$[\"png\", \"jpeg\"]`\n\n\n`cat foo.png foo.jpeg bar.png bar.jpeg`\n\n\n\n\n\n2.14.4.4 Quoting\nIf you just want to treat some special characters as is, then quote it with paired single quotes '', or quote it with paired double quotes \"\", which means that all characters within paired single quotes will have no special meanings, but some may have within paired double quotes:\n\n`cat '$file'`\n\n\n`cat '$file'`\n\n\n\n\nfile = \"text.txt\"\n`cat \"$file\"`\n\n\n`cat text.txt`\n\n\n\nAs can be seen, this mechanism used here is the same one as is used in shell, so you can just copy and paste a valid shell commands into here, and it will works properly.\n\n\n2.14.4.5 Pipelines\nShell metacharacters, such as |, &, and &gt;, need to be quoted (or escaped) inside of Julia’s backticks:\n\nrun(`echo hello \\| sort`)  # here, | is not a pipe, just a normal character\n\nhello | sort\n\n\n\nProcess(`echo hello '|' sort`, ProcessExited(0))\n\n\n\n\nUse pipeline() to construct a pipe:\n\n\nrun(pipeline(`cut -d : -f 3 /etc/passwd`, `head -n 6`, `sort -n`))\n\n0\n1\n2\n3\n4\n5\n\n\n\nBase.ProcessChain(Base.Process[Process(`cut -d : -f 3 /etc/passwd`, ProcessExited(0)), Process(`head -n 6`, ProcessExited(0)), Process(`sort -n`, ProcessExited(0))], Base.DevNull(), Base.DevNull(), Base.DevNull())\n\n\n\n\nRun multiple commands in parallel using &:\n\n\nrun(`echo hello` & `echo world` & `echo Tom`)  # the order of the output here is non-deterministic\n\nhello\nworld\nTom\n\n\n\nBase.ProcessChain(Base.Process[Process(`echo hello`, ProcessExited(0)), Process(`echo world`, ProcessExited(0)), Process(`echo Tom`, ProcessExited(0))], Base.DevNull(), Base.DevNull(), Base.DevNull())\n\n\n\nCombine both | and &:\n\nrun(pipeline(`echo world` & `echo hello`, `sort`))  # a single UNIX pipe is created and written to by both echo processes, and the other end of the pipe is read from by the sort command\n\nhello\nworld\n\n\n\nBase.ProcessChain(Base.Process[Process(`echo world`, ProcessExited(0)), Process(`echo hello`, ProcessExited(0)), Process(`sort`, ProcessExited(0))], Base.DevNull(), Base.DevNull(), Base.DevNull())\n\n\n\n\nproducer() = `awk 'BEGIN{for (i = 0; i &lt;= 6; i++) {print i; system(\"sleep 1\")}}'`\nconsumer(flag) = `awk '{print \"'$flag' \"$1; system(\"sleep 2\")}'`  # to make the interpolation $flag work, you have to put it between single quotes\nrun(pipeline(producer(), consumer(\"A\") & consumer(\"B\") & consumer(\"C\")))\n\nB 0\nA 1\nC 2\nB 3\nA 4\nC 5\nB 6\n\n\n\nBase.ProcessChain(Base.Process[Process(`awk 'BEGIN{for (i = 0; i &lt;= 6; i++) {print i; system(\"sleep 1\")}}'`, ProcessExited(0)), Process(`awk '{print \"A \"$1; system(\"sleep 2\")}'`, ProcessExited(0)), Process(`awk '{print \"B \"$1; system(\"sleep 2\")}'`, ProcessExited(0)), Process(`awk '{print \"C \"$1; system(\"sleep 2\")}'`, ProcessExited(0))], Base.DevNull(), Base.DevNull(), Base.DevNull())"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#julia-documentation-system",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#julia-documentation-system",
    "title": "Julia syntax basics",
    "section": "3 Julia documentation system",
    "text": "3 Julia documentation system\n\n\"Store propellant for a rocket\"\nabstract type OhTank end\n\n\"\"\"\n    total(t::OhTank) -&gt; Float64\n\nMass of propellant tank `t` when it is full.\n\"\"\"\nfunction totalmass end\n\ntotalmass\n\n\nThe Julia documentation system works by prefixing a function or type definition with a regular Julia text string, quoted by double or triple quotes. This is totally different from a comment with the # symbol. Comments don’t get stored in the Julia help system.\nInside this text string, you can document your function or type definition using markdown syntax."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#modules-and-pakcages",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#modules-and-pakcages",
    "title": "Julia syntax basics",
    "section": "4 Modules and Pakcages",
    "text": "4 Modules and Pakcages\nThe core Julia language imposes very little; many functions are extended by modules and packages.\nJulia code is organized into files, modules, and packages. Files containing Julia code use the .jl file extension.\n\n4.1 Modules\nModules help organize code into coherent units. They are delimited syntactically inside module &lt;NameOfModule&gt; ... end, and have the following features:\n\nModules are separate namespaces, each introducing a new global scope. This allows the same name to be used for different functions or global variables without conflict, as long as they are in separate modules.\nModules have facilities for detailed namespace management: each defines a set of names it exports, and can import names from other modules with using and import.\nModules can be precompiled for faster loading, and may contain code for runtime initialization.\n\nModule definition:\n\nmodule &lt;NameOfModule&gt;\n\n# using, import, export statements are usually here\n\ninclude(\"file1.jl\")\ninclude(\"file2.jl\")\n\nend\n\n\n\n\n\n\n\nNote\n\n\n\n\nFiles and file names are mostly unrelated to modules, since modules are associated only with module expression. One can have multiple files per module, and multiple modules per file.\ninclude behaves as if the contents of the source file were evaluated in the global scope of the including module.\nThe recommended style is not to indent the body of the module. It is also common to use UpperCamelCase for module names, and use the plural form if applicable.\n\n\n\n\n4.1.1 Namespace management\nNamespace management refers to the facilities the language offers for making names in a module available in other modules.\n\n4.1.1.1 Qualified names\nNames for functions, variables, and types in the global scope always belong to a module, called the parent module. One can use parentmodule() to find the parent module of a name.\nOne can also refer to those names outside their parent module by prefixing them with their module name, e.g. Base.UnitRange. This is called a qualified name.\nThe parent module may be accessible using a chain of submodules like Base.Math.sin, where Base.Main is called the module path.\nDue to syntactic ambiguities, qualifying a name that contains only symbols, such as an operator, requires inserting a colon, e.g. Base.:+. A small number of operators additionally require parentheses, e.g. Base.:(==).\n\n\n4.1.1.2 Export lists\nNames can be added to the export list of a module with export: these are symbols that are imported when using the module.\n\nmodule NiceStuff\n\nexport nice, DOG\n\n# definitions of nice and DOG\n\nend\n\nIn fact, a module can have multiple export statements in arbitrary locations.\n\n\n4.1.1.3 using and import\n\nusing: brings the module name and the elements of the export list into the surrounding global namespace.\nimport: brings only the module name into scope.\n\n\n\n\n\n\n\nNote\n\n\n\n\nTo load a module from a locally defined module, a dot needs to be added before the module name like using .ModuleName.\nOne can specify which identifiers to be loaded in a module, e.g., using .NiceStuff: nice, DOG.\nRenaming imported identifiers with as.\n\n\nimport CSV as C  # This only works with import\nimport CSV: read as rd\nusing CSV: read as rd\n\n\n\n\n\n\n4.1.2 How does Julia find a module\n\nJulia looks for module files in directories defined in the LOAD_PATH variable:\n\n\nLOAD_PATH\n\n3-element Vector{String}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n\n\n\nTo make it look in other places, add some more using push!():\n\n\npush!(LOAD_PATH, \"/path/to/my/julia/projects\")\n\n4-element Vector{String}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n \"/path/to/my/julia/projects\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo avoid doing this every time you run Julia, put this line into your startup file ~/.julia/config/startup.jl, which runs each time you start an interactive Julia session.\n\n\n\nJulia looks for files in those directories in the form of a package with the structure: ModuleName/src/file.jl.\nOr, if not in package form, it will look for a filename that matches the name of your module.\n\n\n\n\n4.2 Standard modules\nThere are three most important modules:\n\nCore\n\nCore contains all identifiers considered “built in” to the language, i.e. part of the core language and not libraries.\nEevery module implicitly specifies using Core, since you cannot do anything without these definitions.\n\nBase\n\nBase contains basic functionality.\nAll modules implicitly contain using Base.\n\nMain\n\nMain is the top-level module, and Julia starts with Main set as the current module.\nVariables defined at the prompt go in Main, and varinfo() lists variables in Main.\n\n\n4.3 Packages\nJulia uses git for organizing and controlling packages.\nBy convention, all packages are stored in git repositories.\n\n\n4.4 Organizing your code into modules and packages\n\n4.4.1 Setting up your working environment\nIn Julia, different environments can have totally different packages and versions installed from another environment.\nThis makes it possible that you can construct an environment tailored to your project, which makes your project completely reproducible.\n\n## Make the job directory in the shell mode\nshell&gt; mkdir job\n\n## Activate the job environment in the package mode\n(@v1.10) pkg&gt; activate job\n  Activating new project at `~/temp/job`\n\n## Add packages into the job environment\n(job) pkg&gt; add CairoMakie ElectronDisplay\n\n## Check what packages are added into the job environment\n(job) pkg&gt; status\nStatus `~/temp/job/Project.toml`\n  [13f3f980] CairoMakie v0.11.5\n  [d872a56f] ElectronDisplay v1.0.1\n\n## Julia adds packages into the job environment by adding information of packages into the following two files of the job environment:\n# 1. Project.toml: specifies what packages are added to this environment\n\nshell&gt; cat Project.toml\n[deps]\nCairoMakie = \"13f3f980-e62b-5c42-98c6-ff1f3baf88f0\"  # The string is the universally unique identifier (UUID) of the CairoMakie package, which allows you to install different packages with the same package name. If there was another CairoMakie package, you should add this one with the command: add CairoMakie=13f3f980-e62b-5c42-98c6-ff1f3baf88f0\nElectronDisplay = \"d872a56f-244b-5cc9-b574-2017b5b909a8\"\n\n# 2. Manifest.toml: specifies the information of packages which those packages we just installed depend on\n\nshell&gt; head Manifest.toml\n# This file is machine-generated - editing it directly is not advised\n\njulia_version = \"1.10.0\"\nmanifest_format = \"2.0\"\nproject_hash = \"666c5e651c78c84e1125a572f7fba0bc8b920e62\"\n\n[[deps.AbstractFFTs]]\ndeps = [\"LinearAlgebra\"]\ngit-tree-sha1 = \"d92ad398961a3ed262d8bf04a1a2b8340f915fef\"\nuuid = \"621f4979-c628-5d54-868e-fcf4e3e8185c\"\nversion = \"1.5.0\"\nweakdeps = [\"ChainRulesCore\", \"Test\"]\n\n    [deps.AbstractFFTs.extensions]\n    AbstractFFTsChainRulesCoreExt = \"ChainRulesCore\"\n    AbstractFFTsTestExt = \"Test\"\n\n[[deps.AbstractLattices]]\ngit-tree-sha1 = \"222ee9e50b98f51b5d78feb93dd928880df35f06\"\nuuid = \"398f06c4-4d28-53ec-89ca-5b2656b7603d\"\nversion = \"0.3.0\"\n\nThese two files (Project.toml and Manifest.toml) are automatically created by Julia.\n\n\n4.4.2 Creating your own module and package\n\nshell&gt; cd job\n/home/yangrui/temp/job\n\nshell&gt; tree\n.\n├── Manifest.toml\n└── Project.toml\n\n0 directories, 2 files\n\n## Create a package scaffolding with the `generate` command in the package mode\n# You can also use the PkgTemplate library to create peackages with a more sophisticated way\n(job) pkg&gt; generate ToyPackage\n  Generating  project ToyPackage:\n    ToyPackage/Project.toml\n    ToyPackage/src/ToyPackage.jl\n\nshell&gt; tree\n.\n├── Manifest.toml\n├── Project.toml\n└── ToyPackage\n    ├── Project.toml  # In fact, Julia package is also an environment, which means you can add other packages it depends on\n    └── src\n        └── ToyPackage.jl  # This file contains the top-level module having the same name as the package\n\n2 directories, 4 files\n\nshell&gt; cat ToyPackage/src/ToyPackage.jl\nmodule ToyPackage  # You can now add code into this module (e.g. import names from other packages by using the `using` and `import` statements; specify what names should be exported by using the `export` statement; include other .jl files by using the `include()` function; you can also directly define variables, functions, types here)\n\ngreet() = print(\"Hello World!\")\n\nend # module ToyPackage\n\n## To make packages you are developing available when importing them by using the `using` and `import` statements, you can use the `dev` command to add your package info into the metadata files of the job environment\n(@v1.10) pkg&gt; activate job\n  Activating new project at `~/temp/job/job`\n\nshell&gt; ls\nManifest.toml  Project.toml  ToyPackage\n\n(job) pkg&gt; dev ./ToyPackage\n   Resolving package versions...\n    Updating `~/temp/job/job/Project.toml`\n  [0bc4f551] + ToyPackage v0.1.0 `../ToyPackage`\n    Updating `~/temp/job/job/Manifest.toml`\n  [0bc4f551] + ToyPackage v0.1.0 `../ToyPackage`\n\n(job) pkg&gt; status\nStatus `~/temp/job/job/Project.toml`\n  [0bc4f551] ToyPackage v0.1.0 `../ToyPackage`\n\nTwo packages are very useful when modifying and developing packages:\n\nOhMyREPL: provides syntax highlighting and history matching in the Julia REPL;\nRevise: monitors code changes to packages loaded into the REPL and updates the REPL with these changes.\n\n\n\n4.4.3 Testing your package\nYou can use the Test package to test your package.\n\n## In the ToyPackage/test/runtests.jl  # This is essential\nusing ToyPackage\nusing Test\n\n# Each test is contained in this block\n@testset \"All tests\" begin\n    include(\"trigtests.jl\")\nend\n\n## In the ToyPackage/test/trigtests.jl  # This is not essential if you write all tests into the above file\n@testset \"trigonometric tests\" begin\n    @test cos(0) = 1.0  # Each test starts with the macro @test. For floating-point numbers, the results may be not exactly identical, so you can use the ≈ (\\approx) or use the isapprox() function to specify the tolerance\n    @test sin(0) = 0.0\nend\n\n@testset \"polynomial tests\" begin\n    # Some more tests\nend\n\n## Test your package with the `test` command in the package mode\n(job) pkg&gt; activate ToyPackage  # Of course, this is not essential. You can test the ToyPackage package in any enviroment which knows where this package is (e.g. in the job environment)\n  Activating project at `~/temp/job/ToyPackage`\n\n(ToyPackage) pkg&gt; test ToyPackage  # If you are in the ToyPackage environment, only use the `test` command without the package name is fine"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#appendices",
    "href": "Blogs/Computer/posts/Programming/Julia/julia_syntax_basics/index.html#appendices",
    "title": "Julia syntax basics",
    "section": "5 Appendices",
    "text": "5 Appendices\n\n5.1 Heap and Stack\nHeap and stack are two important regions in computer memory used for storing data.\nThere are some differences between heap and stack:\n\nHeap: the heap is a larger memory area that is manually requested and released by the programmer or the memory manager of a programming language. Memory allocation on the heap is more flexible and can be dynamically adjusted according to the needs of the program. However, since it requires tracking all allocated and released memory blocks, heap management is usually more complex and slower than stack management. The heap is used to store objects whose size and lifetime are uncertain, such as dynamic arrays, object instances, etc.\nStack: the stack is a memory area managed automatically by the operating system or runtime environment. It follows the Last In, First Out (LIFO) principle, meaning the last element entered is the first one to be removed. Memory allocation and deallocation on the stack are very fast because these operations only involve moving pointers, without the need for complex memory management algorithms. The stack is typically used to store local variables and context information for function calls.\n\nJulia stores mutable data types in heap, and immutable data types in stack, which means the memory address pointed to an immutable value, such as an integer, may be unstable (changed often). So In Julia, you can only reliably get the memory address of mutable data by the follows:\n\na = [1, 2, 3, 4, 5, 6]\n\np = pointer_from_objref(a)  # get the memory address of a Julia object as a Ptr (Ptr{T} means a memory address referring to data of type T)\nprintln(p)\n\nx = unsafe_pointer_to_objref(p)  # convert a Ptr to an object reference (assuming the pointer refers to a valid heap-allocated Julia object)\nprintln(x)\n\n# ===/≡ is used to judge whether two objects are identical:\n# first the types of the two are compared\n# then mutable objects are compared by memory address\n# and immutable objects are compared by contents at the bit level\nprintln(a === x)\n\n# if x === y then objectid(x) == objectid(y)\n\n# == is used to compare whether the contents of the two obejcts are identical though other properties may also be taken into account\nx = 1 # Int64\ny = 1.0  # Float64\nprintln(x === y)\nprintln(x == y)\n\nPtr{Nothing} @0x00007f9158e0f9d0\n[1, 2, 3, 4, 5, 6]\ntrue\nfalse\ntrue\n\n\n\n\n5.2 Julia installation and configuration\n\nSetting some environmental variables globally and permanently\n\nCreating a ~/.julia/config/startup.jl file with the contents:\n\n# Customizing package server\nENV[\"JULIA_PKG_SERVER\"] = \"https://mirrors.pku.edu.cn/julia\"\n\n# Customizing https proxy\nENV[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n\n\n\n5.3 Julia REPL mode\n\njulia&gt;: the standard Julia mode.\nhelp?&gt;: the help mode. Enter help mode by pressing ?.\npkg&gt;: the package mode for installing and removing packages. Enter package mode by pressing ].\nshell&gt;: the shell mode. Enter shell mode by pressing ;.\n\nTo back to the standard Julia mode, press Backspace.\n\n\n5.4 Installing third-party packages\nPkg is Julia’s builtin package manager, which can be used to install, update, and remove packages.\nYou can install packages either by calling Pkg functions in the standard Julia mode or by executing Pkg commands in the package mode.\n\nIn the package mode:\n\n\n# To install packages (multiple packages are separated by comma or space), use add\n(@v1.9) pkg&gt; add JSON, StaticArrays\n\n# To install packages with specified versions using the @ symbol\n(@v1.9) pkg&gt; add CairoMakie@0.5.10\n\n# To remove packages, use rm or remove (some Pkg REPL commands have a short and a long version of the command)\n(@v1.9) pkg&gt; rm JSON, StaticArrays\n\n# To update packages, use up or update\n(@v1.9) pkg&gt; up\n\n# To see installed packages, use st or status\n(@v1.9) pkg&gt; st\n\n\n\n\n\n\n\nNote\n\n\n\nIn the REPL prompt, (@v1.9) lets you know that v1.9 is the active environment.\nDifferent environments can have totally different packages and versions installed from another environment.\nThis makes it possible that you can construct an environment tailored to your project, which makes your project completely reproducible.\n\n\n\nIn the standard Julia mode\n\n\njulia&gt; Pkg.add([\"JSON\", \"StaticArrays\"])\n\n# Pkg.remove()\n# Pkg.update()\n# Pkg.status()"
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html",
    "href": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html",
    "title": "R resource list",
    "section": "",
    "text": "Official documentations at https://cran.r-project.org/manuals.html.\nHands-on programming with R by Garrett Grolemund at https://rstudio-education.github.io/hopr.\nAdvanced R by Hadley Wickham at https://adv-r.hadley.nz/."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#get-started",
    "href": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#get-started",
    "title": "R resource list",
    "section": "",
    "text": "Official documentations at https://cran.r-project.org/manuals.html.\nHands-on programming with R by Garrett Grolemund at https://rstudio-education.github.io/hopr.\nAdvanced R by Hadley Wickham at https://adv-r.hadley.nz/."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#tidyverse-series-packages",
    "href": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#tidyverse-series-packages",
    "title": "R resource list",
    "section": "2 Tidyverse series packages",
    "text": "2 Tidyverse series packages\nA collection of packages for data science at https://github.com/tidyverse.\nIncluding:\n\nI/O of delimited data: vroom, readr.\nGiving you a modern data frame: tibble.\nData manipulation: dplyr.\nGiving you tidy data: tidyr.\nWorking with strings: stringr.\nWorking with factors: forcats.\nWorking with functions and vectors: purrr.\nWorking with dates and date-times: lubridate.\nWorking with time-of-day values: hms.\nWorking with binary data: blob.\nWorking with pipes: magrittr.\nFormatting your strings: glue.\nData visualization with 2D plot: ggplot2 at https://ggplot2-book.org."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#bioconductor-series-packages",
    "href": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#bioconductor-series-packages",
    "title": "R resource list",
    "section": "3 Bioconductor series packages",
    "text": "3 Bioconductor series packages\nA collection of packages for bioinfomatics at https://bioconductor.org."
  },
  {
    "objectID": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#futureverse-series-packages",
    "href": "Blogs/Computer/posts/Programming/R/r_resource_list/index.html#futureverse-series-packages",
    "title": "R resource list",
    "section": "4 Futureverse series packages",
    "text": "4 Futureverse series packages\nA unifying parallelization framework at https://www.futureverse.org/."
  },
  {
    "objectID": "Blogs/Galaxy/index.html",
    "href": "Blogs/Galaxy/index.html",
    "title": "Galaxy",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow to run ENCODE bulk RNA-seq pipeline in NeuroBorder Galaxy\n\n\n\n\n\n\nencode\n\n\nbulk rna-seq\n\n\ngalaxy\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nGuides for NeuroBorder Shiny App\n\n\n\n\n\n\nr\n\n\nneuroborder\n\n\nshiny\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\nRui Yang\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Galaxy"
    ]
  },
  {
    "objectID": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html",
    "href": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html",
    "title": "Guides for NeuroBorder Shiny App",
    "section": "",
    "text": "For convenience of internal use, NeuroBorder Shiny App is also deployed in Ubuntu Linux. It can be visited via http://172.16.50.209:61111, which is an internal IP and not a permanent IP. This means that it cannot be visited outside and may be changed in the future. If it’s invalid sometime in the future, please let me know. I will update it as I can."
  },
  {
    "objectID": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#introduction",
    "href": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#introduction",
    "title": "Guides for NeuroBorder Shiny App",
    "section": "",
    "text": "For convenience of internal use, NeuroBorder Shiny App is also deployed in Ubuntu Linux. It can be visited via http://172.16.50.209:61111, which is an internal IP and not a permanent IP. This means that it cannot be visited outside and may be changed in the future. If it’s invalid sometime in the future, please let me know. I will update it as I can."
  },
  {
    "objectID": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#notes-for-graph-apps",
    "href": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#notes-for-graph-apps",
    "title": "Guides for NeuroBorder Shiny App",
    "section": "2 Notes for Graph Apps",
    "text": "2 Notes for Graph Apps\n\nggplot2 aesthetic specifications: you can find supported formats for ggplot2 aesthetics of color, line type, point shape, etc. here."
  },
  {
    "objectID": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#download-and-run-neuroborder-shiny-app-in-your-laptop",
    "href": "Blogs/Galaxy/posts/Shiny/guides_for_neuroborder_shiny_app/index.html#download-and-run-neuroborder-shiny-app-in-your-laptop",
    "title": "Guides for NeuroBorder Shiny App",
    "section": "3 Download and run NeuroBorder Shiny App in your laptop",
    "text": "3 Download and run NeuroBorder Shiny App in your laptop\n\n3.1 Install R, Rtools, RStudio\nDownload and install suitable versions of R and/or Rtools (recommended for Windows users) for your computer from the offical R website or more directly from some mirrors of it, like TUNA Team of Tsinghua University.\nIt’s highly recommended to download and install suitable version of RStudio for your computer from the offical RStudio website although this is unnecessary for running NeuroBorder Shiny App. But it can provide a great deal of convenience for you to use R.\n\n\n3.2 Install R packages\nOpen your R/RStudio, and paste the following code into your console, and enter to run it:\n\ninstall.packages(\"FactoMineR\")\ninstall.packages(\"aplot\")\ninstall.packages(\"bslib\")\ninstall.packages(\"digest\")\ninstall.packages(\"enrichplot\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"gginnards\")\ninstall.packages(\"ggnewscale\")\ninstall.packages(\"ggprism\")\ninstall.packages(\"ggrepel\")\ninstall.packages(\"ggtext\")\ninstall.packages(\"glue\")\ninstall.packages(\"magrittr\")\ninstall.packages(\"scales\")\ninstall.packages(\"shiny\")\ninstall.packages(\"stringr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"uuid\")\ninstall.packages(\"openxlsx\")\ninstall.packages(\"vroom\")\ninstall.packages(\"jsonlite\")\ninstall.packages(\"DT\")\ninstall.packages(\"unigd\")\n\n\n\n3.3 Download and run NeuroBorder Shiny App\nNow, you can download the NeuroBorder Shiny App from here. And then just click Code \\(\\to\\) Download ZIP step by step.\nOnce you have downloaded and unzipped it, you will find a file named app.R, which is the top entry for this app. Of course, you don’t need to care about what it is.\nNext, open your R/RStudio, and paste the following code into your console, and enter to run it:\n\nshiny::runApp(appDir = \"C:/Users/yangrui/Downloads/NeuroBorder-ShinyWebApp-main\", launch.browser = TRUE)\n\nNOTE: you should replace the path C:/Users/yangrui/Downloads/NeuroBorder-ShinyWebApp-main with yours. This is a path to the directory containing app.R.\nIn theory, the system’s default web browser will be launched automatically after the app is started. If it is not the case, you can find the listening address from its output like Listening on http://127.0.0.1:3593, and paste http://127.0.0.1:3593 to your browser yourself. So far, you can use it just like a web page."
  },
  {
    "objectID": "Blogs/Mathematics/index.html",
    "href": "Blogs/Mathematics/index.html",
    "title": "Mathematics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProbability and statistics (misc)\n\n\n\n\n\n\nprobability\n\n\nstatistics\n\n\nmisc\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nProbability and statistics with Julia\n\n\n\n\n\n\nprobability\n\n\nstatistics\n\n\njulia\n\n\n\n\n\n\n\n\n\nSep 22, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical inference concepts of binomial distribution\n\n\n\n\n\n\nstatistical inference\n\n\nbinomial distribution\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic function\n\n\n\n\n\n\nlogistic function\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial distribution approximations\n\n\n\n\n\n\nbinomial distribution\n\n\napproximation\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nLorenz curve and Gini index\n\n\n\n\n\n\nlorenz curve\n\n\ngini index\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nP-P plot and Q-Q plot\n\n\n\n\n\n\np-p plot\n\n\nq-q plot\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nROC and AUC\n\n\n\n\n\n\nroc\n\n\nauc\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nRui Yang\n\n\n\n\n\n\n\n\n\n\n\n\nTrigonometric functions\n\n\n\n\n\n\ntrigonometric functions\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nRui Yang\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Mathematics"
    ]
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/logistic_function/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/logistic_function/index.html",
    "title": "Logistic function",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser. No biggie. You can click here to download the PDF file."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/logistic_function/index.html#introduction",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/logistic_function/index.html#introduction",
    "title": "Logistic function",
    "section": "",
    "text": "It appears you don't have a PDF plugin for this browser. No biggie. You can click here to download the PDF file."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html",
    "title": "P-P plot and Q-Q plot",
    "section": "",
    "text": "In statistics, a P-P plot (probability-probability plot or percent-percent plot or P value plot) is a probability plot for assessing how closely two datasets agree, or for assessing how closely a dataset fits a particular model.\nIt works by plotting the two cumulative distribution functions against each other; if they are similar, the data will appear to be nearly a straight line.\nA P-P plot plots two cumulative distribution functions (CDFs) against each other: given two probability distributions with CDFs F and G, it plots \\((F(z), G(z))\\) as \\(z\\) ranges from \\(-\\infty\\) to \\(\\infty\\). As a CDF has range \\([0, 1]\\), the domain of this parametric graph is \\((-\\infty, \\infty)\\), and the range is the unit square \\([0,1] \\times [0,1]\\).\nThus for input \\(z\\), the output is the pair of numbers giving what percentage of \\(F\\) and what percentage of \\(G\\) fall at or below \\(z\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html#p-p-plot",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html#p-p-plot",
    "title": "P-P plot and Q-Q plot",
    "section": "",
    "text": "In statistics, a P-P plot (probability-probability plot or percent-percent plot or P value plot) is a probability plot for assessing how closely two datasets agree, or for assessing how closely a dataset fits a particular model.\nIt works by plotting the two cumulative distribution functions against each other; if they are similar, the data will appear to be nearly a straight line.\nA P-P plot plots two cumulative distribution functions (CDFs) against each other: given two probability distributions with CDFs F and G, it plots \\((F(z), G(z))\\) as \\(z\\) ranges from \\(-\\infty\\) to \\(\\infty\\). As a CDF has range \\([0, 1]\\), the domain of this parametric graph is \\((-\\infty, \\infty)\\), and the range is the unit square \\([0,1] \\times [0,1]\\).\nThus for input \\(z\\), the output is the pair of numbers giving what percentage of \\(F\\) and what percentage of \\(G\\) fall at or below \\(z\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html#q-q-plot",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/p-p_plot_and_q-q_plot/index.html#q-q-plot",
    "title": "P-P plot and Q-Q plot",
    "section": "2 Q-Q plot",
    "text": "2 Q-Q plot\nIn statistics, a Q-Q plot (quantile-quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. A point \\((x, y)\\) on the plot corresponds to one of the quantiles of the second distribution (\\(y\\)-coordinate) plotted against the same quantile of the first distribution (\\(x\\)-coordinate). This defines a parametric curve where the parameter is the index of the quantile interval.\n\nIf the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the identity line \\(y = x\\).\nIf the distributions are linearly related, the points in the Q-Q plot will approximately lie on a line, but not necessarily on the line \\(y = x\\).\n\n\nusing Random, Distributions, CairoMakie, StatsBase\n\nRandom.seed!(1234)\n\n# assume that we have a sample of size n\nn = 10\n# observations are sampled from the standard normal distribution and i.i.d\n# this process is quite similar with the process\n# where we repeat an experiment n times and get n i.i.d observations\n# subjected to some unknown distribution\ndist = Normal()\n\n# we divide the standard normal distribution into n equal parts\n# which are denoted by their middle points\n# e.g. the k-th middle point is (k - 0.5) / n\n# this means that the probability of sampling any of the n middle points is 1/n in a single experiment\n# i.e. in a single experiment, we have the same chance to sample any of the n middle points\nmiddle_quantiles = [(k - 0.5) / n for k in 1:n]\nequal_intervals = [(quantile(dist, q - 0.5 / n), quantile(dist, q + 0.5 / n)) for q in middle_quantiles]\n\nN = 10^6\nd = Array{Float64}(undef, N)\n\nfor i in 1:N\n    r = rand(dist, 1)\n    d[i] = middle_quantiles[@. (r &gt; first(equal_intervals)) && (r &lt; last(equal_intervals))][1]\nend\nmiddle_quantiles_count_dict = countmap(d)\nmiddle_quantiles_count = [middle_quantiles_count_dict[k] for k in middle_quantiles]\nmiddle_quantiles_count = middle_quantiles_count ./ sum(middle_quantiles_count)\nstem(middle_quantiles, middle_quantiles_count)"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html",
    "title": "Probability and statistics with Julia",
    "section": "",
    "text": "Statistics with Julia by Yoni Nazarathy (2021)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#references",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#references",
    "title": "Probability and statistics with Julia",
    "section": "",
    "text": "Statistics with Julia by Yoni Nazarathy (2021)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#pseudorandom-number-generation",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#pseudorandom-number-generation",
    "title": "Probability and statistics with Julia",
    "section": "2 Pseudorandom number generation",
    "text": "2 Pseudorandom number generation\nFor pseudorandom number generation, there is some deterministic (non-random and well defined) sequence \\(\\{x_n\\}\\), specified by\n\\[\nx_{n+1} = f(x_n, x_{n-1}, ...)\n\\]\noriginating from some specified seed \\(x_0\\). The mathematical function \\(f(\\cdot)\\) is designed to yield desirable properties for the sequence \\(\\{x_n\\}\\) that make it appear random.\nThose properties include:\n\nElements \\(x_i\\) and \\(x_j\\) for \\(i \\neq j\\) should appear statistically independent. That is, knowing the value of \\(x_i\\) should not yield any information about the value \\(x_j\\).\nThe distribution of \\(\\{x_n\\}\\) should appear uniform. That is, there shouldn’t be values (or ranges of values) where elements of \\(\\{x_n\\}\\) occur more frequently than others.\nThe range covered by \\(\\{x_n\\}\\) should be well defined.\nThe sequence should repeat itself as rarely as possible.\n\nIn Julia, the main player for pseudorandom number generation is the function rand(), which generates a random number in each call without giving any arguments once a seed is set (it is usually set to the current time by default). You can set the seed yourself by using the Random.seed!() function from the Random package.\n\nusing Random\n\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2024)\nprintln(\"Seed 2024: \", rand(), \"\\t\", rand(), \"\\t\", rand())\nRandom.seed!(2023)\nprintln(\"Seed 2023: \", rand(), \"\\t\", rand(), \"\\t\", rand())\n\nSeed 2023: 0.1321419481484759   0.37179020701747134 0.9005132318421792\nSeed 2024: 0.10884245939837256  0.7189031628453905  0.5826662196960152\nSeed 2023: 0.1321419481484759   0.37179020701747134 0.9005132318421792\n\n\nAs can be seen from the output, setting the same seed will generate the same sequence.\n\n2.1 Creating a simple pseudorandom number generator\nHere, we create a Linear Congruential Generator (LCG). The function \\(f(\\cdot)\\) used here is just a linear transformation modulo \\(m\\): \\(x_{n+1} = (ax_n + c) \\mod m\\).\nHere, we pick \\(m = 2^{32}\\), \\(a = 69069\\), \\(c = 1\\), which yields sensible performance.\n\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\na, c, m = 69069, 1, 2^32\nnext(x) = (a * x + c) % m\n\nN = 10^6\nvec = Array{Float64,1}(undef, N)\n\nx = 2024  # Seed\nfor i in 1:N\n    global x = next(x)\n    vec[i] = x / m  # Scale x to [0, 1]\nend\ndf = DataFrame(x=1:N, y=vec)\n\nfig = Figure()\np1 = data(first(df, 5000)) * mapping(:x, :y) * visual(Scatter, markersize=3)\np2 = data(df) * mapping(:y) * visual(Hist, bins=50, normalization=:pdf)\ndraw!(fig[1, 1], p1, axis=(xlabel=\"n\", ylabel=L\"\\mathbf{x_n}\"))\ndraw!(fig[1, 2], p2, axis=(xlabel=\"x\", ylabel=\"Density\"))\nfig\n\n\n\n\n\n\n2.2 More about Julia’s pseudorandom number generator\nIn addition to rand(), we can also use randn() to generate normally distributed random numbers.\nAfter invoking using Random, the following functions are available:\n\nRandom.seed!()\nrandsubseq()\nrandstring()\nrandcycle()\nbitrand()\nrandperm() and shuffle() for permutations\n\nIn addition, in Julia, we can create an object representing a pseudorandom number generator implemented via a specified algorithm, for example, the Mersenne Twister pseudorandom number generator, which is considerably more complicated than the LCG described above. In Julia, we can create such an object of the Mersenne Twister pseudorandom number generator by calling rng = MersenneTwister(seed), and then pass the rng to rand() to let it use the given pseudorandom number generator to generate pseudorandom numbers."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#monte-carlo-simulation",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#monte-carlo-simulation",
    "title": "Probability and statistics with Julia",
    "section": "3 Monte Carlo simulation",
    "text": "3 Monte Carlo simulation\nThe core idea of Monte Carlo simulation lies in building a mathematical relationship between an unknown quantity to be estimated and the probability of a certain event, which can be estimated by statistical sampling. Then, we can get an estimate of this unknown quantity.\nWe can use this idea to estimate the value of \\(\\pi\\).\n\nusing DataFrames, AlgebraOfGraphics, CairoMakie\n\nline_df = DataFrame(x=[0, 0, 1, 1, 0],\n    y=[0, 1, 1, 0, 0])\n\nx = range(0, 1, length=1000)\nquarter_circle_df = DataFrame(x=x,\n    y=@. sqrt(1 - x^2))\n\nrect = data(line_df) * mapping(:x, :y) * visual(Lines)\nquarter_circle = data(quarter_circle_df) * mapping(:x, :y) * visual(Lines)\ndraw(rect + quarter_circle, axis=(limits=(0, nothing, 0, nothing),))\n\n\n\n\nAs can be seen from the above figure, we know:\n\nThe area of the unit square is 1;\nThe area of the first quadrant of the unit circle is \\(\\pi / 4\\);\nThen, if we randomly throw a ball within the unit square, the probability of the event that this ball falls into the area of the first quadrant of the unit circle is \\(\\pi / 4\\). Further, we know that the probability of this event can be estimated by its frequency if we repeat this experiment infinitely many times; therefore, we can estimate the value of \\(\\pi\\) by the following formula:\n\n\\[\n\\hat{\\pi} = 4 \\frac{\\text{The number of times falling in }x^2 + y^2 \\leq 1}{\\text{Total number of times}}\n\\]\n\nusing Random, LinearAlgebra, AlgebraOfGraphics, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nN = 10^5\ndf = DataFrame([(x=rand(), y=rand()) for _ in 1:N])\ntransform!(df, [:x, :y] =&gt; ByRow((x, y) -&gt; ifelse(norm([x, y]) &lt;= 1, \"in\", \"out\")) =&gt; :flag)\npi_estimate = 4 * count(df.flag .== \"in\") / N\nprintln(\"π estimate: \", pi_estimate)\n\nfig = Figure()\np = data(df) * mapping(:x, :y, color=:flag) * visual(Scatter, markersize=1)\ndraw!(fig, p, axis=(limits=(0, nothing, 0, nothing),))\nfig\n\nπ estimate: 3.14688"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#distributions-and-related-packages-for-probability-distributions",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#distributions-and-related-packages-for-probability-distributions",
    "title": "Probability and statistics with Julia",
    "section": "4 Distributions and related packages for probability distributions",
    "text": "4 Distributions and related packages for probability distributions\n\n4.1 Introduction\nPackages:\n\nStatistics (built-in)\nStatsBase\nDistributions\n\n\n4.1.1 Weighted vectors\nThe StatsBase package provides the “weighted vector” object via Weights(), which allows for an array of values to be given probabilistic weights.\nAn alternative of Weights() is to use the Categorical distribution supplied by the Distributions package.\nTogether with Weights(), you can use the sample() function from StatsBase to generate observations.\n\nusing StatsBase, Random\n\nRandom.seed!(1234)\n\ngrades = 'A':'E'\nweights = Weights([0.1, 0.2, 0.1, 0.2, 0.4])\n\nN = 10^6\nd = sample(grades, weights, N)\n[count(i -&gt; i == g, d) for g in grades] / N\n\n5-element Vector{Float64}:\n 0.099651\n 0.200224\n 0.099522\n 0.20053\n 0.400073\n\n\n\n\n4.1.2 Distribution type objects\nThe Distributions package allows us to create distribution type objects based on what family they belong to. Then these distribution type objects can be used as arguments for other functions.\n\nusing Distributions, CairoMakie\n\ndist = TriangularDist(0, 2, 1)  # Triangular distribution\nx = 0:0.01:2\nu = 0:0.01:1\n\nfig = Figure(size=(800, 250))\nlines!(Axis(fig[1, 1], xlabel=\"x\", ylabel=\"f(x)\"), x, pdf.(dist, x))  # PDF\nlines!(Axis(fig[1, 2], xlabel=\"x\", ylabel=\"F(x)\"), x, cdf.(dist, x))  # CDF\nlines!(Axis(fig[1, 3], xlabel=\"u\", ylabel=L\"\\mathbf{F^{-1}(u)}\"), u, quantile.(dist, u))  # ICDF\nfig\n\n\n\n\n\nprintln(\"Parameters: \", params(dist))\nprintln(\"Central descriptors: \", mean(dist), \", \", median(dist))\nprintln(\"Dispersion descriptos: \", var(dist), \", \", std(dist))\nprintln(\"Higher-order moment shape descriptors: \", skewness(dist), \", \", kurtosis(dist))\nprintln(\"Range: \", minimum(dist), \", \", maximum(dist))\nprintln(\"Mode: \", mode(dist), \", \", modes(dist))  # Value(s) of x where PMF or PDF is maximized\n\nParameters: (0.0, 2.0, 1.0)\nCentral descriptors: 1.0, 1.0\nDispersion descriptos: 0.16666666666666666, 0.408248290463863\nHigher-order moment shape descriptors: 0.0, -0.6\nRange: 0.0, 2.0\nMode: 1.0, [1.0]"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#univariate-distributions",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#univariate-distributions",
    "title": "Probability and statistics with Julia",
    "section": "5 Univariate distributions",
    "text": "5 Univariate distributions\n\n5.1 Families of discrete distributions\n\n5.1.1 Discrete uniform distribution\n\nusing StatsBase, CairoMakie\n\nfaces, N = 1:6, 10^6\n\nmcEstimate = counts(rand(faces, N), faces) / N  # rand(faces, N) is identical to rand(DiscreteUniform(1, 6), N)\ntheory = [1 / 6 for _ in faces]\n\nfig, ax = stem(faces, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, faces, theory, label=\"Theory\",\n    color=:red, stemcolor=:red)\nylims!(ax, nothing, 0.25)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.1.2 Binomial distribution\n\nusing StatsBase, Distributions, CairoMakie\n\nbinomialRV(n, p) = sum(rand(n) .&lt; p)\n\np, n, N = 0.25, 10, 10^6\n\nb_dist = Binomial(n, p)\nx = 0:n\nb_pmf = pdf.(b_dist, x)\nest_data = [binomialRV(n, p) for _ in 1:N]\nest_pmf = counts(est_data, 0:n) / N\n\nfig, ax = stem(x, est_pmf, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, b_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.1.3 Geometric distribution\nConsider an infinite sequence of independent trials, each with sucess probability \\(p\\), and let \\(X\\) be the first trial that is successful. Then the PMF is:\n\\[\nP(X=x) = p(1-p)^{x-1}\n\\]\nfor \\(x = 1, 2, ...\\).\nAn alternative version is to count the number of failures until success. Obviously, we have \\(\\tilde{X} = X - 1\\). Then the PMF is:\n\\[\nP(\\tilde{X} = x) = p(1-p)^x\n\\]\nfor \\(x = 0, 1, 2, ...\\).\nIn Distributions package, Geometric stands for the distribution of \\(\\tilde{X}\\).\n\nusing StatsBase, Distributions, CairoMakie\n\nfunction geometricRV(p)\n    x = 0\n    while true\n        if rand() &lt; p\n            return x\n        end\n        x += 1\n    end\nend\n\np = 0.25\nx = 0:25\nN = 10^6\n\ng_dist = Geometric(p)\ng_pmf = pdf.(g_dist, x)\nmcEstimate = counts([geometricRV(p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, g_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.1.4 Negative binomial distribution\n\\(X\\) stands for the number of trials until the \\(r\\)-th success. The PMF is:\n\\[\nP(X=x) = \\binom{x-1}{r-1} p^r (1-p)^{x-r}\n\\]\nfor \\(x = r, r+1, r+2, ...\\).\nSimilarly to the geometric distribution, we usually count the number of failures until the \\(r\\)-th success. The PMF is:\n\\[\nP(\\tilde{X} = x) = \\binom{x+r-1}{x} p^r (1-p)^x\n\\]\nfor \\(x = 0, 1, 2, ...\\).\n\nusing StatsBase, Distributions, CairoMakie\n\nfunction nbRV(r, p)\n    x = 0\n    success = 0\n    while true\n        if success == r\n            return x\n        end\n        if rand() &lt; p\n            success += 1\n        else\n            x += 1\n        end\n    end\nend\n\nr = 5\np = 0.25\nx = 0:60\nN = 10^6\n\nnb_dist = NegativeBinomial(r, p)\nnb_pmf = pdf.(nb_dist, x)\nmcEstimate = counts([nbRV(r, p) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, nb_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nSo far, we’ve seen that binomial distribution, Bernoulli distribution (0-1 distribution, two-point distribution), geometric distribution, and negative binomial distribution all involve Bernoulli trials which has exactly two possible outcomes, “success”, and “failure”, where the probability of success is the same every time the experiment is conducted.\nIn a word:\n\nBinomial distribution (\\(X \\sim B(n, p)\\)): \\(X\\) indicates the number of successes in \\(n\\) Bernoulli experiments.\nBernoulli distribution (\\(X \\sim B(1, p)\\)): \\(X\\) indicates the number of successes in \\(1\\) Bernoulli experiments.\nGeometric distribution (\\(X \\sim Ge(p)\\)): \\(X\\) indicates the number of total Bernoulli experiments until the first success.\nNegative binomial distribution (\\(X \\sim Nb(r, p)\\)): \\(X\\) indicates the number of total Bernoulli experiments until the \\(r\\)-th success.\n\nObviously, a binomial distribution or a negative binomial distribution can be divided into \\(n\\) Bernoulli distributions or \\(r\\) geometric distributions, respectively.\n\n\n\n\n5.1.5 Hypergeometric distribution\nHypergeometric distribution means sampling without replacement, which means the probability of success changes for each subsequent sample.\nThe PMF is:\n\\[\np(x) = \\frac{\\binom{M}{x} \\binom{N-M}{n-x}}{\\binom{N}{n}}\n\\]\nfor \\(x = max(0, n+M-N), ..., min(n, M)\\), where \\(N\\) (the population size), \\(M\\) (the number of successes), and \\(n\\) (the sample size) are all parameters.\nNote: \\(max(0, n+M-N)\\): if \\(n \\gt N-M\\) (i.e., \\(n\\) is greater than the number of failures), then at least \\(n - (N-M)\\) successes must occur.\n\nusing Distributions, CairoMakie\n\nN, M, n = 500, 100, 60\nx = max(0, n - (N - M)):min(n, M)\n\nh_dist = Hypergeometric(M, N - M, n)  # the 1st is the number of successes; the 2nd is the number of failures; the 3rd is the sample size\nh_pmf = pdf.(h_dist, x)\n\nstem(x, h_pmf,\n    color=:black, stemcolor=:black,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\n\n\n\n\n\n\n5.1.6 Poisson distribution\nThe Poisson process is a stochastic process (random process) which can be used to model occurrences of events over time or more generally in space.\nIn a Poisson process, during an infinitesimally small time interval, \\(\\Delta t\\), it is assumed that as \\(\\Delta t \\rightarrow 0\\):\n\nThere is an occurrence with probability \\(\\lambda \\Delta t\\) and no occurrence with probability \\(1 - \\lambda \\Delta t\\).\nThe chance of 2 or more occurences during an interval of length \\(\\Delta t\\) tends to \\(0\\).\n\nHere, \\(\\lambda \\gt 0\\) is the intensity of the Poisson process, and has the property that when multiplied by an interval of length \\(T\\), the mean occurrences during the interval is \\(\\lambda T\\).\nFor a Poisson process over the time interval \\([0, T]\\), the Poisson distribution can be used to describe the number of occurrences. The PMF is:\n\\[\nP(x\\text{ Poisson process occurrences during interval }[0, T]) = \\frac{(\\lambda T)^x}{x!} e^{-\\lambda T}\n\\]\nfor \\(x = 0, 1, 2, ...\\).\nWhen the interval is \\([0, 1]\\), then we have the PMF:\n\\[\np(x) = \\frac{\\lambda ^x}{x!} e^{-\\lambda}\n\\]\nfor \\(x = 0, 1, 2, ...\\), where \\(\\lambda\\) is the mean of occurences.\nIn addition, the times between occurrences in the Poisson process are exponentially distributed.\nThe Poisson process has many elegant analytic properties. One such property is to consider the random variable \\(N \\ge 0\\) such that\n\\[\n\\prod_{i=1}^{N} U_i \\ge e^{-\\lambda} \\gt \\prod_{i=1}^{N+1} U_i\n\\]\nwhere \\(U_1, U_2, ...\\) is a sequence of i.i.d uniform \\((0, 1)\\) random variables and \\(\\prod_{i=1}^{0} \\equiv 1\\).\nIt turns out that \\(N\\) is Poisson distributed with mean \\(\\lambda\\).\n\nusing StatsBase, Distributions, CairoMakie\n\nfunction pRV(lambda)\n    N, p = 0, 1\n    while p &gt;= MathConstants.e^(-lambda)\n        N += 1\n        p *= rand()\n    end\n    return N - 1\nend\n\nx = 0:20\nlambda = 5.5\nN = 10^6\n\np_dist = Poisson(lambda)\np_pmf = pdf.(p_dist, x)\n\nmcEstimate = counts([pRV(lambda) for _ in 1:N], x) / N\n\nfig, ax = stem(x, mcEstimate, label=\"Estimate\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, p_pmf, label=\"Theory\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n\n\n\n\n\n\n\n5.2 Families of continuous distributions\n\n5.2.1 Continuous uniform distribution\n\nusing Distributions, CairoMakie\n\nx = 0:0.1:2π\nN = 10^6\n\nc_unif_dist = Uniform(0, 2π)\nc_unif_pmf = pdf.(c_unif_dist, x)\nest_data = rand(N) * 2π  # Equivalent to rand(c_unif_dist, N)\n\nfig, ax = stephist(est_data, normalization=:pdf, color=:black, label=\"Estimate\")\nlines!(ax, x, c_unif_pmf, color=:red, label=\"Theory\")\nylims!(ax, nothing, 0.3)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.2.2 Exponential distribution\nAs mentioned before, the exponential distribution is often used to model random durations between occurrences in the Poisson process.\nA non-negative random variable \\(X\\), expoentially distributed with a rate parameter \\(\\lambda\\), has PDF:\n\\[\nf(x) = \\lambda e^{-\\lambda x}\n\\]\nAs can be verified, the mean is \\(\\frac{1}{\\lambda}\\), the variance is \\(\\frac{1}{\\lambda ^2}\\), and the CCDF is \\(\\bar{F}(x) = e^{-\\lambda x}\\).\nIn addition, exponential random variables possess a lack of memory property:\n\\[\nP(X&gt;t+s|X&gt;t) = P(X&gt;s)\n\\]\nWhile geometric random variables also have such a property, this hints at the fact that exponential random variables are the continuous analogs of geometric random variables.\n\n\n\n\n\n\nSuppose that X is an exponential random variable, and \\(Y = \\lfloor X \\rfloor\\), where \\(\\lfloor \\cdot \\rfloor\\) represents the floor function. Then we’ll know that \\(Y\\) is a geometric random variable:\n\\[\np_Y (y) = P(\\lfloor X \\rfloor = y) = \\int_y^{y+1} \\lambda e^{-\\lambda x} \\mathrm{d}x = (e^{-\\lambda})^y (1-e^{-\\lambda})\n\\]\nfor \\(y = 0, 1, 2, ...\\).\nIf we set \\(p = 1 - e^{-\\lambda}\\), then \\(Y\\) is a geometric random variable (representing the number of failures until the first success) which starts at \\(0\\) and has the success parameter \\(p\\).\n\n\n\nNote: the parameter of Exponential is \\(\\frac{1}{\\lambda}\\), instead of \\(\\lambda\\).\nExponential distribution:\n\nusing Distributions, CairoMakie\n\nlambda = 1\nx = 0:0.01:10\n\nexp_dist = Exponential(1 / lambda)\nexp_pmf = pdf.(exp_dist, x)\n\nlines(x, exp_pmf, color=:black)\n\n\n\n\nThe PMF of the floor of an exponential random variable is a geometric distribution:\n\nusing StatsBase, Distributions, CairoMakie\n\nlambda = 1\nN = 10^6\nx = 0:6\n\nexp_dist = Exponential(1 / lambda)\nfloor_data = counts(convert.(Int, floor.(rand(exp_dist, N))), x) / N\n\ngeom_dist = Geometric(1 - MathConstants.e^-lambda)\n\nfig, ax = stem(x, floor_data, label=\"Floor of Exponential\",\n    color=:black, stemcolor=:black,\n    stemwidth=6, markersize=18,\n    axis=(xlabel=\"x\", ylabel=\"f(x)\"))\nstem!(ax, x, geom_dist, label=\"Geometric\",\n    color=:red, stemcolor=:red)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.2.3 Gamma distribution\nThe gamma distribution is commonly used to model asymmetric non-negative data.\nIt generalizes the exponential distribution and the chi-squared distribution.\nConsider such an example, where the lifetimes of light bulbs are exponentially distributed with mean \\(\\frac{1}{\\lambda}\\). Now imagine that we are lighting a room continuously with a single light bulb, and that we replace the bulb with a new one when it burns out. If we start at time \\(0\\), what is the distribution of time until \\(n\\) bulbs are replaced?\nOne way to describe this time is by the random variable \\(T\\), where\n\\[\nY = X_1 + X_2 + ... + X_n\n\\]\nand \\(X_i\\) are i.i.d. exponential random variables representing the lifetimes of light bulbs. It turns out that the distribution of \\(T\\) is a gamma distribution.\nAt a first glance, this is quite similar with the case, where the random variable of geometric distribution indicates the total number of Bernoulli trials until the first success, while the random variable of negative binomial distribution indicates the total number of Bernoulli trials until the \\(r\\)-th success, and we have \\(Y = X_1 + X_2 + ... + X_r\\), where \\(Y\\) is a random variable of negative binomial distribution, and \\(X_i\\) are i.i.d. geometric random variables.\nThe PDF of the gamma distribution is proportional to \\(x^{\\alpha - 1} e^{-\\lambda x}\\), where \\(\\alpha\\) is called the shape parameter, and \\(\\lambda\\) is called the rate parameter.\nIn order to normalize \\(x^{\\alpha - 1} e^{-\\lambda x}\\), we need to divide by \\(\\int_0^\\infty x^{\\alpha -1} e^{-\\lambda x} \\mathrm{d}x\\), which can be represented by \\(\\frac{\\Gamma (\\alpha)}{\\lambda ^\\alpha}\\), where \\(\\Gamma (\\cdot)\\) is called the gamma function.\nThen, the PDF of the gamma distribution is:\n\\[\nf(x) = \\frac{\\lambda ^\\alpha}{\\Gamma (\\alpha)} x^{\\alpha - 1} e^{-\\lambda x}\n\\]\ni.e., \\(X \\sim Ga(\\alpha, \\lambda)\\).\nIn the light bulbs case, we have \\(T \\sim Ga(n, \\lambda)\\), where \\(\\alpha = n\\).\nIt can also be evaluated that \\(E[X] = \\frac{\\alpha}{\\lambda}\\) and \\(Var(X) = \\frac{\\alpha}{\\lambda ^2}\\).\n\n\n\n\n\n\nSquared coefficient of variation\n\n\n\nSquared coefficient of variation is often used for non-negative random variables:\n\\[\nSCV = \\frac{Var(X)}{[E(X)]^2}\n\\]\nThe SCV is a normalized or unit-less version of the variance.\nThe lower it is, the less variability in the random variable.\nIt can be seen that for a gamma random variable, the SCV is \\(\\frac{1}{\\alpha}\\).\nFor the light bulbs case, the SCV is \\(\\frac{1}{n}\\), which indicates for large \\(n\\), i.e., more light bulbs, there is less variability.\n\n\n\nusing Distributions, CairoMakie\n\nlambda = 1 / 3\nN = 10^6\nbulbs = [1, 10, 50]  # α = 1 is exponential\nx = 0:0.1:20\ncolors = [:blue, :red, :green]\n\n# Theoretical gamma PDFs\n# For each case, we set the rate parameter at λn, so that the mean time until all light bulbs run out is n/(λn) = 1/λ, independent of n\n# For the rate parameter, like Exponential, Gamma also accepts 1/λ, not λ\nga_dists = [Gamma(n, 1 / (n * lambda)) for n in bulbs]\n\n# Generate exponentially distributed pseudorandom numbers by using the inverse probability transformation\nfunction approxBySumExp(dist::Gamma)\n    n = Int64(shape(dist))  # shape() is used to get the shape parameter α\n    [sum(-(1 / (n * lambda)) * log.(rand(n))) for _ in 1:N]  # Generate n exponentially distributed pseudorandom numbers, and then add them up to generate N gamma distributed pseudorandom numbers\nend\n\nest_data = approxBySumExp.(ga_dists)\n\nfig = Figure()\nax = Axis(fig[1, 1])\nfor i in 1:length(bulbs)\n    label = string(\"Shape = \", round(shape(ga_dists[i]), digits=2), \", Scale = \", round(Distributions.scale(ga_dists[i]), digits=2))  # The inverse of the rate parameter is called the scale parameter. Of coourse, you can also use the rate() function to get the rate parameter (λ)\n    stephist!(ax, est_data[i], normalization=:pdf, color=colors[i], label=label, bins=50)\nend\nfor i in 1:length(bulbs)\n    lines!(ax, x, pdf.(ga_dists[i], x), color=colors[i])\nend\nxlims!(ax, 0, 20)\nylims!(ax, 0, 1)\naxislegend(ax)\nfig\n\n\n\n\nNote: in the above code, we generate the exponentially distributed pseudorandom numbers by using the inverse probability transformation: \\(F(x) = P(X \\le x) = 1 - e^{-\\lambda x} \\Longrightarrow U = F(X) \\Longrightarrow U = 1 - e^{-\\lambda X} \\Longrightarrow X = -\\frac{1}{\\lambda} \\log(1-U) \\Longrightarrow X = -\\frac{1}{\\lambda} \\log U\\) (since we’ll use the rand function to generate uniformly distributed pseudorandom numbers in \\([0, 1]\\), it’s reasonable that replacing \\(1-U\\) with \\(U\\)).\n\n\n5.2.4 Beta distribution\nThe beta distribution is a commonly used distribution when seeking a parameterized shape over a finite support.\nThe PDF is:\n\\[\nf(x) = \\frac{x^{\\alpha - 1} (1-x)^{\\beta -1}}{B(\\alpha, \\beta)}\n\\]\nfor \\(x \\in [0, 1]\\). Both \\(\\alpha\\) and \\(\\beta\\) are shape parameters.\n\nusing Distributions, CairoMakie\n\nx = 0:0.01:1\n\nfig, ax = lines(x, pdf.(Beta(2, 2), x), label=\"α = β = 2\")\nlines!(ax, x, pdf.(Beta(1, 1), x), label=\"α = β = 1\")  # U(0, 1)\naxislegend(ax)\nfig\n\n\n\n\nNote: you can use mathematical special functions like beta or gamma function calling beta or gamma provided by the SpecialFunctions package. In addition, QuadGK provides the quadgk function to integrate one-dimensional function.\n\n\n5.2.5 Weibull distribution\nFor a random variable \\(T\\), representing the lifetime of an individual or a component, an interesting quantity is the instantaneous chance of failure at any time, given that the component has been operating without failure up to time \\(x\\).\nThe instantaneous chance of failure at time \\(x\\) can be expressed as\n\\[\nh(x) = \\lim_{\\Delta \\to 0} \\frac{1}{\\Delta} P(T \\in [x, x+\\Delta] | T \\gt x)\n\\]\nAlternatively, by using the conditional probability (\\(P(T \\in [x, x+\\Delta] | T \\gt x) = \\frac{P(T \\in [x, x+\\Delta])}{P(T \\gt x)} = \\frac{P(T \\in [x, x+\\Delta])}{1 - P(X \\le x)}\\)) and noticing that the PDF \\(f(x)\\) satisfies \\(f(x)\\Delta \\approx P(x \\le T \\lt x + \\Delta)\\) for small \\(\\Delta\\), we can express \\(h(x)\\) as\n\\[\nh(x) = \\lim_{\\Delta \\to 0} \\frac{f(x)\\Delta}{\\Delta (1-F(x))} = \\frac{f(x)}{1-F(x)}\n\\]\nHere the function \\(h(\\cdot)\\) is called the hazard rate function, which is often used in reliability analysis and survival analysis. It’s a common method of viewing the distribution for lifetime random variables \\(T\\).\nIn fact, we can reconstruct the CDF of \\(T\\) as\n\\[\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n\\]\nThe Weibull distribution is defined through the hazard rate function of the form \\(h(x) = \\lambda x^{\\alpha - 1}\\). Where \\(\\lambda\\) is positive, and \\(\\alpha\\) takes on any real value.\nThe parameter \\(\\alpha\\) gives the Weibull distribution different modes of behavior:\n\n\\(\\alpha = 1\\): the hazard rate is constant, in which case the Weibull distribution is actually an exponential distribution with rate \\(\\lambda\\).\n\\(\\alpha &gt; 1\\): the hazard rate increases over time. This depicts a situation of “aging components”, i.e., the longer a components has lived, the higher the instantaneous chance of failure. This is sometimes called Increasing Failure Rate (IFR).\n\\(\\alpha &lt; 1\\): this is an opposite case against \\(\\alpha &gt; 1\\). This is sometimes called Decreasing Failure Rate (DFR).\n\nFor Weibull distribution, we have\n\\[\nF(x) = 1 - \\exp(-\\int_0^x h(t)\\mathrm{d}t)\n\\]\nand\n\\[\nh(x) = \\lambda x^{\\alpha - 1}\n\\]\nThen the CDF and PDF are\n\\[\nF(x) = 1 - e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n\\]\nand\n\\[\nf(x) = \\lambda x^{\\alpha - 1} e^{-\\frac{\\lambda}{\\alpha} x^\\alpha}\n\\]\nNote that in Julia, the distribution is parameterized via\n\\[\nf(x) = \\frac{\\alpha}{\\theta} (\\frac{x}{\\theta})^{\\alpha - 1} e^{-(\\frac{x}{\\theta})^\\alpha} = \\alpha \\theta ^{-\\alpha} x^{\\alpha - 1} e^{-\\theta ^{-\\alpha} x^\\alpha}\n\\]\nwhere the bijection from \\(\\lambda\\) to \\(\\theta\\) is\n\\[\n\\lambda = \\alpha \\theta ^{-\\alpha}\n\\]\nand\n\\[\n\\theta = (\\frac{\\alpha}{\\lambda})^{\\frac{1}{\\alpha}}\n\\]\nIn this case, \\(\\theta\\) is called the scale parameter, and \\(\\alpha\\) is the shape parameter.\n\nusing Distributions, CairoMakie\n\nalphas = [0.5, 1, 1.5]\ngiven_lambda = 2\nx = 0.01:0.01:10\ncolors = [:blue, :red, :green]\n\nactual_lambda(dist::Weibull) = shape(dist) * Distributions.scale(dist)^(-shape(dist))\ntheta(lambda, alpha) = (alpha / lambda)^(1 / alpha)\n\nwb_dists = [Weibull(alpha, theta(given_lambda, alpha)) for alpha in alphas]\n\nhazardA(dist, x) = pdf(dist, x) / ccdf(dist, x)\nhazardB(dist, x) = actual_lambda(dist) * x^(shape(dist) - 1)\n\n# We usually use the hazard rate function to view the Weibull distribution\nhazardsA = [hazardA.(dist, x) for dist in wb_dists]\nhazardsB = [hazardB.(dist, x) for dist in wb_dists]\n\nprintln(\"Maximum difference between two implementations of hazard: \",\n    maximum(maximum.(hazardsA - hazardsB)))\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"Instantaneous failure rate\")\nfor i in 1:length(hazardsA)\n    label = string(\"λ = \", round(actual_lambda(wb_dists[i]), digits=2), \", α = \", round(shape(wb_dists[i]), digits=2))\n    lines!(ax, x, hazardsA[i], color=colors[i], label=label)\nend\nxlims!(ax, 0, 10)\nylims!(ax, 0, 10)\naxislegend(ax)\nfig\n\nMaximum difference between two implementations of hazard: 1.7763568394002505e-15\n\n\n\n\n\n\n\n5.2.6 Normal distribution\nThe normal distribution (also known as Gaussian distribution) is defined by two parameters, \\(\\mu\\) and \\(\\sigma ^2\\), which are the mean and variance respectively.\nThe PDF is\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma ^2}}\n\\]\nThe normal distribution usually comes with the standard form with \\(\\mu = 0\\) and \\(\\sigma ^2 = 1\\). The PDF is\n\\[\nf(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}}\n\\]\nThe CDF of the standard normal distribution (the CDF of the normal distribution is not available as a simple expression) is\n\\[\n\\Phi (u) = \\int_{-\\infty}^u \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} \\mathrm{d}t = \\frac{1}{2} (1 + \\mathrm{erf}(\\frac{x}{\\sqrt{2}})\n\\]\nwhere \\(\\mathrm{erf}(\\cdot)\\) is a mathematical special function, called error function, and defined as\n\\[\n\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2} \\mathrm{d}t\n\\]\nFor any general normal random variable with mean \\(\\mu\\) and variance \\(\\sigma ^2\\), the CDF is available via \\(\\Phi(\\frac{x-\\mu}{\\sigma})\\).\n\nusing Distributions, Calculus, SpecialFunctions, CairoMakie\n\nx = -5:0.01:5\n\n# erf function from the SpecialFunctions package\nphiA(x) = 0.5 * (1 + erf(x / sqrt(2)))  # Calculate Φ(u) using the error function\nphiB(x) = cdf(Normal(), x)  # Calculate Φ(u)\n\nprintln(\"Maximum difference between two CDF implementations: \",\n    maximum(phiA.(x) - phiB.(x)))\n\nnormalDensity(x) = pdf(Normal(), x)\n\n# Calculate the numerical derivatives from the Calculus package\nd0 = normalDensity.(x)\nd1 = derivative.(normalDensity, x)  # We'll know that x = 0 is the unique extremum\nd2 = second_derivative.(normalDensity, x)  # We'll know x=±1 are two inflection points\n\nfig, ax = lines(x, d0, color=:red, label=\"f(x)\")\nlines!(x, d1, color=:blue, label=\"f'(x)\")\nlines!(x, d2, color=:green, label=\"f''(x)\")\naxislegend(ax)\nfig\n\nMaximum difference between two CDF implementations: 1.1102230246251565e-16\n\n\n\n\n\n\n\n5.2.7 Rayleigh distribution\nConsider an exponentially distributed random variable \\(X\\), with rate parameter \\(\\lambda = \\frac{\\sigma ^{-2}}{2}\\), where \\(\\sigma &gt; 0\\).\nLet \\(R = \\sqrt{X}\\), and then we have\n\\[\nF_R(y) = P(R \\le y) = P(\\sqrt{X} \\le y) = P(X \\le y^2) = F_X(y^2) = 1 - exp(-\\frac{y^2}{2\\sigma ^2})\n\\]\nand by differentiating, we get\n\\[\nf_R(y) = \\frac{y}{\\sigma ^2} exp(-\\frac{y^2}{2\\sigma ^2})\n\\]\nwhich is called the PDF of Rayleigh distribution.\nThe mean of a Rayleigh random variable is \\(\\sigma \\sqrt{\\frac{\\pi}{2}}\\).\nAs mentioned before, we have \\(U \\sim U(0, 1) \\xrightarrow{X = -\\frac{1}{\\lambda}\\log U} X \\sim Exp(\\lambda) \\xrightarrow{R=\\sqrt{X}, \\lambda = \\frac{\\sigma ^{-2}}{2}} R \\sim Rl(\\sigma)\\)\nIn addition, if \\(N_1\\) and \\(N_2\\) are two i.i.d. normally distributed random variables, each with \\(\\mu = 0\\) and std. \\(\\sigma\\), then \\(\\tilde{R} = \\sqrt{N_1^2 + N_2^2}\\) is also Rayleigh distributed just as \\(R\\) above.\nTherefore, we have three ways to generate Rayleigh distributed random variables:\n\nusing Distributions, CairoMakie\n\nN = 10^6\nsigma = 1.5\n\n# U(0, 1) ⟶ Exp(λ) ⟶ Rl(σ)\nrlA = sqrt.(-(2 * sigma^2) * log.(rand(N)))\n\n# From two i.i.d. normally distributed random variables\nnormal_dist = Normal(0, sigma)\nrlB = sqrt.(rand(normal_dist, N) .^ 2 + rand(normal_dist, N) .^ 2)\n\nrlC = rand(Rayleigh(sigma), N)\n\nmean.([rlA, rlB, rlC, sigma * sqrt(π / 2)])\n\n4-element Vector{Float64}:\n 1.880909546203193\n 1.8788405402037578\n 1.877567319733627\n 1.8799712059732503\n\n\nA common way to generate normal random variables, called the Box-Muller Transform, is to use the relationship \\(R = \\sqrt{N_1^2 + N_2^2}\\).\nThe relationship between the pair \\((N_1, N_2)\\) and their polar coordinate counterpart \\((\\theta, R)\\) is\n\\[\n\\begin{cases}\n   N_1 = R\\cos(\\theta) \\\\\n   N_2 = R\\sin(\\theta)\n\\end{cases}\n\\]\nwhere \\(\\theta\\) is a uniformly distributed random variable on \\([0, 2\\pi]\\), and \\(R\\) is a Rayleigh distributed random variable with parameter \\(\\sigma\\).\nGiven this, we can first generate \\(\\theta\\) and \\(R\\), and then transform them via the above formula into \\(N_1\\) and \\(N_2\\). Often, \\(N_2\\) is not needed. Hence, in practice, given two independent uniform \\((0, 1)\\) random variables \\(U_1\\) and \\(U_2\\), we set \\(Z = \\sqrt{-2\\sigma ^2 \\log U_1} \\cdot \\cos(2\\pi U_2)\\). Here \\(Z\\) is a normally distributed random variable with \\(\\mu = 0\\) and std. \\(\\sigma\\).\nGenerate \\(N(0, 1)\\):\n\nusing Distributions, CairoMakie\n\nZ(sigma) = sqrt(-2 * sigma * log(rand())) * cos(2 * pi * rand())\n\nfig, ax = hist([Z(1) for _ in 1:10^6], bins=50,\n    normalization=:pdf, label=\"MC estimate\")\nlines!(-4:0.01:4, pdf.(Normal(), -4:0.01:4),\n    label=\"PDF\", color=:red, linewidth=3)\naxislegend(ax)\nfig\n\n\n\n\n\n\n5.2.8 Cauchy distribution\nThe PDF is\n\\[\nf(x) = \\frac{1}{\\pi \\gamma (1 + (\\frac{x - x_0}{\\gamma})^2)}\n\\]\nwhere \\(x_0\\) is the location parameter at which the peak is observed, and \\(\\gamma\\) is the scale parameter.\nNote: the mean and variance are undefined for Cauchy distribution.\n\n\n5.2.9 Summary\n\n\n\n\n\n\n\nA brief summay of univariate distributions"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#multivariate-distributions",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#multivariate-distributions",
    "title": "Probability and statistics with Julia",
    "section": "6 Multivariate distributions",
    "text": "6 Multivariate distributions\nConsider \\(\\mathbf{X} = (X_1, ..., X_n)\\) as a random vector with multiple random variables, defined in the same probability space.\n\n6.1 Covarianve and correlation coefficient\nCovariance: \\(Cov(X, Y) = E[(X-\\mu_X)(Y-\\mu_Y)] = E[XY] - \\mu_X \\mu_Y\\).\nCorrelation coefficient: \\(\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}\\), where \\(-1 \\le \\rho_{XY} \\le 1\\).\nThe correlation coefficient is a normalized covariance standing for the linear correlation relationship between \\(X\\) and \\(Y\\).\n\n\n6.2 Expectation vector and covariance matrix\nConsider a random vector \\(X = [X_1, ..., X_n]^\\top\\):\nThe expectation vector is defined as\n\\[\n\\mu_{\\mathbf{X}} = [E(X_1), ..., E(X_n)]^\\top\n\\]\nThe covariance matrix is defined as\n\\[\n\\Sigma_\\mathbf{X} = Cov(\\mathbf{X}) = E[(\\mathbf{X} - \\mu_\\mathbf{X})(\\mathbf{X} - \\mu_\\mathbf{X})^\\top]\n\\]\nAs can be verified, the \\(i,j\\)-th element of \\(\\Sigma_\\mathbf{X}\\) is \\(Cov(\\mathbf{X}_i, \\mathbf{X}_j)\\), and hence the diagonal elements are the variances.\n\n\n6.3 Affine transformation\nFor any collection of random variables,\n\\[\nE[X_1+ ... + X_n] = E[X_1] + ... + E[X_n]\n\\]\nFor uncorrelated random variables,\n\\[\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i)\n\\]\nMore generally, if we allow the random variables to be correlated, then\n\\[\nVar(X_1 + ... + X_n) = \\sum_{i} Var(X_i) + 2\\sum_{i &lt; j} Cov(X_i, X_j)\n\\]\nObviously, the right-hand side is the sum of the elements of the matrix \\(Cov(\\mathbf{X})\\).\nThe above is a special case of the affine transformation, where we take a random vector \\(\\mathbf{X} = [X_1, ..., X_n]^\\top\\) with covariance matrix \\(\\Sigma_\\mathbf{X}\\), and an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and \\(m\\) vector \\(\\mathbf{b}\\). We then set\n\\[\n\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\n\\]\nThen, the new random vector \\(\\mathbf{Y}\\) has expectation and covariance\n\\[\nE[\\mathbf{Y}] = \\mathbf{A}E[\\mathbf{X}] + \\mathbf{b}\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ Cov(\\mathbf{Y}) = \\mathbf{A}\\Sigma_\\mathbf{X}\\mathbf{A}^\\top\n\\]\nThe above case can be retrieved by setting \\(\\mathbf{A} = [1, ..., 1]\\), and \\(\\mathbf{b} = \\mathbf{0}\\).\n\n\n6.4 The Cholesky decomposition and generating random vectors\nNow we want to create an \\(n\\)-dimensional random vector \\(\\mathbf{Y}\\) with some specified expectation vector \\(\\mu_\\mathbf{Y}\\) and covariance matrix \\(\\Sigma_\\mathbf{Y}\\), which are known.\nFirst, we can generate a random vector \\(\\mathbf{X}\\) with \\(\\mu_\\mathbf{X} = \\mathbf{0}\\) and identity-covariance matrix \\(\\Sigma_\\mathbf{X} = \\mathbf{I}\\) (e.g., a sequence of \\(n\\) i.i.d. N(0, 1) random variables).\nThen, by applying the affine transformation \\(\\mathbf{Y} = \\mathbf{A}\\mathbf{X} + \\mathbf{b}\\), we have \\(\\mu_\\mathbf{Y} = \\mathbf{b}\\) and a matrix \\(\\mathbf{A}\\) which satisfies \\(\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top\\). The Cholesky decomposition will help us get \\(\\mathbf{A}\\) from \\(\\Sigma_\\mathbf{Y} = \\mathbf{A}\\mathbf{A}^\\top\\).\n\nusing Distributions, LinearAlgebra, Random, CairoMakie\n\nRandom.seed!(1)\n\nN = 10^5\n\nmuY = [15; 20]\nSigY = [6 4; 4 9]\n\nA = cholesky(SigY).L  # The Cholesky decomposition; get the lower triangular form\n\nrngGens = [() -&gt; rand(Normal()),\n    () -&gt; rand(Uniform(-sqrt(3), sqrt(3))),\n    () -&gt; rand(Exponential()) - 1]  # Expectation 0; variance 1\n\nlabels = [\"Normal\", \"Uniform\", \"Exponential\"]\ncolors = [:blue, :red, :green]\n\nrv(rng) = A * [rng(), rng()] + muY\n\nds = [[rv(rng) for _ in 1:N] for rng in rngGens]\n\nprintln(\"E1\\tE2\\tVar1\\tVar2\\tCov\")\nfor d in ds\n    println(round(mean(first.(d)), digits=2), \"\\t\", round(mean(last.(d)), digits=2), \"\\t\",\n        round(var(first.(d)), digits=2), \"\\t\", round(var(last.(d)), digits=2), \"\\t\",\n        round(cov(first.(d), last.(d)), digits=2))\nend\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"X_1\",\n    ylabel=L\"X_2\")\nfor i in 1:length(ds)\n    scatter!(ax, first.(ds[i]), last.(ds[i]), color=colors[i], label=labels[i], markersize=2)\nend\naxislegend(ax, position=:rb)\nfig\n\nE1  E2  Var1    Var2    Cov\n15.01   20.01   5.97    9.04    3.99\n15.01   20.0    6.0 8.99    3.98\n15.0    20.0    5.99    8.89    4.0\n\n\n\n\n\n\n\n6.5 Bivariate normal distribution\n\\[\n\\mu_\\mathbf{XY} = \\left[\\begin{matrix} \\mu_\\mathbf{X} \\\\ \\mu_\\mathbf{Y} \\end{matrix}\\right]\n\\]\n\\[\n\\Sigma_\\mathbf{XY} = \\left[\\begin{matrix} \\sigma_\\mathbf{X}^2 & \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho \\\\ \\sigma_\\mathbf{X}\\sigma_\\mathbf{Y}\\rho & \\sigma_\\mathbf{Y}^2\\end{matrix} \\right]\n\\]\n\nusing Distributions, CairoMakie\n\nmeanVect = [27.1554, 26.1638]\ncovMat = [16.1254 13.047; 13.047 12.3673]\n\nbiNorm = MvNormal(meanVect, covMat)  # Multivariate normal distribution\n\nN = 10^3\npoints = rand(biNorm, N)\n\nsupport = 15:0.5:40\nz = [pdf(biNorm, [x, y]) for x in support, y in support]\n\nfig = Figure(size=(900, 400))\nax2 = Axis(fig[1, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nscatter!(ax2, points[1, :], points[2, :], markersize=4, color=:black)\ncontour!(support, support, z, levels=[0.001, 0.005, 0.02], color=:red, linewidth=2)\nax3 = Axis3(fig[1, 2],\n    xlabel=\"x\",\n    ylabel=\"y\",\n    zlabel=\"z\")\nsurface!(support, support, z)\ncolsize!(fig.layout, 1, Auto(0.65))\nfig"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#processing-and-summarizing-data",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#processing-and-summarizing-data",
    "title": "Probability and statistics with Julia",
    "section": "7 Processing and summarizing data",
    "text": "7 Processing and summarizing data\n\n7.1 Processing data\nData cleaning.\n\n\n7.2 Summarizing data\nDescriptive statistics.\n\n7.2.1 Single sample\nGiven a set of observations \\(x_1, x_2, ..., x_n\\).\n\nSample mean: measure of centrality.\n\n\nArithmetic mean:\n\n\\[\n\\bar{x} = \\frac{\\displaystyle\\sum_{i=1}^{n} x_i}{n}\n\\]\n\nGeometric mean:\n\n\\[\n\\bar{x}_g = \\sqrt[n]{\\displaystyle\\prod_{i=1}^n x_i}\n\\]\nUseful for averaging growth factors.\ne.g., if we start with an original base level say \\(L\\) with growths of \\(x_1\\), \\(x_2\\), and \\(x_3\\) in three consecutive periods, then after three periods, we have\n\\[\n\\text{Value after three periods} = L\\cdot x_1\\cdot x_2\\cdot x_3 = L\\cdot \\bar{x}_g^3\n\\]\nHere, the average growth factor is \\(\\bar{x}_g\\).\n\nHarmonic mean:\n\n\\[\n\\bar{x}_h = \\frac{n}{\\displaystyle\\sum_{i=1}^n \\frac{1}{x_i}}\n\\]\nUseful for averaging rates or speeds.\nAssume that you are on a brisk hike, walking \\(5\\) km up a mountain and then \\(5\\) km back down.\nSay your speed going up is \\(x_1 = 5 \\text{ km/h}\\), and your speed going down is \\(x_2 = 10 \\text{ km/h}\\).\nYou travel up for \\(1\\) h, and down for \\(0.5\\) h and hence your total travel time is \\(1.5\\) h.\nWhat is your average speed for the whole journey?\nThe avearge speed shoud be \\(\\frac{10 \\text{ km}}{1.5 \\text{ h}} = 6.6\\bar{6} \\text{ km/h}\\).\nThis is not the arithmetic mean which is \\(\\frac{x_1 + x_2}{2} = \\frac{5 \\text{ km/h}+ 10 \\text{ km/h}}{2} = 7.5 \\text{ km/h}\\) but rather equals the harmonic mean.\n\nVariance: a measure of dispersion.\n\n\nSample variance: the dispersion degree of sample observations away from the sample mean.\n\n\\[\ns^2 = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} = \\frac{\\displaystyle\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}{n-1}\n\\]\nNote that the denominator is \\(n-1\\) instead of \\(n\\), which is the population variance (\\(s^2\\) defined in the above way is an unbiased estimator of the population variance).\n\nSample standard deviation: \\(s = \\sqrt{s^2}\\).\nStandard error: \\(\\frac{s}{\\sqrt{n}}\\) (the dispersion degree of sample mean away from the population mean).\n\nAnother breed of descriptive statistics is based on order statistics. This term is used to describe the sorted sample, denoted by\n\\(x_{(1)} \\le x_{(2)} \\le ... \\le x_{(n)}\\)\nBased on the order statistics, we can define a variety of statistics.\n\nminimum: \\(x_{(1)}\\).\nmaximum: \\(x_{(n)}\\).\nmedian: which in case of \\(n\\) being odd is \\(x_{(\\frac{n+1}{2})}\\); in case of \\(n\\) being even is the arithmetic mean of \\(x_{(\\frac{n}{2})}\\) and \\(x_{(\\frac{n}{2} + 1)}\\). A measure of centrality. It is not influenced by very high or very low measurements.\n\\(\\alpha\\)-quantile: which is \\(x_{(\\widetilde{\\alpha n})}\\), where \\(\\widetilde{\\alpha n}\\) denotes a rounding of \\(\\alpha n\\) to the nearest element of \\(\\{1, ..., n\\}\\).\n\n\\(\\alpha = 0.25\\) and \\(\\alpha = 0.75\\) is called the first quantile and the third quantile, the difference of which is called the inter-quantile range (IQR), which is a measure of dispersion.\n\nrange: \\(x_{(n)} - x_{(1)}\\), which is also a measure of dispersion.\n\nA measure of centrality: mean (arithmetic mean, geometric mean, harmonic mean), median (i.e., \\(0.5\\)-quantile).\nA measure of dispersion: variance (sample variance, sample standard deviation, standard error), IQR, range.\nIn Julia, packages Statistics together with StatsBase is usually used to perform descriptive statistics:\n\nusing Statistics, StatsBase, Distributions\n\nd = rand(Exponential(1 / 2), 10^6)\n\nprintln(\"Sample arithmetic mean, sample geometric mean, sample harmonic mean: \", (mean(d), geomean(d), harmmean(d)))\nprintln(\"Sample variance, sample standard deviation, sample standard error: \", (var(d), std(d), sem(d)))\nprintln(\"Minimum, maximum, range: \", (minimum(d), maximum(d), maximum(d) - minimum(d)))\nprintln(\"95th percentile, 0.95 quantile, IQR: \", (percentile(d, 95), quantile(d, 0.95), iqr(d)), \"\\n\")\n\nsummarystats(d)\n\nSample arithmetic mean, sample geometric mean, sample harmonic mean: (0.4989885578097969, 0.2802727834891644, 0.0040630310498548415)\nSample variance, sample standard deviation, sample standard error: (0.24873470917951312, 0.4987331041544296, 0.0004987331041544296)\nMinimum, maximum, range: (4.750186936682846e-9, 6.627393536148787, 6.6273935313986)\n95th percentile, 0.95 quantile, IQR: (1.4925592077411902, 1.4925592077411889, 0.5482163698428605)\n\n\n\nSummary Stats:\nLength:         1000000\nMissing Count:  0\nMean:           0.498989\nStd. Deviation: 0.498733\nMinimum:        0.000000\n1st Quartile:   0.143779\nMedian:         0.346183\n3rd Quartile:   0.691995\nMaximum:        6.627394\n\n\n\n\n7.2.2 Observations in pairs\nWhen data is configured in the form of pairs, \\((x_1, y_1), ..., (x_n, y_n)\\), we often consider the (1) sample covariance\n\\[\n\\widehat{cov}_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\]\nor its normalized form - (2) correlation coefficient (Pearson correlation coefficient)\n\\[\n\\hat{\\rho}_{x,y} = \\frac{\\widehat{cov}_{x,y}}{s_x s_y}\n\\]\nWe often represent the variances and covariances in the (3) sample covariance matrix\n\\[\n\\hat{\\Sigma} = \\left[ \\begin{matrix} \\widehat{cov}_{x,x} & \\widehat{cov}_{x,y} \\\\ \\widehat{cov}_{x,y} & \\widehat{cov}_{y,y} \\end{matrix} \\right] = \\left[ \\begin{matrix} s_x^2 & \\hat{\\rho}_{x,y} s_x s_y \\\\ \\hat{\\rho}_{x,y} s_x s_y & s_y^2 \\end{matrix} \\right]\n\\]\n\nusing CSV, DataFrames, Statistics\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\n\nx = d.Brisbane\ny = d.GoldCoast\n\ncovXY = cov(x, y)\nsigX = std(x)\nsigY = std(y)\nrhoXY = covXY / (sigX * sigY)\n\nprintln(\"covXY: \", covXY, \"\\n\",\n    \"sigX: \", sigX, \"\\n\",\n    \"sigY: \", sigY, \"\\n\",\n    \"rhoXY: \", rhoXY)\n\nmeanVect = [mean(x), mean(y)]\ncovMat = [sigX^2 covXY\n    covXY sigY^2]\n\nprintln(\"meanVect: \", meanVect)\nprintln(\"covMat: \", covMat)\n\ncovXY: 13.046961200891614\nsigX: 4.015643106449177\nsigY: 3.5167106180015955\nrhoXY: 0.923884392762155\nmeanVect: [27.155405405405407, 26.163835263835264]\ncovMat: [16.1253895583728 13.046961200891614; 13.046961200891614 12.367253570765163]\n\n\n\n\n7.2.3 Observations in vectors\nThe data is represented by an \\(n\\times p\\) matrix, \\(\\mathbf{X}\\), where the rows are observations and the columns are features.\n\\[\n\\mathbf{X} = [\\mathbf{X_1}, ..., \\mathbf{X_p}]\n\\]\n\\(\\mathbf{X_j}\\) represents the \\(j\\)-th feature.\nBasically, we can summarize the data matrix \\(\\mathbf{X}\\) by these statistics:\n\nSample mean vector\n\n\\[\n\\bar{\\mathbf{x}} = [\\bar{x}_1, ..., \\bar{x}_p]^\\top\n\\]\n\nSample standard deviation vector\n\n\\[\n\\mathbf{s} = [s_1, ..., s_p]^\\top\n\\]\nWith these two statistics, we often standardize or normalize the data by creating a new \\(n\\times p\\) matrix \\(\\mathbf{Z}\\) with entries\n\\[\nz_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_j}, i = 1, ..., n,\\ \\ j = 1, ..., p\n\\]\nalso called z-scores.\nThe normalized data has the attribute that each column has a \\(0\\) sample mean and a unit standard deviation. Hence the first- and second-order information of the \\(j\\)-th feature is lost when moving from \\(\\mathbf{X}\\) to \\(\\mathbf{Z}\\).\nIt can be created via\n\\[\n\\mathbf{Z} = (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)diag(\\mathbf{s})^{-1}\n\\]\nwhere \\(diag(\\cdot)\\) creates a diagonal matrix from a vector by using the Diagonal function, and then get the inverse matrix by using the grammar D^-1, both of which are from the LinearAlgebra package.\nIn Julia this can be calculated using the zscore function.\n\nSample covariance matrix\n\n\\[\n\\begin{align}\n\\hat{\\Sigma} & = \\frac{1}{n-1} (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top)^\\top (\\mathbf{X} - \\mathbf{1}\\mathbf{\\bar{x}}^\\top) \\\\\n             & = \\frac{1}{n-1} \\mathbf{X}^\\top (\\mathbf{I} - n^{-1} \\mathbf{1} \\mathbf{1}^\\top) \\mathbf{X}\n\\end{align}\n\\]\n\nSample correlation matrix\n\nThe following only picks two columns called \\(x\\) and \\(y\\) to perform deduction:\n\\[\n\\begin{align}\n\\hat{\\rho}_{x,y} & = \\frac{\\widehat{cov}_{x,y}}{s_x s_y} \\\\\n                 & = \\frac{\\displaystyle\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(n-1) s_x s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n \\frac{x_i - \\bar{x}}{s_x} \\cdot \\frac{y_i - \\bar{y}}{s_y} \\\\\n                 & = \\frac{1}{n-1} \\displaystyle\\sum_{i=1}^n z_{ix} z_{iy} \\\\\n                 & = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}\n\\end{align}\n\\]\nIn julia this can be performed via the cor function.\n\nusing Statistics, StatsBase, LinearAlgebra, DataFrames, CSV\n\ndf = CSV.read(\"./data/3featureData.csv\", DataFrame, header=false)\nprintln(df, \"\\n\")\n\nX = Matrix(df)\nprintln(X, \"\\n\")\n\nn, p = size(X)\n\n# Sample mean vector\nxbarA = X' * ones(n) / n\nxbarB = [mean(X[:, j]) for j in 1:p]\nxbarC = sum(X, dims=1) / n\nprintln(\"Sample mean vector: \", \"\\n\", xbarA, \"\\n\", xbarB, \"\\n\", xbarC, \"\\n\")\n\n# Sample standard deviation vector\nsA = [std(X[:, j]) for j in 1:p]\nsB = std(X, dims=1)\nprintln(\"Sample standard deviation vector: \", \"\\n\", sA, \"\\n\", sB, \"\\n\")\n\nxbar = xbarB\ns = sA\n\n# Z-scores matrix\nZA = [((X[i, j] - mean(X[:, j])) / std(X[:, j])) for i in 1:n, j in 1:p]\nZB = (X - ones(n) * xbar') * Diagonal(s)^-1\nZC = hcat([zscore(X[:, j]) for j in 1:p]...)\nprintln(\"Z-scores matrix: \", \"\\n\", ZA, \"\\n\", ZB, \"\\n\", ZC, \"\\n\")\n\n# Sample covariance matrix\ncovA = (X - ones(n) * xbar')' * (X - ones(n) * xbar') / (n - 1)\ncovB = X' * (I - ones(n, n) / n) * X / (n - 1)\ncovC = [cov(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncovD = [cor(X[:, i], X[:, j]) * std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncovE = cov(X)\nprintln(\"Sample covariance matrix: \", \"\\n\", covA, \"\\n\", covB, \"\\n\", covC, \"\\n\", covD, \"\\n\", covE, \"\\n\")\n\nZMat = ZC\n\n# Sample correlation coefficient matrix\ncorA = cov(X) ./ [std(X[:, i]) * std(X[:, j]) for i in 1:p, j in 1:p]\ncorB = cov(X) ./ (std(X, dims=1)' * std(X, dims=1))\ncorC = [cor(X[:, i], X[:, j]) for i in 1:p, j in 1:p]\ncorD = ZMat' * ZMat / (n - 1)\ncorE = cov(ZMat)\ncorF = cor(X)\nprintln(\"Sample correlation coefficient matrix: \", \"\\n\", corA, \"\\n\", corB, \"\\n\", corC, \"\\n\", corD, \"\\n\", corE, \"\\n\", corF, \"\\n\")\n\n7×3 DataFrame\n Row │ Column1  Column2  Column3 \n     │ Float64  Float64  Float64 \n─────┼───────────────────────────\n   1 │     0.9      2.1      1.2\n   2 │     1.1      1.9      2.5\n   3 │     1.7      1.9      3.4\n   4 │     0.8      2.3      2.3\n   5 │     1.3      1.6      9.4\n   6 │     0.7      2.7      1.3\n   7 │     0.9      2.1      4.4\n\n[0.9 2.1 1.2; 1.1 1.9 2.5; 1.7 1.9 3.4; 0.8 2.3 2.3; 1.3 1.6 9.4; 0.7 2.7 1.3; 0.9 2.1 4.4]\n\nSample mean vector: \n[1.0571428571428572, 2.085714285714286, 3.5]\n[1.0571428571428572, 2.085714285714286, 3.5]\n[1.0571428571428572 2.085714285714286 3.5]\n\nSample standard deviation vector: \n[0.34572215654165056, 0.34846602621858486, 2.834313555930842]\n[0.34572215654165056 0.34846602621858486 2.834313555930842]\n\nZ-scores matrix: \n[-0.45453510621013815 0.04099600308453923 -0.8114839641461745; 0.12396411987549243 -0.5329480400990126 -0.35281911484616285; 1.859461798132383 -0.5329480400990126 -0.03528191148461632; -0.7437847192529532 0.6149400462680898 -0.4233829378153955; 0.7024633459611227 -1.3938641048743392 2.081632777592361; -1.0330343322957687 1.7628281326351936 -0.7762020526615583; -0.45453510621013815 0.04099600308453923 0.31753720336154667]\n[-0.45453510621013815 0.04099600308453923 -0.8114839641461745; 0.12396411987549243 -0.5329480400990126 -0.35281911484616285; 1.859461798132383 -0.5329480400990126 -0.03528191148461632; -0.7437847192529532 0.6149400462680898 -0.4233829378153955; 0.7024633459611227 -1.393864104874339 2.081632777592361; -1.0330343322957687 1.7628281326351936 -0.7762020526615583; -0.45453510621013815 0.04099600308453923 0.31753720336154667]\n[-0.45453510621013815 0.04099600308453923 -0.8114839641461745; 0.12396411987549243 -0.5329480400990126 -0.35281911484616285; 1.859461798132383 -0.5329480400990126 -0.03528191148461632; -0.7437847192529532 0.6149400462680898 -0.4233829378153955; 0.7024633459611227 -1.393864104874339 2.081632777592361; -1.0330343322957687 1.7628281326351936 -0.7762020526615583; -0.45453510621013815 0.04099600308453923 0.31753720336154667]\n\nSample covariance matrix: \n[0.11952380952380953 -0.0873809523809524 0.44; -0.0873809523809524 0.12142857142857146 -0.715; 0.44 -0.715 8.033333333333335]\n[0.11952380952380959 -0.08738095238095228 0.44000000000000034; -0.08738095238095202 0.12142857142857223 -0.7149999999999989; 0.44000000000000034 -0.714999999999999 8.033333333333337]\n[0.11952380952380953 -0.0873809523809524 0.44; -0.0873809523809524 0.12142857142857146 -0.715; 0.44 -0.715 8.033333333333335]\n[0.11952380952380953 -0.0873809523809524 0.44; -0.08738095238095242 0.12142857142857147 -0.7150000000000001; 0.44000000000000006 -0.7150000000000001 8.033333333333335]\n[0.11952380952380953 -0.0873809523809524 0.44; -0.0873809523809524 0.12142857142857146 -0.715; 0.44 -0.715 8.033333333333335]\n\nSample correlation coefficient matrix: \n[1.0 -0.7253191060768939 0.44903228675078916; -0.7253191060768939 0.9999999999999999 -0.7239318847019132; 0.44903228675078916 -0.7239318847019132 1.0]\n[1.0 -0.7253191060768939 0.44903228675078916; -0.7253191060768939 0.9999999999999999 -0.7239318847019132; 0.44903228675078916 -0.7239318847019132 1.0]\n[1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0]\n[1.0 -0.7253191060768939 0.4490322867507892; -0.7253191060768939 0.9999999999999999 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 0.9999999999999999]\n[1.0 -0.7253191060768939 0.4490322867507892; -0.7253191060768939 0.9999999999999998 -0.7239318847019132; 0.4490322867507892 -0.7239318847019132 0.9999999999999998]\n[1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0]\n\n\n\n\n\n\n7.3 Plots for single samples and time series\nHere, we focus on a single collection of observations, \\(x_1, ..., x_n\\).\nIf the observations are obtained by randomly sampling a population, then the order of the observations is inconsequential.\nIf the observations represent measurement over time then we call the data time-series, and in this case, plotting the observations one after another is the standard way for considering temporal patterns in the data.\n\n7.3.1 Histograms\nConsidering frequencies of occurrences.\nFirst denote the support of the observations via \\([l, m]\\), where \\(l\\) is the minimal observation and \\(m\\) is the maximal observation.\nThen the interval \\([l, m]\\) is partitioned into a finite set of bins \\(B_1, ..., B_L\\), and the frequency in each bin is recorded via\n\\[\nf_j = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}{\\{x_i \\in B_j}\\}, \\ \\ \\ \\ \\text{for}\\ j = 1, ..., L\n\\]\nHere \\(\\mathbf{1}\\{\\cdot\\}\\) is \\(1\\) for \\(x_i \\in \\B_j\\), or \\(0\\) if not.\nWe have that \\(\\sum f_j = 1\\), and hence \\(f_i, ..., f_L\\) is a PMF.\nA histogram is then just a visual representation of PMF. One way to plot the frequencies is via a stem plot. However, such a plot would not represent the widths of the bins. Instead we plot \\(h(x)\\) defined as\n\\[\nh(x) = \\sum_{j=1}^L \\frac{f_j}{|B_j|} \\mathbf{1}{\\{x_i \\in B_j}\\}\n\\]\nwhere \\(|B_j|\\) is the width of bin \\(j\\). Hence \\(h(x)\\) is actually a PDF.\nIn a word, calculate frequencies of occurrences in each bin \\(\\implies\\) PMF; further normalized by bin widths \\(\\implies\\) PDF.\n\nusing Distributions, CairoMakie\n\nn = 2000\nd = rand(Normal(), n)\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:purple, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\naxislegend(ax)\nfig\n\n\n\n\n\n\n7.3.2 Density plots and kernel density estimation\nA more modern and visually applealing alternative to histograms is the smoothed histogram, also known as a density plot, often generated via a kernel density estimate.\n\nMixture model\n\nGenerating observations from a mixture model means that we sample from populations made up of heterogeneous sub-populations.\nEach sub-population has its own probability distribution and these are “mixed” in the process of sampling.\nAt first, a latent (un-observed) random variable determines which sub-population is used, and then a sample is taken from that sub-population.\nThat is if the \\(M\\) sub-populations have densities \\(g_1(x), ..., g_M(x)\\) with weights \\(p_1, ..., p_M\\), and \\(\\sum p_i = 1\\), then the density of the mixture is\n\\[\nf(x) = \\sum_{i=1}^M p_i g_i(x)\n\\]\n\nKernel density estimate\n\nGiven a set of observations, \\(x_1, ..., x_n\\), the KDE is the function\n\\[\n\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K(\\frac{x-x_i}{h})\n\\]\nwhere \\(K(\\cdot)\\) is some specified kernel function and \\(h &gt; 0\\) is the bandwidth parameter.\nThe kernel function is a function that satisfies the properties of a PDF. A typical example is the Gaussian kernel\n\\[\nK(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n\\]\nWith such a kernel, the estimate \\(\\hat{f}(x)\\) is a PDF because it is a weighted superposition of scaled kernel fucntions centered about each of the observations.\nA very small bandwidth implies that the density\n\\[\n\\frac{1}{h} K(\\frac{x-x_i}{h})\n\\]\nis very concentrated around \\(x_i\\).\nFor any value of \\(h\\), it can be proved under general conditions that if the data is distributed according to some density \\(f(\\cdot)\\), then \\(\\hat{f}(\\cdot)\\) converges to \\(f(\\cdot)\\) when the sample size grows.\n\nusing Distributions, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\nmixRV(p) = (rand() &lt;= p) ? rand(dist1) : rand(dist2)\n\nn = 2000\nd = [mixRV(0.3) for _ in 1:n]\n\n# PMF\nfig, ax = hist(d, bins=20, normalization=:probability, color=:skyblue, label=\"PMF\")\n# PDF\nstephist!(ax, d, bins=20, normalization=:pdf, color=:red, label=\"PDF\")\n# Smoothed PDF\ndensity!(ax, d, color=(:white, 0), label=\"Smoothed PDF\", strokecolor=:green, strokewidth=2)\naxislegend(ax)\nfig\n\n\n\n\nIn a word, the KDE is a useful way to estimate the PDF of the unknown underlying distribution given some sample data.\n\n\n7.3.3 Empirical cumulative distribution function\nThe Empirical Cumulative Distribution Function (ECDF) can be viewed as an estimate of the underlying CDF.\nIn contrast to histograms and KDEs, ECDFs provide an unique representation of the data independent of tuning parameters.\nThe ECDF is a stepped function which, given \\(n\\) data points, increases by \\(\\frac{1}{n}\\) at each point.\nMathematically, given the sample, \\(x_1, ..., x_n\\), the ECDF is given by\n\\[\n\\hat{F}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1} \\{x_i \\le t\\}\\ \\ \\ \\ \\text{where }\\mathbf{1}\\text{ is the indicator function}\n\\]\nIn the case of i.i.d. data from an underlying distribution with CDF \\(F(\\cdot)\\), the Glivenko-Cantelli theorem ensures that the ECDF \\(\\hat{F}(\\cdot)\\) approaches \\(F(\\cdot)\\) as the sample size grows.\n\nusing Distributions, StatsBase, CairoMakie\n\nmu1, sigma1 = 10, 5\nmu2, sigma2 = 40, 12\n\ndist1, dist2 = Normal(mu1, sigma1), Normal(mu2, sigma2)\n\np = 0.3\n\nmixRV(p) = (rand() &lt;= p) ? rand(dist1) : rand(dist2)\nmixCDF(x) = p * cdf(dist1, x) + (1 - p) * cdf(dist2, x)\n\nn = [30, 100]\n\ndata1 = [mixRV(p) for _ in 1:n[1]]\ndata2 = [mixRV(p) for _ in 1:n[2]]\n\nempiricalCDF1 = ecdf(data1)\nempiricalCDF2 = ecdf(data2)\n\nx = -10:0.1:80\n\nfig, ax = lines(x, empiricalCDF1, label=\"ECDF with n = $(n[1])\")\nlines!(x, empiricalCDF2, label=\"ECDF with n = $(n[2])\")\nlines!(x, mixCDF.(x), label=\"Underlying CDF\")\naxislegend(ax, position=:lt)\nfig\n\n\n\n\n\n\n7.3.4 Normal probability plot\nSee Section 7.4.1 for details.\n\n\n7.3.5 Visualizing time series\nIn cases where the time-series data appears to be stationary (a stationary sequence is one in which the distributional law of observations doesn’t depend on the exact time. This means that there isn’t an apparent trend nor a cyclic component.), then a histogram is immediately insightful; otherwise, plotting data points one after the other along the time axis is necessary.\n\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndiff = brisbane - goldcoast\ndates = string.([Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n                 for i in 1:nrow(d)])\n\nfortnight_range = 250:263\ndate_fortnight = dates[fortnight_range]\nbris_fortnight = brisbane[fortnight_range]\ngold_fortnight = goldcoast[fortnight_range]\n\nfig = Figure(size=(1100, 900))\n\nax1_slice_indexes = [1, 389, 777]\nax1 = Axis(fig[1, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax1_slice_indexes, dates[ax1_slice_indexes]))\nseries!(ax1, stack(zip(brisbane, goldcoast)))\naxislegend(ax1, position=:rb)\n\nax2_slice_indexes = [1, 7, 14]\nax2 = Axis(fig[2, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature\",\n    xticks=(ax2_slice_indexes, date_fortnight[ax2_slice_indexes]))\nseries!(ax2, stack(zip(bris_fortnight, gold_fortnight)))\nscatter!(1:length(date_fortnight), bris_fortnight)\nscatter!(1:length(date_fortnight), gold_fortnight)\naxislegend(ax2, position=:lb)\n\nax3_slice_indexes = [1, 389, 777]\nax3 = Axis(fig[3, 1],\n    xlabel=\"Time\",\n    ylabel=\"Temperature Difference\",\n    xticks=(ax3_slice_indexes, dates[ax3_slice_indexes]))\nseries!(ax3, reshape(diff, 1, length(diff)))\n\nax4 = Axis(fig[4, 1],\n    xlabel=\"Temperature Difference\",\n    ylabel=\"Frequency\")\nhist!(ax4, diff, bins=50)\n\nfig\n\n\n\n\n\n\n7.3.6 Radial plot\nRadial plot is useful for presenting time-series or cyclic data.\nA variation of radial plot is the radar plot, which is often used to visualize the levels of different categorical variables on the one plot.\n\nusing DataFrames, CSV, Dates, CairoMakie\n\nd = CSV.read(\"./data/temperatures.csv\", DataFrame)\nsubset!(d, :Year =&gt; x -&gt; x .== 2015)\nbrisbane = d.Brisbane\ngoldcoast = d.GoldCoast\n\ndates = [Date(Year(d.Year[i]),\n    Month(d.Month[i]),\n    Day(d.Day[i]))\n         for i in 1:nrow(d)]\n\nx = 0:2pi/(length(brisbane)-1):2pi |&gt; collect\nax_slice_indexes = [findfirst(Dates.month.(dates) .== m) for m in 1:12]\n\nfig = Figure(size=(600, 600))\nax = PolarAxis(fig[1, 1],\n    thetaticks=(x[ax_slice_indexes], Dates.monthabbr.(1:12)))\nseries!(ax, x, [brisbane goldcoast]')\n\nfig\n\n\n\n\n\n\n\n7.4 Plots for comparing two or more samples\n\n7.4.1 Quantile-Quantile (Q-Q) plot\nThe Q-Q plot checks if the distributional shape of two samples is the same or not.\nFor this plot, we require that the sample sizes are the same.\nThen the ranked quantiles of the first sample are plotted against the ranked quantiles of the second sample.\nIn the case where the samples have a similar distributional shape, the resulting plot appears like a collection of increasing points along a straight line.\n具体原理解释如下：\n给定一列数据 \\(x_1, ..., x_n\\)，假定其服从正态分布。现取一个正态分布作为模板，将其 PDF 下的面积等分成 \\(n\\) 份，即每一块区域代表的概率都是相等的，都是 \\(\\frac{1}{n}\\)。如果现在要从这个正态分布中抽取一个随机数，在理想情况下，这个数出现在任何一个小区域内的概率都是相等的。也就是说，在该正态分布被分成 \\(n\\) 等份后，如果我们要从其中抽出 \\(n\\) 个随机数，在理想情况下，应该是刚好每个小区间都被抽出了一个数，并且我们预期这些数应该是每个小区间的中位数（二分位数）。\n在下图中，\\(n = 10\\)：\n\nusing Distributions, CairoMakie\n\nd = Normal()\n\nn = 10\nx = -4:0.01:4\ny = pdf.(d, x)\nq = quantile.(d, collect(1/n:1/n:(n-1)/n))\n\nfig, ax = lines(x, y, color=:black)\nvlines!(ax, q, color=:red)\nfig\n\n\n\n\n对于 \\(n = 10\\) 来说，累积概率分位数分隔点分别为 \\(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\)（\\(\\frac{1}{n}, \\frac{2}{n}, ..., \\frac{n-1}{n}\\)），对应的每个小区间的累积概率二分位数应为 \\(0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95\\)（\\(\\frac{i-0.5}{n}\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n\\)），再利用公式 \\(\\Phi^{-1}\\left(\\frac{i-0.5}{n}\\right)\\ \\ \\ \\ \\text{for}\\ i = 1,2, .., n\\) 得到每个小区间相应的二分位数值。\n在理想情况下，排完序的实际观测值 \\(x_1, ..., x_n\\) 应该和上述 \\(n\\) 个二分位数值一致，即以实际值作为纵轴，理论值作为横轴，画出的这些点应该位于斜线 \\(y = x\\) 上。\n值得注意的是，取理论分位数值这一步有很多方法，除了等分概率分布取二分位数之外，也有直接将概率分布等分为 \\(n+1\\) 份，直接取对应的 \\(n\\) 个分位数即可。\n\nusing Random, Distributions, CairoMakie, Statistics\n\nRandom.seed!(1234)\n\nn = 2000\nmu, sigma = 10, 1\nrank = collect(1:2000)\n\nd = Normal(mu, sigma)\n\nempirical_data = rand(d, n) |&gt; sort\ntheoretical_data = quantile.(Normal(), @. (rank - 0.5) / n)\n\n# x = σU + μ\nfig, ax = scatter(theoretical_data, empirical_data, color=:steelblue)\nablines!(ax, mu, sigma, color=:red)\nqqnorm!(Axis(fig[2, 1]), empirical_data, qqline=:fitrobust, color=:red, markercolor=:steelblue)\nfig\n\n\n\n\nIf you want roughly to see if the two given data sets \\(x_1, ..., x_n\\), and \\(y_1, ..., y_n\\) have exactly the same distributional shape, you can just do the following:\n\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1)\n\n# Plot the ranked quantiles of x against the ranked quantiles of y\nx = randn(2000) |&gt; sort\ny = randn(2000) |&gt; sort\n\nfig, ax = scatter(x, y, color=:steelblue)\nablines!(0, 1, color=:red)\nqqplot!(Axis(fig[2, 1]), x, y, qqline=:identity, color=:red, markercolor=:steelblue)\nfig\n\n\n\n\n\n\n7.4.2 Box plot\nThe box plot, also known as a box and whisker plot, which displays the first and the third quantiles along with the median. The location of the whiskers is typically given by\n\\[\n\\text{minimum} = Q1 - 1.5 IQR\\ \\ \\text{,}\\ \\ \\text{maximum} = Q3 + 1.5 IQR\n\\]\nwhere IQR is the inter-quantile range. Observations that lie outside this range are called outliers.\n\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\n# notch is used to test the significance of the difference between two medians under the 0.95 confidence interval\nboxplot(categories, v, show_notch=true, color=:cyan)\n\n\n\n\n\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nboxplot(categories, v, dodge=dodge, show_notch=true, color=map(d -&gt; d == 1 ? :cyan : :magenta, dodge))\n\n\n\n\n\n\n7.4.3 Violin plot\nIt is similar to the box plot, however, the shape of each sample is represented by a mirrored kernel density estimate of the data.\n\nusing CairoMakie\n\ncategories = repeat(1:3, outer=300)\nv = randn(900)\n\nviolin(categories, v, color=:cyan, datalimits=extrema)\n\n\n\n\n\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\ndodge = repeat(repeat(1:2, outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, dodge=dodge, color=map(d -&gt; d == 1 ? :cyan : :magenta, dodge), datalimits=extrema)\n\n\n\n\n\nusing CairoMakie\n\ncategories = repeat(1:3, inner=800)\nside = repeat(repeat([:left, :right], outer=3), inner=400)\nv = randn(2400)\n\nviolin(categories, v, side=side, color=map(d -&gt; d == :left ? :cyan : :magenta, side), datalimits=extrema)\n\n\n\n\n\n\n\n7.5 Plots for multivariate and high-dimensional data\nFor vectors of observations, \\((x_{11}, ..., x_{1p}), ..., (x_{n1}, ..., x_{np})\\), where \\(n\\) is the number of observations and \\(p\\) is the number of variables, or features. In case where \\(p\\) is large the data is called high dimensional.\n\n7.5.1 Scatter plot matrix\nIt consists of taking each possible pair of variables and plotting a scatter plot for that pair.\nObviously, with \\(p\\) variables, we need at least \\(\\frac{p^2-p}{2}\\) scatters.\n\nusing RDatasets, AlgebraOfGraphics, DataFrames, CairoMakie\n\ndf = dataset(\"datasets\", \"iris\")\n\nfeature_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\", \"Species\"]\n\nrename!(df, feature_names)\n\nfig = Figure(size=(1200, 1200))\nfor i in 1:4\n    for j in 1:4\n        scatter = data(df) * mapping(feature_names[i], feature_names[j], color=feature_names[5]) * visual(Scatter)\n        ax_scatter = Axis(fig[i, j],\n            xlabel=feature_names[i],\n            ylabel=feature_names[j])\n        grid = draw!(ax_scatter, scatter)\n        if i == 1 && j == 1\n            legend!(fig[i, j], grid; tellheight=false, tellwidth=false, halign=:left, valign=:top)\n        end\n    end\nend\nfig\n\n\n\n\n\n\n7.5.2 Heat map with marginals\nIn cases of pairs of observations \\((x_1, y_1), ..., (x_n, y_n)\\), the bivariate data can be constructed into a bivariate histogram (shown in the form of heat map in the 2D plane) in a manner similar to the univariate histogram. In addition, we can also add two marginal histograms beside the heat map, which are two separate histograms, one for \\(x_1, ..., x_n\\), and the other for \\(y_1, ..., y_n\\).\n\nusing Distributions, DataFrames, AlgebraOfGraphics, CairoMakie\n\nN = 10^6\nmeanVect = [27, 26]\ncovMat = [16 13; 13 12]\nbiNorm = MvNormal(meanVect, covMat)\nsimData = DataFrame(rand(biNorm, N)', [:x, :y])\n\nfig = Figure(size=(600, 600))\ngl = fig[1, 1] = GridLayout()\n\nax_x = Axis(gl[1, 1])\nhist!(ax_x, simData[!, :x], bins=50, normalization=:pdf)\n\nax_y = Axis(gl[2, 2])\nhist!(ax_y, simData[!, :y], bins=50, normalization=:pdf, direction=:x)\n\nfor ax in [ax_x, ax_y]\n    hidedecorations!(ax)\n    hidespines!(ax)\nend\n\nax_hm = Axis(gl[2, 1],\n    xlabel=\"x\",\n    ylabel=\"y\")\nhm = data(simData) * mapping(:x, :y) * AlgebraOfGraphics.density(npoints=50)\ngrid = draw!(ax_hm, hm)\ncolorbar!(gl[3, 1], grid; tellheight=true, tellwidth=true, vertical=false, flipaxis=false)\n\ncolgap!(gl, 0)\nrowgap!(gl, 0)\ncolsize!(gl, 2, Auto(0.25))\nrowsize!(gl, 1, Auto(0.25))\n\nfig\n\n\n\n\n\n\n\n7.6 Andrews plot\nThe idea of Andrews plot is to represent a data vector \\((x_{i1}, ..., x_{ip})\\) via a real-valued function. For any individual vector, such a transformation cannot be generally useful; however, when comparing groups of vectors, it may yield a way to visualize structural differences in the data.\nThe specific transformation rule that we present here creates a plot known as Andrews plot.\nHere, for the \\(i\\)-th data vector \\((x_{i1}, ..., x_{ip})\\), we create the function \\(f_i(\\cdot)\\) defined on \\([-\\pi, \\pi]\\) via,\n\\[\nf_i(t) = \\frac{x_{i1}}{\\sqrt{2}} + x_{i2}\\sin(t) + x_{i3}\\cos(t) + x_{i4}\\sin(2t) + x_{i5}\\cos(2t) + x_{i6}\\sin(3t) + x_{i7}\\cos(3t) + \\cdots\n\\]\nwith the last term involving a \\(\\sin()\\) if \\(p\\) is even and a \\(\\cos()\\) is \\(p\\) is odd. For \\(i = 1, ..., n\\), the functions \\(f_1(\\cdot), ..., f_n(\\cdot)\\) are plotted.\nIn cases where each \\(i\\) has an associated label from a small finite set, different colors or line patterns can be used.\n\nusing RDatasets, AlgebraOfGraphics, DataFrames, StatsBase, CairoMakie\n\nfunction gen_uni_str(n::Int; exclude_strs::Vector{String}=String[], iter_n::Int=1000)\n    alphabet = [collect('a':'z'); collect('A':'Z')]\n    num_underscore = [collect('0':'9'); \"_\"]\n\n    for i in 1:iter_n\n        uni_str = join([rand(alphabet, 1); rand([alphabet; num_underscore], n - 1)])\n        if uni_str .∉ Ref(exclude_strs)\n            return uni_str\n        end\n    end\n    error(\"cannot generate an unique string against the given arguments\")\nend\n\nfunction andrewsplot(df::DataFrame, features::Vector{String}; npoints::Int=100, scale::Bool=true)\n    if nrow(df) &lt; 1 || length(features) &lt; 1\n        error(\"both the data frame and features must have at least 1 element\")\n    end\n\n    if npoints &lt; 1\n        error(\"the npoints must be an integer greater than 0\")\n    end\n\n    tmp_df = transform(df, eachindex =&gt; \"row_number\")\n    transform!(tmp_df, :row_number =&gt; (x -&gt; string.(x)) =&gt; :row_number)\n    n_vars = length(features)\n\n    if scale\n        # scale each column to mean 0 and std 1\n        # to ensure that all features contribute equally to the shape of the curve\n        scaled_df = DataFrame(hcat([zscore(tmp_df[!, j]) for j in features]...), features)\n    else\n        scaled_df = tmp_df[!, features]\n    end\n\n    if iseven(n_vars)\n        placeholder_column_name = gen_uni_str(12; exclude_strs=features)\n        scaled_df[!, placeholder_column_name] = zeros(nrow(scaled_df))\n        n_vars = n_vars + 1\n    end\n\n    fvs = Vector{Float64}(undef, nrow(scaled_df) * npoints)\n    fvs_index_pairs = [[(i - 1) * npoints + 1, min(i * npoints, length(fvs))] for i in 1:Int(ceil(length(fvs) / npoints))]\n    ob_index_pairs = [[(i - 1) * 2 + 1, min(i * 2, n_vars - 1)] for i in 1:Int(ceil((n_vars - 1) / 2))]\n    f_range = collect(range(-π, π; length=npoints))\n    for i in 1:nrow(scaled_df)\n        ob = Vector(scaled_df[i, :])\n        f_it0 = popfirst!(ob) / √2\n        for j in eachindex(f_range)\n            t = f_range[j]\n            f_it = f_it0\n            for multiplier in 1:length(ob_index_pairs)\n                x1, x2 = ob[ob_index_pairs[multiplier]]\n                f_it = f_it + x1 * sin(multiplier * t) + x2 * cos(multiplier * t)\n            end\n            fvs[fvs_index_pairs[i][1]+j-1] = f_it\n        end\n    end\n\n    fvs_df = DataFrame(andrew_plot_x=repeat(f_range; outer=nrow(scaled_df)),\n        andrew_plot_y=fvs,\n        row_number=repeat(1:nrow(scaled_df); inner=npoints))\n    transform!(fvs_df, :row_number =&gt; (x -&gt; string.(x)) =&gt; :row_number)\n    return innerjoin(tmp_df, fvs_df; on=:row_number, renamecols=\"_raw\" =&gt; \"_new\")\nend\n\niris = dataset(\"datasets\", \"iris\")\nfeatures = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\ndf = andrewsplot(iris, features; scale=false)\np = data(df) * mapping(:andrew_plot_x_new, :andrew_plot_y_new; group=:row_number, color=:Species_raw) * visual(Lines)\ndraw(p; figure=(size=(800, 500),))\n\n\n\n\n\n\n7.7 Plots for the board room\n\n7.7.1 Pie chart\nUsed to convey relative proportions.\n\nusing CairoMakie\n\nd = [36, 12, 68, 5, 42, 27]\ncolors = [:yellow, :orange, :red, :blue, :purple, :green]\n\npie(d,\n    color=colors,\n    radius=4,  # the radius of the pie plot\n    inner_radius=2,  # the inner radius between 0 and radius to create a donut chart\n    strokecolor=:white,\n    strokewidth=5,\n    axis=(autolimitaspect=1,),\n)\n\n\n\n\nNote: introduction to two Axis() parameters:\n\naspect=nothing: defined as the axis aspect ratio of the width over height.\n\nThis will change the size of the axis.\nIf you set it to DataAspect(), the axis aspect ratio width/heigth will matches that of the data limits.\nFor example, if the x limits range from 0 to 300 and the y limits from 100 to 250, then DataAspect() will result in an aspect ratio of (300 - 0) / (250 - 100) = 2. This can be useful when plotting images, because the image will be displayed unsquished.\nAxisAspect(ratio) reduces the effective axis size within the available layout space so that the axis aspect ratio width/height matches ratio.\n\nautolimitaspect=nothing: the ratio of the limits to the axis size equals that number.\n\nFor example, if the axis size is \\(100\\times 200\\), then with autolimitaspect=1, the autolimits will also have a ratio of 1 to 2.\n\nusing CairoMakie\n\npie([π / 2, 2π / 3, π / 4],\n    normalize=false,\n    offset=π / 2,\n    color=[:orange, :purple, :green],\n    axis=(autolimitaspect=1,),\n)\n\n\n\n\n\n\n7.7.2 Bar plot\nUsed to convey relative proportions.\n\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"]; levels=[\"C\", \"B\", \"A\"])\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, stack=:Type) * visual(BarPlot)\ndraw(p)\n\n\n\n\n\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\n\np = data(df) * mapping(:Year, :MarketCap; color=:Type, dodge=:Type) * visual(BarPlot)\ndraw(p)\n\n\n\n\n\n\n7.7.3 Stack plot\nShow how constituent amounts of a metric change over time.\n\nusing CSV, DataFrames, AlgebraOfGraphics, CairoMakie, CategoricalArrays\n\nfunction areaplot(df::DataFrame, x::AbstractString, y::AbstractString, group::AbstractString)\n    if nrow(df) == 0\n        error(\"the data frame is empty\")\n    end\n\n    tmp_df = groupby(df[:, [x, y, group]], group)\n    final_df = DataFrame([[], [], [], []], [x, y, group, \"row_number\"])\n    for i in 1:length(tmp_df)\n        sort!(tmp_df[i], x; rev=false)\n        transform!(tmp_df[i], eachindex =&gt; :row_number)\n        if i == 1\n            sub_tmp_df = copy(tmp_df[i])\n            sub_tmp_df[!, y] = repeat([0], nrow(sub_tmp_df))\n        else\n            sub_tmp_df = copy(tmp_df[i-1])\n            sub_tmp_df[!, group] = tmp_df[i][:, group]\n            tmp_df[i][!, y] = tmp_df[i][!, y] .+ sub_tmp_df[!, y]\n        end\n        final_df = vcat(final_df, sort(sub_tmp_df, x; rev=false), sort(tmp_df[i], x; rev=true))\n    end\n    transform!(final_df, Cols(:row_number, y) =&gt; ByRow(((x, y) -&gt; (x, y))) =&gt; :Point)\n\n    return final_df\nend\n\ndf = CSV.read(\"./data/companyData.csv\", DataFrame)\ndf[!, \"Year\"] = categorical(df[!, \"Year\"])\ndf[!, \"Type\"] = categorical(df[!, \"Type\"])\nx, y, group = \"Year\", \"MarketCap\", \"Type\"\nfinal_df = areaplot(df, x, y, group)\n\np = data(final_df) * mapping(:Point; color=:Type) * visual(Poly)\ndraw(p)\n\n\n\n\n\n\n\n7.8 Working with files and remote servers"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#statistical-inference-concepts",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#statistical-inference-concepts",
    "title": "Probability and statistics with Julia",
    "section": "8 Statistical inference concepts",
    "text": "8 Statistical inference concepts\nThe statistical inference concepts involve using mathematical techniques to make conclusions about unkown population parameters based on collected data.\nThe analyses and methods of statistical inference can be categorized into:\n\nFrequentist (classical): based on the assumption that population parameters of some underlying distribution, or probability law, exist and are fixed, but are yet unknown. The process of statistical inference then deals with making conclusions about these parameters based on sampled data.\nBayesian: only assumes that there is a prior distribution of the parameters. The key process deals with analyzing a posterior distribtution of the parameters.\nMachine learning.\n\nIn general, a statistical inference process involves data, model, and analysis. The data is assumed to be comprised of random samples from the model. The goal of the analysis is to make informed statements about population parameters of the model based on the data.\nSuch statements typically take one of the following forms:\n\nPoint estimation: determination of a single value (or vector of values) representing a best estimate of the parameter/parameters.\nConfidence intervals: determination of a range of values where the parameter lies. Under the model and the statistical process used, it is guaranteed that the parameter lies within this range with a pre-specified probability.\nHypothesis tests: the process of determining if the parameter lies in a given region, in the complement of that region, or fails to take on a specific value.\n\n\n8.1 A random sample\nWhen carrying out frequentist statistical inference, we assume that there is some underlying distribution \\(F(x; \\theta)\\) from which we are sampling, where \\(\\theta\\) is the scalar or vector-valued unknown parameter we wish to know.\nWe assume that each observation is statistically independent and identically distributed as the rest. That is, from a probablistic perspective, the observations are taken as independent and identically distributed (i.i.d) random variables. In mathematical statistics, this is called a random sample. We denote the random variables of the observations by \\(X_1, ..., X_n\\), and their respective values by \\(x_1, ..., x_n\\).\nTypically, we compute statistics from the random sample, such as the sample mean and sample variance. We can consider each observation as a random variable, so these statistics are random variables too.\n\\[\n\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X-\\overline{X})^2\n\\]\nFor \\(S^2\\), we use \\(n-1\\), which makes \\(S^2\\) an unbiased estimator of the population variance.\nHere, we consider the sample statistics, such as the sample mean and sample variance, as random variables. This means that these statistics also subject to some underlying distributions. To know what distribution each statistics subject to is the first step to do statistical inference.\n\n\n8.2 Sampling from a normal population\nWe often assume that the distribution we sample from is a normal distribution (i.e. \\(F(x; (\\mu, \\sigma^2))\\)).\nUnder the normality assumption, the distribution of the random variables \\(\\overline{X}\\) and \\(S^2\\) as well as transformations of them are well known:\n\\[\n\\begin{align}\n\\overline{X} &\\backsim N(\\mu, \\frac{\\sigma^2}{n}) \\\\\n\\frac{(n-1)S^2}{\\sigma^2} &\\backsim \\chi^2_{n-1} \\\\\nT := \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} &\\backsim t_{n-1}\n\\end{align}\n\\]\nThe notations \\(\\chi^2_{n-1}\\) and \\(t_{n-1}\\) denote a chi-squared distribution and a student T-distribution, respectively.\n\n\n8.3 Independence of the sample mean and sample variance\nIn many cases, the sample mean and sample variance calculated from the same sample group are not independent, but in the special case where the samples \\(X_1, ..., X_n\\) are from a normal distribution, independence between \\(\\overline{X}\\) and \\(S^2\\) holds. In fact, this property characetrizes the normal distribution - that is, this property only holds for the normal distribution.\n\nusing Distributions, CairoMakie, Random, DataFrames\n\nRandom.seed!(1234)\n\nfunction mean_var(dist, n)\n    sample = rand(dist, n)\n    (mean(sample), var(sample))\nend\n\nuni_dist = Uniform(-sqrt(3), sqrt(3))\nn, N = 3, 10^5\n\n# the sample mean and sample variance are calculated from the same sample group\n# so the two are not independent\ndata_uni = DataFrame([mean_var(uni_dist, n) for _ in 1:N], [:mean, :var])\n# the sample mean and sample variance are calculated from two different sample groups\n# so the two are independent\ndata_uni_ind = DataFrame([(mean(rand(uni_dist, n)), var(rand(uni_dist, n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_uni.mean, data_uni.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_uni_ind.mean, data_uni_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Uniform Distribution\"\naxislegend(ax)\nfig\n\n\n\n\n\n# in the case where we sample from the normal distribution\n# the sample mean and sample variance are always independent\n# independent of the way we calculate them i.e., from the same sample group or from two different sample groups\ndata_norm = DataFrame([mean_var(Normal(), n) for _ in 1:N], [:mean, :var])\ndata_norm_ind = DataFrame([(mean(rand(Normal(), n)), var(rand(Normal(), n))) for _ in 1:N], [:mean, :var])\n\nfig, ax = scatter(data_norm.mean, data_norm.var; color=:blue, label=\"Same group\", markersize=2)\nscatter!(ax, data_norm_ind.mean, data_norm_ind.var; color=:orange, label=\"Separate group\", markersize=2)\nax.xlabel = L\"\\overline{X}\"\nax.ylabel = L\"S^2\"\nax.title = \"Normal Distribution\"\naxislegend(ax)\nfig\n\n\n\n\n\n\n8.4 T-Distribution\nThe random variable T-statistic is given by\n\\[\nT = \\frac{\\overline{X}-\\mu}{S/\\sqrt{n}} \\backsim t_{n-1}\n\\]\nDenoting the mean and variance of the normally distributed observations by \\(\\mu\\) and \\(\\sigma^2\\), respectively, we can represent the T-statistic as\n\\[\nT = \\frac{\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{(n-1)S^2}{\\sigma^2}\\frac{1}{n-1}}} = \\frac{Z}{\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}}\n\\]\nHere, the numerator \\(Z\\) is a standard normal random variable, and in the denominator the random variable \\(\\chi^2_{n-1} = (n-1)S^2/\\sigma^2\\) is chi-distributed wit \\(n-1\\) degrees of freedom. Furthermore, the numerator and denominator random variables are independent because they are based on the sample mean and sample variance, respectively.\nHence, \\(T \\backsim t(n-1)\\), which means a “T-Distribution with \\(n-1\\) degrees of freedom”.\nHere, we check the above fact that T-statistic is derived from two independent random variables (the numerator is a standard normal random variable, while the denominator is a random variable chi-distributed with \\(n-1\\) degrees of freedom):\n\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction tStat(degree)\n    z = rand(Normal())\n    c = rand(Chisq(degree))\n    z / sqrt(c / degree)\nend\n\nn, N = 10, 10^6\n\nsimulationTStats = [tStat(n - 1) for _ in 1:N]\n\nxGrid = -5:0.01:5\n\nfig, ax = stephist(simulationTStats; bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, xGrid, pdf.(TDist(n - 1), xGrid); color=:red, label=\"Analytical\")\nax.limits = (first(xGrid), last(xGrid), nothing, nothing)\nfig\n\n\n\n\nA T-Distribution with \\(k\\) degrees of freedom can be shown to have a density function,\n\\[\nf(x) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{k\\pi} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}\n\\]\nNote that \\(E(\\chi^2_{n-1}) = n-1\\) and \\(Var(\\chi^2_{n-1}) = 2(n-1)\\), so \\(E\\left(\\frac{\\chi^2_{n-1}}{n-1}\\right) = 1\\), and \\(Var(\\frac{\\chi^2_{n-1}}{n-1}) = \\frac{2}{n-1}\\).\nTherefore, we have \\(\\frac{\\chi^2_{n-1}}{n-1} \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\), with the same holding for \\(\\sqrt{\\frac{\\chi^2_{n-1}}{n-1}}\\).\nHence, for large \\(n\\), the distribution of \\(T\\) will converge to the distribution of \\(Z\\).\n\nusing Distributions, Random, CairoMakie, DataFrames\n\nRandom.seed!(1234)\n\nn, N, alpha = 3, 10^7, 0.1\n\nmyT(n) = rand(Normal()) / sqrt(rand(Chisq(n - 1)) / (n - 1))\nmcQuantile = quantile([myT(n) for _ in 1:N], alpha)\nanalyticQuantile = quantile(TDist(n - 1), alpha)\n\nprintln(\"Quantile from Monte Carlo: \", mcQuantile)\nprintln(\"Analytic quantile: \", analyticQuantile)\n\nxGrid = -5:0.1:5\n\nfig = Figure()\nax = fig[1, 1] = Axis(fig)\n\nlines!(ax, xGrid, pdf.(Normal(), xGrid), label=\"Normal\", color=:red)\nscatter!(ax, xGrid, pdf.(TDist(1), xGrid), label=\"DOF = 1\", color=:blue)\nscatter!(ax, xGrid, pdf.(TDist(5), xGrid), label=\"DOF = 5\", color=:purple)\nscatter!(ax, xGrid, pdf.(TDist(10), xGrid), label=\"DOF = 10\", color=:orange)\nscatter!(ax, xGrid, pdf.(TDist(100), xGrid), label=\"DOF = 100\", color=:green)\n\naxislegend(ax)\n\nfig\n\nQuantile from Monte Carlo: -1.8845517968939285\nAnalytic quantile: -1.8856180831641263\n\n\n\n\n\n\n\n8.5 Two samples and the F-Distribution\nMany statistical procedures involve the ratio of sample variances, or similar quantities, for two or more samples.\nFor example, if \\(X_1, ..., X_{n_1}\\) is one sample, and \\(Y_1, ..., Y_{n_2}\\) is another sample, and both samples are distributed normally with the same parameters, then the ratio of the two sample variances\n\\[\nF = \\frac{S_X^2}{S_Y^2}\n\\]\nIt turns out such a statistic distributed as the F-Distribution, with density given by\n\\[\nf(x) = K(a, b) \\frac{x^{\\frac{a}{2}-1}}{(ax+b)^{\\frac{a+b}{2}}}\\ \\ \\ \\ \\text{with}\\ \\ \\ \\ K(a, b) = \\frac{\\Gamma(\\frac{a+b}{2}) a^{\\frac{a}{2}} b^{\\frac{b}{2}}}{\\Gamma(\\frac{a}{2}) \\Gamma(\\frac{b}{2})}\n\\]\nHere, the parameters \\(a\\) and \\(b\\) are the numerator degrees of freedom and denominator degrees of freedom, respectively.\n\nusing Distributions, CairoMakie\n\nn1, n2 = 10, 15\nN = 10^6\nmu, sigma = 10, 4\nnorm_dist = Normal(mu, sigma)\n\nfvs = Array{Float64}(undef, N)\n\nfor i in 1:N\n    d1 = rand(norm_dist, n1)\n    d2 = rand(norm_dist, n2)\n    fvs[i] = var(d1) / var(d2)\nend\n\nf_range = 0:0.1:5\nfig, ax = stephist(fvs, bins=400, color=:blue, label=\"Simulated\", normalization=:pdf)\nlines!(ax, f_range, pdf.(FDist(n1 - 1, n2 - 1), f_range), color=:red, label=\"Analytic\")\nxlims!(ax, low=0, high=5)\naxislegend(ax)\nfig\n\n\n\n\n\n\n8.6 The central limit theorem\nThe Central Limit Theorem (CLT) indicates that summations of a large number of independent random quantities, each with finite variance, yield a sum that is approximately normally distributed.\nThis is why the normal distribution is ubiquitous in nature.\nConsider an i.i.d sequence \\(X_1, X_2, ...\\), where all \\(X_i\\) are distributed according to some distribution \\(F(x_i; \\theta)\\) with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\).\nThen consider the random variable\n\\[\nY_n := \\sum_{i=1}^n X_i\n\\]\nIt is clear that \\(E(Y_n) = n\\mu\\) and \\(Var(Y_n) = n\\sigma^2\\).\nHence, we may consider a random variable\n\\[\n\\widetilde{Y}_n := \\frac{Y_n - n\\mu}{\\sqrt{n}\\sigma}\n\\]\nObserve that \\(\\widetilde{Y}_n\\) is zero mean and unit variance. The CLT states that as \\(n \\rightarrow \\infty\\), the ditribution of \\(\\widetilde{Y}_n\\) converges to a standard normal distribution. That is, for every \\(x \\in R\\),\n\\[\n\\lim_{n \\rightarrow \\infty} P(\\widetilde{Y}_n \\le x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{u^2}{2}} du\n\\]\nAlternatively, this may be viewed as indicating that for non-small \\(n\\)\n\\[\nY_n\\ \\ \\widetilde{\\text{approx}}\\ \\ N(n\\mu, n\\sigma^2)\n\\]\nIn addition, we have\n\\[\n\\overline{Y}_n = \\frac{Y_n}{n}\\ \\ \\widetilde{\\text{approx}}\\ \\ N(\\mu, \\frac{\\sigma^2}{n})\n\\]\nThis means that sample means from i.i.d samples with finite variances are asymptotically distributed according to a normal distribution as the sample size grows.\n\nusing Distributions, Random, CairoMakie\n\nRandom.seed!(1234)\n\n# note that n = 30 isn't enough to get a perfect fit to a normal distribution in the case of exponential distribution\nn, N = 30, 10^6\n\ndist1 = Uniform(1 - sqrt(3), 1 + sqrt(3))\ndist2 = Exponential(1)\ndist3 = Normal(1, 1)\n\ndata1 = [mean(rand(dist1, n)) for _ in 1:N]\ndata2 = [mean(rand(dist2, n)) for _ in 1:N]\ndata3 = [mean(rand(dist3, n)) for _ in 1:N]\n\nfig, ax = stephist(data1, bins=100, color=:blue, label=\"Average of Uniforms\", normalization=:pdf)\nstephist!(ax, data2, bins=100, color=:orange, label=\"Average of Exponentials\", normalization=:pdf)\nstephist!(ax, data3, bins=100, color=:green, label=\"Average of Normals\", normalization=:pdf)\nlines!(ax, 0:0.01:2, pdf.(Normal(1, 1 / sqrt(n)), 0:0.01:2), color=:red, label=\"Analytic Normal Distribution\")\naxislegend(ax)\nfig\n\n\n\n\n\n\n8.7 Point estimation\nGiven a random sample, \\(X_1, ..., X_n\\), a common task of statistical inference is to estimate a parameter \\(\\theta\\), or a function of it, say \\(h(\\theta)\\).\nThe process of designing an estimator, analyzing its performance, and carrying out the estimation is called point estimation.\nAlthough we can never know the underlying parameter \\(\\theta\\), or \\(h(\\theta)\\) exactly, we can arrive at an estimate for it via an estimator \\(\\hat{\\theta} = f(X_1, ..., X_n)\\). Here, the design of the estimator is embodied by \\(f(\\cdot)\\), a function that specifies how to construct the estimate from the sample.\nWhen performing point estimation, the first question we must answer is how close is \\(\\hat{\\theta}\\) to the actual unknown quantity \\(\\theta\\) or \\(h(\\theta)\\)?\n\n8.7.1 Describing the performance and behavior of estimators\nWhen analyzing the performance of an estimator \\(\\hat{\\theta}\\), it is important to understand that it is a random variable.\nOne common measure of its performance is the Mean Squared Error (MSE),\n\\[\n\\begin{align}\nMSE_\\theta(\\hat{\\theta}) &:= E[(\\hat{\\theta}-\\theta)^2] \\\\\n&= E(\\hat{\\theta}^2-2\\hat{\\theta}\\theta+\\theta^2) \\\\\n&= E(\\hat{\\theta}^2) - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= E(\\hat{\\theta}^2) - [E(\\hat{\\theta})]^2 + [E(\\hat{\\theta})]^2 - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n&= Var(\\hat{\\theta}) + (E(\\hat{\\theta}) - \\theta)^2 \\\\\n&:= variance + bias^2\n\\end{align}\n\\]\nHere, the MSE can be decomposed into the variance of the estimator and its bias squared.\n\nThe variance of the estimator represents the dispersion degree of the estimator itself. Low variance is clearly a desirable performance measure. This indicates the stability of the estimator.\nThe bias squared represents whether the estimator \\(\\hat{\\theta}\\) is an unbiased estimator of the parameter \\(\\theta\\) or \\(h(\\theta)\\). This indicates how close is the \\(E(\\hat{\\theta})\\) to \\(\\theta\\) or \\(h(\\theta)\\).\n\nThis can be illustrated by the folllowing plot:\n\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nest_pts = rand(Normal(3, 1), 10)\n\nfig = Figure()\nax = Axis(fig[1, 1])\n\nscatter!(repeat([0], 10), est_pts; color=:black, markersize=10, label=L\"\\hat{\\theta}\")\nscatter!(0, mean(est_pts); label=L\"E(\\hat{\\theta})\", color=:cyan, markersize=20)\nscatter!(0, 10; color=:red, markersize=20, label=L\"\\theta\")\n\nbracket!(0, mean(est_pts), 0, 10; text=L\"(E(\\hat{\\theta})-\\theta)^2\", offset=20, style=:square, orientation=:up)\nbracket!(0, minimum(est_pts), 0, maximum(est_pts); text=L\"Var(\\hat{\\theta})\", offset=20, style=:square, orientation=:down)\n\naxislegend(ax)\nfig\n\n\n\n\nCertainly, we are really interested in whether an estimator is unbiased, and then how low the variance of the estimator is.\nWhether an estimator is unbiased means that \\(E(\\hat{\\theta}) = \\theta\\).\nHere, we give some examples:\nConsider \\(X_1, ..., X_n\\) distributed according to any distribution with a finite mean \\(\\mu\\).\n\nThe sample mean \\(\\overline{X}\\) is an unbiased estimator of the population mean \\(\\mu\\):\n\n\\[\n\\begin{align}\nE(\\overline{X}) &= E\\left[\\frac{1}{n} \\sum_{i=1}^n X_i\\right] \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E(X_i) \\\\\n&= \\frac{1}{n} n\\mu \\\\\n&= \\mu\n\\end{align}\n\\]\n\\[\n\\begin{align}\nVar(\\overline{X}) &= Var\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) \\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n Var(X_i) \\\\\n&= \\frac{1}{n^2} n\\sigma^2 \\\\\n&= \\frac{\\sigma^2}{n}\n\\end{align}\n\\]\nSo the sample mean \\(\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator of the population mean \\(\\mu\\) with the variance \\(\\frac{\\sigma^2}{n}\\).\n\nIn the case where the population mean \\(\\mu\\) is known, but the population variance \\(\\sigma^2\\) is unknown, then \\(\\hat{\\sigma^2} := \\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2\\) is an unbiased estimator of the population variance \\(\\sigma^2\\), but \\(\\hat{\\sigma} := \\sqrt{\\hat{\\sigma^2}}\\) is not an unbiased estimator of \\(\\sigma\\) (in fact, \\(\\hat{\\sigma}\\) is asymptotically unbiased. That is, the bias tends to \\(0\\) as the sample size grows):\n\n\\[\n\\begin{align}\nE(\\hat{\\sigma^2}) &= E\\left(\\frac{1}{n} \\sum_{i=1}^n (X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} \\sum_{i=1}^n E\\left((X_i-\\mu)^2\\right) \\\\\n&= \\frac{1}{n} n\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n\\]\nSo in the case where the population mean \\(\\mu\\) is known, \\(\\hat{\\sigma^2}\\) is an unbiased estimator of \\(\\sigma^2\\), but \\(\\hat{\\sigma}\\) is not an unbiased estimator of \\(\\sigma\\).\n\nusing Random, Statistics\n\nRandom.seed!(1234)\n\n# consider an uniform distribution over [0, 1]\ntrueVar, trueStd = 1 / 12, sqrt(1 / 12)\n\nfunction estVar(n)\n    sample = rand(n)\n    sum((sample .- 0.5) .^ 2) / n\nend\n\nN = 10^7\nfor n in 10:20:90\n    biasVar = mean([estVar(n) for _ in 1:N]) - trueVar\n    biasStd = mean([sqrt(estVar(n)) for _ in 1:N]) - trueStd\n    println(\"n = \", n, \" Var bias: \", round(biasVar; digits=6),\n        \"\\t Std bias: \", round(biasStd; digits=5))\nend\n\nn = 10 Var bias: -4.0e-6     Std bias: -0.00304\nn = 30 Var bias: -3.0e-6     Std bias: -0.00098\nn = 50 Var bias: 3.0e-6  Std bias: -0.00058\nn = 70 Var bias: 4.0e-6  Std bias: -0.00042\nn = 90 Var bias: -1.0e-6     Std bias: -0.00032\n\n\n\nIn the case where the population mean \\(\\mu\\) is not known, the sample variance \\(S^2 := \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\) is an unbiased estimator of \\(\\sigma^2\\):\n\n\\[\n\\begin{align}\n\\sum_{i=1}^{n}(x_i-\\bar{x})^2 &= \\sum_{i=1}^{n}(x_i^2-2x_i\\bar{x}+\\bar{x}^2) \\\\\n&= \\sum_{i=1}^{n}x_i^2 - 2n\\bar{x}\\frac{1}{n}\\sum_{i=1}^{n}x_i + n\\bar{x}^2 \\\\\n&= \\sum_{i=1}^{n}x_i^2 - n\\bar{x}^2\n\\end{align}\n\\]\n\\[\nE(X^2) = Var(X) + [E(X)]^2 = \\sigma^2 + \\mu^2\n\\]\n\\[\n\\begin{align}\nE(S^2) &= E\\left(\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\right) \\\\\n&= \\frac{1}{n-1}E\\left(\\sum_{i=1}^{n}X_i^2 - n\\overline{X}^2\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^{n}E(X_i^2) - nE(\\overline{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n(\\sigma^2+\\mu^2) - n(\\frac{\\sigma^2}{n}+\\mu^2)\\right) \\\\\n&= \\frac{1}{n-1} (n-1)\\sigma^2 \\\\\n&= \\sigma^2\n\\end{align}\n\\]\nIn summary, we can evaluate the performance and behavior of an estimator from different aspects, such as unbias (\\(E(\\hat{\\theta}) = \\theta\\)), effectiveness (\\(Var(\\hat{\\theta})\\) is as small as possible), consistency (an estimator is consistent if it converges to the true value as the number of observations grows to infinity), etc.\n\n\n8.7.2 Designing estimators\n\n8.7.2.1 Method of moments\nThe key idea is that the \\(k\\)’s moment estimator calculated from the random sample should be equal to the \\(k\\)’s moment of the underlying distribution from which we sample (i.e. \\(\\hat{m}_k = m_k\\)). Then we can obtain parameter estimates for a distribution.\n\\[\n\\hat{m}_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\n\\]\n\\[\nm_k = E(X^k) = \\begin{cases}\n\\sum_{i=1}^{\\infty} x_i p(x_i) &\\text{for discrete case} \\\\\n\\int_{-\\infty}^{\\infty} x p(x) dx &\\text{for continuous case}\n\\end{cases}\n\\]\nIn cases where there are multiple unkown parameters, say \\(K\\), we use the first \\(K\\) moment estimates to formulate a system of \\(K\\) equations and \\(K\\) unkowns. This system of equations can be written as \\(E[X^k; \\theta_1, ..., \\theta_K] = \\hat{m}_k\\ \\ \\ \\ \\text{for}\\ \\ \\ \\ k=1,...,K\\).\n\nusing Random, Distributions, NLsolve\n\nRandom.seed!(1234)\n\n# Triangular distribution\na, b, c = 3, 5, 4\ndist = TriangularDist(a, b, c)\nn = 2000\nsamples = rand(dist, n)\n\nm_k(k, data) = 1 / n * sum(data .^ k)\nmHats = [m_k(i, samples) for i in 1:3]\n\nfunction equations(F, x)\n    F[1] = 1 / 3 * (x[1] + x[2] + x[3]) - mHats[1]\n    F[2] = 1 / 6 * (x[1]^2 + x[2]^2 + x[3]^2 +\n                    x[1] * x[2] + x[1] * x[3] + x[2] * x[3]) - mHats[2]\n    F[3] = 1 / 10 * (x[1]^3 + x[2]^3 + x[3]^3 +\n                     x[1]^2 * x[2] + x[1]^2 * x[3] + x[2]^2 * x[1] +\n                     x[2]^2 * x[3] + x[3]^2 * x[1] + x[3]^2 * x[2] +\n                     x[1] * x[2] * x[3]) - mHats[3]\nend\n\nnlOutput = nlsolve(equations, [0.1, 0.1, 0.1])\nsol = sort(nlOutput.zero)\naHat, bHat, cHat = sol[1], sol[3], sol[2]\nprintln(\"Found estimates for (a, b, c) = \", (aHat, bHat, cHat), \"\\n\")\nprintln(nlOutput)\n\nFound estimates for (a, b, c) = (3.0233820362818355, 5.003274273506214, 4.0251075704074575)\n\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.1, 0.1, 0.1]\n * Zero: [3.0233820362818355, 5.003274273506214, 4.0251075704074575]\n * Inf-norm of residuals: 0.000000\n * Iterations: 13\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 14\n * Jacobian Calls (df/dx): 13\n\n\n\n\n8.7.2.2 Maximum likelihood estimation (MLE)\nThe key is to consider the likelihhod of the parameter \\(\\theta\\) having a specific value given observations \\(x_1, ..., x_n\\). This is done via the likelihood function,\n\\[\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta)\n\\]\nIf \\(X_1, ..., X_n\\) are i.i.d., where the joint PDF of \\(X_1, ..., X_n\\) is represented as the product of the individual PDF, then we have\n\\[\nL(\\theta; x_1, ..., x_n) = f_{X_1, ..., X_n}(x_1, ..., x_n; \\theta) = \\prod_{i=1}^n f(x_i; \\theta)\n\\]\nNow given the likelihood function, the maximum likelihood estimator is a value \\(\\theta\\) that maximizes \\(L(\\theta; x_1, ..., x_n)\\). So an MLE is the maximizer of the likelihood.\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(1234)\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nn = 10^2\nsamples = rand(gammaDist, n)\n\nalphaGrid = collect(1:0.02:3)\nlambdaGrid = collect(1:0.02:5)\n\nlikelihood = [prod(pdf.(Gamma(a, 1 / l), samples)) for a in alphaGrid, l in lambdaGrid]\n\nsurface(alphaGrid, lambdaGrid, likelihood, axis=(type=Axis3,))\n\n\n\n\nObserve that any maximizer \\(\\hat{\\theta}\\) of \\(L(\\theta; x_1, ..., x_n)\\) will also maximize its logarithm. Practically, both from an analytic and numerical perspective, considering this \\(log-likelihood function\\) is often more attractive:\n\\[\nl(\\theta; x_1, ..., x_n) := \\log L(\\theta; x_1, ..., x_n) = \\sum_{i=1}^{n}\\log \\left(f(x_i; \\theta)\\right)\n\\]\nNote: the second equality holds only for i.i.d. \\(X_1, ..., X_n\\).\nHence, given a sample from a gamma distribution, the log-likelihood function is\nFirst, the PDF of the gamma distribution is\n\\[\nf(x) = \\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\lambda x}\n\\]\nwith parameters \\(\\lambda &gt; 0\\) and \\(\\alpha &gt; 0\\).\n\\[\nl(\\theta; x_1, ..., x_n) = n\\alpha\\log(\\lambda) - n\\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\sum_{i=1}^{n}\\log(x_i) - \\lambda\\sum_{i=1}^{n}x_i\n\\]\nDivide by \\(n\\) to obtain the following function that needs to be maximized:\n\\[\n\\tilde{l}(\\theta; \\bar{x}, \\bar{x}_l) = \\alpha\\log(\\lambda) - \\log(\\Gamma(\\alpha)) + (\\alpha - 1)\\bar{x}_l - \\lambda \\bar{x}\n\\]\nwhere \\(\\bar{x}\\) is the sample mean, and \\(\\bar{x}_l := \\frac{1}{n}\\sum_{i=1}^{n}\\log(x_i)\\).\nFurther simplification is possible by removing the stand-alone \\(-\\bar{x}_l\\) term, as it does not affect the optimal value. Heance, our optimization problem is then,\n\\[\n\\max_{\\lambda &gt; 0, \\alpha &gt; 0} \\alpha (\\log(\\lambda + \\bar{x}_l)) - \\log(\\Gamma(\\alpha)) - \\lambda \\bar{x}\n\\]\nAs is typical in such cases, the function actually depends on the sample only through the two sufficient statistics \\(\\bar{x}\\) and \\(\\bar{x}_l\\).\nThen, taking \\(\\alpha\\) as fixed, we may consider the derivative with respect to \\(\\lambda\\), and equate this to \\(0\\):\n\\[\n\\frac{\\alpha}{\\lambda} - \\bar{x} = 0\n\\]\nHence, for any optimal \\(\\alpha^\\star\\), we have \\(\\lambda = \\frac{\\alpha}{\\bar{x}}\\). This allows us to substitute \\(\\lambda^\\star\\) for \\(\\lambda\\) to obtain\n\\[\n\\max_{\\alpha &gt; 0} \\alpha (\\log(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l) - \\log(\\Gamma(\\alpha)) - \\alpha\n\\]\nNow by taking the derivative with respective to \\(\\alpha\\), and equating this to \\(0\\), we obtain\n\\[\n\\log(\\alpha) + 1 - \\log(\\bar{x}) + \\bar{x}_l - \\psi(\\alpha) - 1 = 0\n\\]\nwhere \\(\\psi(z) := \\frac{d}{dz}\\log(\\Gamma(z))\\) is the well-known digamma function.\nHence, we find that \\(\\alpha^\\star\\) must satisfy\n\\[\n\\log(\\alpha) - \\psi(\\alpha) - \\log(\\bar{x}) + \\bar{x}_l = 0\n\\]\nwith \\(\\lambda^\\star = \\frac{\\alpha^\\star}{\\bar{x}}\\). Our optimal MLE solution is given by \\((\\alpha^\\star, \\lambda^\\star)\\). In order to find this value, we must solve it numerically.\n\nusing SpecialFunctions, Distributions, Roots, CairoMakie, Random\n\nRandom.seed!(1234)\n\neq(alpha, xb, xbl) = log(alpha) - digamma(alpha) - log(xb) + xbl\n\nrealAlpha, realLambda = 2, 3\ngammaDist = Gamma(realAlpha, 1 / realLambda)\n\nfunction mle(sample)\n    alpha = find_zero((a) -&gt; eq(a, mean(sample), mean(log.(sample))), 1)\n    lambda = alpha / mean(sample)\n    return [alpha, lambda]\nend\n\nN = 10^4\n\nmles10 = [mle(rand(gammaDist, 10)) for _ in 1:N]\nmles100 = [mle(rand(gammaDist, 100)) for _ in 1:N]\nmles1000 = [mle(rand(gammaDist, 1000)) for _ in 1:N]\n\nfig = Figure()\nax = Axis(fig[1, 1],\n    xlabel=L\"\\alpha\",\n    ylabel=L\"\\lambda\",\n    limits=(0, 6, 0, 8))\n\nscatter!(ax, first.(mles10), last.(mles10),\n    color=:blue, markersize=2, label=\"n = 10\")\nscatter!(ax, first.(mles100), last.(mles100),\n    color=:red, markersize=2, label=\"n = 100\")\nscatter!(ax, first.(mles1000), last.(mles1000),\n    color=:green, markersize=2, label=\"n = 1000\")\n\nmarker_elem1 = MarkerElement(color=:blue, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem2 = MarkerElement(color=:red, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nmarker_elem3 = MarkerElement(color=:green, marker=:circle, markersize=20, points=Point2f[(0.5, 0.5)])\nLegend(fig[1, 1],\n    [marker_elem1, marker_elem2, marker_elem3],\n    [\"n = 10\", \"n = 100\", \"n = 1000\"],\n    tellwidth=false, tellheight=false,\n    halign=:left, valign=:top)\nfig\n\n\n\n\n\n\n8.7.2.3 Comparing the method of moments and MLE\nNow we use the Mean Squared Error (MSE)\n\\[MSE_\\theta(\\hat{\\theta}) = E[(\\hat{\\theta}-\\theta)^2] = Var(\\hat{\\theta}) + (E[\\hat{\\theta}] - \\theta)^2 = \\text{variance} + \\text{bias}^2\\]\nto compare the performance and behavior of moments and MLE.\nConsider a random sample \\(x_1, ..., x_n\\) from an uniform distribution on the interval \\((a, b)\\).\nThe MLE for the parameter \\(\\theta = (a, b)\\) can be shown to be\n\\[\n\\begin{align}\n\\hat{a} &= \\min\\{x_1, ..., x_n\\} \\\\\n\\hat{b} &= \\max\\{x_1, ..., x_n\\}\n\\end{align}\n\\]\nFor the method of moments, since \\(X \\backsim U(a, b)\\), it follows that\n\\[\n\\begin{align}\nE[X] &= \\frac{a+b}{2} \\\\\nVar(X) &= \\frac{(b-a)^2}{12}\n\\end{align}\n\\]\nHence, by solving for \\(a\\) and \\(b\\), and replacing \\(E[X]\\) and \\(Var(X)\\) with \\(\\bar{x}\\) and \\(s^2\\) respectively, we obtain\n\\[\n\\begin{align}\n\\hat{a} &= \\bar{x}-\\sqrt{3}s \\\\\n\\hat{b} &= \\bar{x}+\\sqrt{3}s\n\\end{align}\n\\]\n\nusing Distributions, CairoMakie\n\nN = 10^5\nnMin, nStep, nMax = 10, 10, 200\nsampleSizes = nMin:nStep:nMax\ntrueB = 5\ntrueDist = Uniform(-2, trueB)\n\nMLEest(data) = maximum(data)\nMMest(data) = mean(data) + sqrt(3) * std(data)\n\nres = Dict{Symbol,Array{Float64}}(\n    (sym -&gt; sym =&gt; Array{Float64}(undef, length(sampleSizes))).(\n        [:MSEMLE, :MSEMM, :VarMLE, :VarMM, :BiasMLE, :BiasMM]))\n\nfor (i, n) in enumerate(sampleSizes)\n    mleEst, mmEst = Array{Float64}(undef, N), Array{Float64}(undef, N)\n    for j in 1:N\n        samples = rand(trueDist, n)\n        mleEst[j] = MLEest(samples)\n        mmEst[j] = MMest(samples)\n    end\n    meanMLE, meanMM = mean(mleEst), mean(mmEst)\n    varMLE, varMM = var(mleEst), var(mmEst)\n\n    res[:MSEMLE][i] = varMLE + (meanMLE - trueB)^2\n    res[:MSEMM][i] = varMM + (meanMM - trueB)^2\n    res[:VarMLE][i] = varMLE\n    res[:VarMM][i] = varMM\n    res[:BiasMLE][i] = meanMLE - trueB\n    res[:BiasMM][i] = meanMM - trueB\nend\n\nfig = Figure(size=(600, 1200))\nax_mse = Axis(fig[1, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"MSE\")\nscatter!(ax_mse, sampleSizes, res[:MSEMLE]; color=:blue, label=\"MSE (MLE)\")\nscatter!(ax_mse, sampleSizes, res[:MSEMM]; color=:red, label=\"MSE (MM)\")\naxislegend(ax_mse)\n\nax_var = Axis(fig[2, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Variance\")\nscatter!(ax_var, sampleSizes, res[:VarMLE]; color=:blue, label=\"Variance (MLE)\")\nscatter!(ax_var, sampleSizes, res[:VarMM]; color=:red, label=\"Variance (MM)\")\naxislegend(ax_var)\n\nax_bias = Axis(fig[3, 1],\n    xlabel=\"Sample size\",\n    ylabel=\"Bias\")\nscatter!(ax_bias, sampleSizes, res[:BiasMLE]; color=:blue, label=\"Bias (MLE)\")\nscatter!(ax_bias, sampleSizes, res[:BiasMM]; color=:red, label=\"Bias (MM)\")\naxislegend(ax_bias; position=:rb)\nfig\n\n\n\n\nIn fact, there is more supporting theory for the usefulness of maximum likelihood estimation as \\(n \\rightarrow \\infty\\).\n\n\n\n\n8.8 Confidence interval\nGiven a single sample \\(X_1, ..., X_n\\), how does one obtain an indication about the accuracy of the estimate?\nConsider the case where we are trying to estimate the parameter \\(\\theta\\). A confidence interval is then an interval \\([L, U]\\) obtained from our sample data, such that \\(P(L\\le \\theta \\le U) = 1-\\alpha\\), where \\(1-\\alpha\\) is called the confidence level.\nConsider a case of a single observation \\(X\\) (\\(n=1\\)) taken from a symmetric triangular distribution, with a spread of 2 and an unknown center (mean) \\(\\mu\\).\nIn this case, we would set\n\\[\n\\begin{align}\nL &= X + q_{\\frac{\\alpha}{2}} \\\\\nU &= X + q_{1-\\frac{\\alpha}{2}}\n\\end{align}\n\\]\nwhere \\(q_u\\) is the \\(u\\)’th quantile of a triangular distribution centered at \\(0\\), and having a spread of \\(2\\).\nSetting \\(L\\) and \\(U\\) in this manner ensures that \\(P(L\\le \\theta \\le U) = 1-\\alpha\\) holds.\nIn this case, we know that \\(q_{\\frac{\\alpha}{2}} = -(1 - \\sqrt{\\alpha})\\) and \\(q_{1-\\frac{\\alpha}{2}} = 1 - \\sqrt{\\alpha}\\).\n\n\n\n\n\n\nImportant\n\n\n\nTo understand the confidence interval \\(P(L\\le \\theta \\le U) = 1-\\alpha\\), a key point is that there is \\(1-\\alpha\\) chance that the actual parameter \\(\\theta\\) is covered by the interval \\([L, U]\\). This means that if the sampling experiment is repeated say \\(N\\) times, then on average, \\(N\\times(1-\\alpha)\\) of the time the actual parameter \\(\\theta\\) is covered by the interval.\n\n\n\nusing Distributions, CairoMakie, Random\n\nRandom.seed!(1234)\n\nalpha = 0.05\nL(obs) = obs - (1 - sqrt(alpha))\nU(obs) = obs + (1 - sqrt(alpha))\n\nmu = 5.57\ntriDist = TriangularDist(mu - 1, mu + 1, mu)\n\nN = 100\nbounds = zeros(N, 2)\nhits = Vector{Symbol}(undef, N)\nfor i in 1:N\n    obs = rand(triDist)\n    LL, UU = L(obs), U(obs)\n    bounds[i, :] = [LL UU]\n    if LL &lt;= mu && mu &lt;= UU\n        hits[i] = :blue\n    else\n        hits[i] = :red\n    end\nend\n\nfig, ax = rangebars(1:N, bounds[:, 1], bounds[:, 2], color=hits)\nhlines!(ax, mu; color=:green, label=\"Parameter value\")\naxislegend(ax)\nfig\n\n\n\n\n\n\n8.9 Hypothesis tests\nThe approach involves partitioning the parameter space \\(\\Theta\\) into \\(\\Theta_0\\) and \\(\\Theta_1\\), and then, based on the sample, concluding whether one of the two hypotheses, \\(H_0 : \\theta \\in \\Theta_0\\) or \\(H_1 : \\theta \\in \\Theta_1\\) holds.\nThe hypothesis \\(H_0\\) is called the null hypothesis and \\(H_1\\) the alternative hypothesis. The former is the default hypothesis, and in carrying out hypothesis testing our general aim is to reject this hypothesis.\nSince our decision is based on a random sample, there is always a chance of making a mistakenly false conclusion. There are two types of errors that can be made in carrying out a hypothesis testing.\n\n\n\n\n\n\n\nType I and type II erorrs with their probabilities \\(\\alpha\\) and \\(\\beta\\) respectively\n\n\nNote that \\(1-\\beta\\) is known as the power of the hypothesis test.\n\n\n\n\n\n\nHow to understand: fail to reject \\(H_0\\) and reject \\(H_0\\)\n\n\n\nNote that in carrying out a hypothesis test, \\(\\alpha\\) is typically specified, while power is not direcly controlled, but rather is influenced by the sample size and other factors.\nAn important point in terminology is that we don’t use the phrase “accept” for the null hypothesis, rather we “fail to reject it” (if we stick with \\(H_0\\)) or “reject it” (if we choose \\(H_1\\)). This is because when we fail to reject \\(H_0\\), we typically don’t know the actual value of \\(\\beta\\), hence we aren’t able to put a level of certainty on \\(H_0\\) being the case. In other words, when we fail to reject \\(H_0\\), this doesn’t mean that \\(H_0\\) is true. We just haven’t enough evidence to reject it based on the sample data. This means that we are still likely to make type II error with the probability \\(\\beta\\) (i.e. in reality, \\(H_0\\) is false, but we fail to reject it) though we fail to reject \\(H_0\\). Because we usualy don’t know the actual value of \\(\\beta\\), we cannot give such an assertion that \\(H_0\\) is true with a specified confidence level. This is why we just say that we fail to reject \\(H_0\\) based on the sample data, instead of \\(H_0\\) is true.\nHowever, if we do reject \\(H_0\\), then by the design of hypothesis tests we can say that our error probability is bounded by \\(\\alpha\\).\n\n\n\n8.9.1 How to design and perform a hypothesis testing\n\nFormulate the scientific question as a hypothesis by partitioning the parameter space \\(\\Theta\\) into \\(\\Theta_0\\) and \\(\\Theta_1\\), and then formimg the two hypotheses \\(H_0 : \\theta \\in \\Theta_0\\) and \\(H_1 : \\theta \\in \\Theta_1\\).\nDefine the test statistic, denoted \\(X^*\\), as a function of the sample data.\n\nSince the test statistic follows some distribution under \\(H_0\\), the next step is to consider how likely it is to observe the specific value (\\(X^*\\)) calculated from the sample data under \\(H_0\\).\nTo this end, in setting up a hypothesis test, we typically choose a significance level \\(\\alpha\\) (e.g. \\(0.05\\) or \\(0.01\\)), which quantifies our level of tolerance for enduring a type I error. For example, setting \\(\\alpha = 0.01\\) implies we wish to design a test where the probability of type I error is at most \\(0.01\\) if \\(H_0\\) holds. Clearly, a low \\(\\alpha\\) is desirable, however, there are tradeoffs involved since seeking a very low \\(\\alpha\\) will imply a high \\(\\beta\\) (low power).\n\nPick a significance level \\(\\alpha\\).\n\nConsider that we have a series of sample observations distributed as continuous uniform between \\(0\\) and some unknown upper bound \\(m\\).\nSay we set\n\\[\nH_0: m=1,\\ \\ \\ \\ H_1: m&lt;1\n\\]\nwith observations \\(X_1, ..., X_n\\), one possible test statistic is the sample range:\n\\[\nX^* = max(X_1, ..., X_n) - min(X_1, ..., X_n)\n\\]\nAs is always the case, the test statistic is a random variable. Under \\(H_0\\), we expect the distribution of \\(X^*\\) to have support \\([0, 1]\\) with the most likely value being close to \\(1\\). This is because low values of \\(X^*\\) are less plausible under \\(H_0\\). The explicit form of the distribution of \\(X^*\\) can be analytically obtained however for simplicity we use a Monte Carlo simulation to estimate it and present the density.\nFor this case, it is sensible to reject \\(H_0\\) if \\(X^*\\) is small enough.\n\nPerforming hypothesis testing.\n\nAt present, there are two alternatives for performing hypothesis tests:\n\nUsing rejection region:\n\nDenoting quantiles of this distribution by \\(q_0(u)\\), then we set the rejection region as \\(R = [0, q_0(\\alpha)]\\). Using Monte Carlo, we compute the rejection region where the critical value is the upper boundary \\(q_0(\\alpha)\\) of the rejection region in this case. Note that computing the rejection region does not require any sample data as it is based on model assumptions and not the sample.\nThe decision rule for this hypothesis test is simple: compare the observed value of the test statistic, \\(x^*\\), to the critical value \\(q_0(\\alpha)\\) and reject \\(H_0\\) if \\(x^*\\le q_0(\\alpha)\\), otherwise do not reject.\n\nUsing p-value:\n\nWe collect the data and compute the observed value of the test statistic \\(x^*\\). The p-value is then the maximal \\(\\alpha\\) under which the test would be rejected with the observed test statistic. In other words, we find \\(p\\) which solves \\(x^* = q_0(p)\\). This is computed via \\(F_0(x^*)\\), where \\(F(\\cdot)\\) is the CDF of \\(X^*\\).\nUsing the p-vaue approach, reporting a low p-value implies that we are very confident in rejecting \\(H_0\\), while a high p-value implies we are not. The p-value approach can be used to decide whether \\(H_0\\) should be rejected or not with a specified \\(\\alpha\\). For this case, simply comparing \\(p\\) and \\(\\alpha\\), and reject \\(H_0\\) if \\(p\\le \\alpha\\).\n\n\n\n\n\n\n\nUsing rejection region or p-value to perform hypothesis testing\n\n\n\nusing Distributions, CairoMakie, Random, Statistics\n\nRandom.seed!(1)\n\nn, N, alpha = 10, 10^7, 0.05\nmActual = 0.7\ndist0, dist1 = Uniform(0, 1), Uniform(0, mActual)\n\nts(sample) = maximum(sample) - minimum(sample)\n\nempiricalDistUnderH0 = [ts(rand(dist0, n)) for _ in 1:N]\nrejectionValue = quantile(empiricalDistUnderH0, alpha)\n\nsamples = rand(dist1, n)\ntestStat = ts(samples)\npValue = sum(empiricalDistUnderH0 .&lt;= testStat) / N\n\nif testStat &gt; rejectionValue\n    println(\"Do not reject: \", round(testStat; digits=4), \" &gt; \", round(rejectionValue; digits=4))\nelse\n    println(\"Reject: \", round(testStat; digits=4), \" &lt;= \", round(rejectionValue; digits=4))\nend\nprintln(\"p-value = \", round(pValue; digits=4))\n\nfig, ax = stephist(empiricalDistUnderH0; bins=100, color=:blue, normalization=:pdf)\nvlines!(ax, testStat; color=:red, label=\"Observed test statistic\")\nvlines!(ax, rejectionValue; color=:black, label=\"Critical value boundary\")\naxislegend(ax; position=:lt)\nax.title = L\"\\text{The distribution of the test statistic }X^*\\text{ under }H_0\"\nfig\n\nReject: 0.5725 &lt;= 0.6059\np-value = 0.032\n\n\n\n\n\n\n\n8.9.2 Understand type I and type II errors\nWhen the alternative parameter spaces \\(\\Theta_0\\) and \\(\\Theta_1\\) are only comprised of a single point each, the hypothesis test is called a simple hypothesis test.\nConsider a container that conatins two identical types of pipes, except that one type weighs \\(15\\) grams on average and the other \\(18\\) grams on avearge. The standard deviation of the weights of both pipe types is \\(2\\) grams.\nImagine that we sample a single pipe, and wish to determine its type. Denote the weight of this pipe by the random variable \\(X\\). For this example, we devise the following statistical hypothesis test: \\(\\Theta_0 = {15}\\) and \\(\\Theta_1 = {18}\\). Now, given a threshold \\(\\tau\\), we reject \\(H_0\\) if \\(X &gt; \\tau\\), otherwise we retain \\(H_0\\).\nIn this case, we can explicitly analyze the probabilities of both the type I and type II errors, \\(\\alpha\\) and \\(\\beta\\) respectively.\n\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1, sd, tau = 15, 18, 2, 17.5\ndist0, dist1 = Normal(mu0, sd), Normal(mu1, sd)\ngrid = 5:0.1:25\nh0grid, h1grid = tau:0.1:25, 5:0.1:tau\n\n# CCDF: complementary CDF\nprintln(\"Probability of Type I error: \", ccdf(dist0, tau))\nprintln(\"Probability of Type II error: \", cdf(dist1, tau))\n\nfig, ax = lines(grid, pdf.(dist0, grid); color=:red, label=\"Bolt type 15g\")\nband!(ax, h0grid, repeat([0], length(h0grid)), pdf.(dist0, h0grid); color=(:red, 0.2))\nlines!(ax, grid, pdf.(dist1, grid); color=:green, label=\"Bolt type 18g\")\nband!(ax, h1grid, repeat([0], length(h1grid)), pdf.(dist1, h1grid); color=(:green, 0.2))\nlinesegments!(ax, [tau, 25], [0, 0]; color=:black, label=\"Rejection region\", linewidth=5)\ntext!([15, 18, 15.5, 18.5], [0.21, 0.21, 0.02, 0.02]; text=[L\"H_0\", L\"H_1\", L\"\\beta\", L\"\\alpha\"], align=repeat([(:center, :center)], 4), fontsize=28)\naxislegend(ax)\nfig\n\nProbability of Type I error: 0.10564977366685525\nProbability of Type II error: 0.4012936743170763\n\n\n\n\n\n\n\n8.9.3 The Receiver Operating Curve (ROC)\nIn the previous example, \\(\\tau = 17.5\\) was arbitrarily chosen.\nClearly, if \\(\\tau\\) was increased, the probability of making a type I error, \\(\\alpha\\), would decrease, while the probability of making a type II error, \\(\\beta\\), would increase. Conversely, if we decreased \\(\\tau\\) the reverse would occur.\nHere we use the Receiver Operating Curve (ROC) to help to visualize the tradeoff between type I and type II errors. It allows us to visualize the error tradeoffs for all possible \\(\\tau\\) values simutaneously for a particular alternative hypothesis \\(H_1\\).\nBelow we plot the analytic coordinates of \\((\\alpha(\\tau), 1-\\beta(\\tau))\\). This is the ROC. It is a parametric plot of the probability of a type I error and power.\n\nusing Distributions, StatsBase, CairoMakie\n\nmu0, mu1a, mu1b, mu1c, sd = 15, 16, 18, 20, 2\ntauGrid = 5:0.1:25\n\ndist0 = Normal(mu0, sd)\ndist1a, dist1b, dist1c = Normal(mu1a, sd), Normal(mu1b, sd), Normal(mu1c, sd)\n\nfalsePositive = ccdf.(dist0, tauGrid)\ntruePositiveA, truePositiveB, truePositiveC = ccdf.(dist1a, tauGrid), ccdf.(dist1b, tauGrid), ccdf.(dist1c, tauGrid)\n\nfig, ax = ablines(0, 1; color=:black, linestyle=:dash, label=L\"H_0 = H_1 = 15\")\nlines!(ax, falsePositive, truePositiveA; color=:blue, label=L\"H_{1a}: \\mu_1 = 16\")\nlines!(ax, falsePositive, truePositiveB; color=:red, label=L\"H_{1b}: \\mu_1 = 18\")\nlines!(ax, falsePositive, truePositiveC; color=:green, label=L\"H_{1c}: \\mu_1 = 20\")\nax.xlabel = L\"\\alpha\"\nax.ylabel = L\"\\text{Power }(1-\\beta)\"\naxislegend(ax; position=:rb)\nfig\n\n\n\n\n\n\n8.9.4 A randomized hypothesis test\nWe now investigate the concept of a randomization test, which is a type of non-parametric test, i.e. a statistical test which does not require that we know what type of distribution the data comes from.\nA virtue of non-parametric tests is that they do not impose a specific model.\nConsider the following example, where a farmer wants to test whether a new fertilizer is effective at increasing the yield of her tomato plants. As an experiment, she took \\(20\\) plants, kept \\(10\\) as controls, and treated the remaining \\(10\\) with fertilizer, After \\(2\\) months, she harvested the plants and recorded the yield of each plant in kg as shown in the following table:\n\n\n\n\n\n\n\nYield in kg for \\(10\\) plants with, and \\(10\\) plants without fertilizer (control)\n\n\nIt can be observed that the group of plants treated with fertilizer have an average yield \\(0.494\\) kg greater than that of the control group. One could argue that this difference is due to the effects of the fertilizer. We now investigate if this is a reasonable assumption. Let us assume for a moment that the fertilizer had no effect on plant yield (\\(H_0\\)) and that the result was simply due to random chance. In such a scenario, we actually have \\(20\\) observations from the same group, and regardless of how we arrange our observations, we would expect to observe similar results.\nHence, we can investigate the likelihood of this outcome occurring by random chance, by considering all possible combinations of \\(10\\) samples from our group of \\(20\\) observations, and counting how many of these combinations result in a difference in sample means greater than or equal to \\(0.494\\) kg. The proportion of times this occurs is analogous to the likelihood that the difference we observe in our sample means was purely due to random chance. It is in a sense the p-value.\nBefore proceeding, we calculate the number of ways one can sample \\(r = 10\\) unique items from \\(n = 20\\) total, which is given by\n\\[\n\\binom{20}{10} = 184,756\n\\]\nHence, the number of possible combinations in our example is computationally manageable. Note that in a different situation where \\(n\\) and \\(r\\) would be bigger, e.g. \\(n = 40\\) and \\(r = 20\\), the number of combinations would be too big for an exhaustive search (about \\(137\\) billion). In such a case, a viable alternative is to randomly sample combinations for estimating the p-value.\n\nusing Combinatorics, Statistics, DataFrames, CSV\n\ndf = CSV.read(\"./data/fertilizer.csv\", DataFrame)\ncontrol = df.Control\nfertilizer = df.FertilizerX\n\nsubGroups = collect(combinations([control; fertilizer], 10))\nmeanFert = mean(fertilizer)\npVal = sum([mean(i) &gt;= meanFert for i in subGroups]) / length(subGroups)\nprintln(\"p-value = \", pVal)\n\np-value = 0.023972157873086666\n\n\n\n\n\n8.10 Bayesian statistics\nIn the Bayesian paradigm, the scalar or vector of parameter \\(\\theta\\) is not assumed to exist as some fixed unknown quantity but instead is assumed to follow a distribution.\nThat is, the parameter itself, is a random variable, and the act of Bayesian inference is the process of obtaining more information about the distribution of \\(\\theta\\).\nThis allows us to incorparate prior beliefs about the parameter before experience from new observations is taken into consideration.\nThe key objects at play are the prior distribution of the parameter and the posterior distribution of the parameter.\nThe former is postulated beforehand, or exists as a consequence of previous inference, while the latter captures the distribution of the parameter after observations are taken into consideration.\nThe relationship between the prior and the posterior is\n\\[\n\\text{posterior} = \\frac{\\text{likelihood}\\times\\text{prior}}{evidence}\\ \\ \\ \\ \\text{or}\\ \\ \\ \\ f(\\theta|x) = \\frac{f(x|\\theta)\\times f(\\theta)}{\\int f(x|\\theta)f(\\theta)d\\theta}\n\\]\nThis is nothing but Bayes’s rule \\(P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{\\sum_{j=1}^{n}P(A|B_j)P(B_j)}, A\\in \\cup_{i=1}^n B_i\\) applied to densities.\nHere the prior distribution (density) is \\(f(\\theta)\\), and the posterior distribution (density) is \\(f(\\theta|x)\\).\nObserve that the denominator, known as evidence or marginal likelihood, is constant with respect to the parameter \\(\\theta\\). This allows the equation to be written as\n\\[\nf(\\theta|x) \\propto f(x|\\theta)\\times f(\\theta)\n\\]\nHence, the posterior distribution can be easily obtained up to the normalizing constant (the evidence) by multiplying the prior with the likelihood.\nIn general, carrying out Beyesian inference involves the following steps:\n\nAssume some distributional model for the data based on the parameter \\(\\theta\\) which is a random variable.\nUse previous inference experience, elicit an expert, or make an educated guess to determine a prior distribution \\(f(\\theta)\\). The prior distribution might be parameterized by its own parameters, called hyper-parameters.\nCollect data \\(x\\) and create an expression or a computational mechanism for the likelihood \\(f(x|\\theta)\\) based on the distributional model chosen.\nUse the Bayes’s rule applied to densities to obtain the posterior distribution of the parameters \\(f(\\theta|x)\\).\n\nIn most cases, the evidence (the denominator) is not easily computable. Hence the posterior distribution is only available up to a normalizing constant. In some special cases, the form of the posterior distribution is the same as the prior distribution. In such cases, conjugacy holds, the prior is called a conjugate prior, and the hyper-parameters are updated from prior to posterior.\n\nThe posterior distribution can then be used to make conclusions about the model.\n\nFor example, if a single specific parameter value is needed to make the model concrete, a Bayes estimate based on the posterior distribution, for example, the posterior mean, may be computed:\n\\[\n\\hat{\\theta} = \\int \\theta f(\\theta|x) d\\theta\n\\]\nFurther analyses such as obtaining credible intervals, similar to condidence intervals, may also be carried out.\n\nThe model with \\(\\hat{\\theta}\\) can then be used for making conclusions. Alternatively, a whole class of models based on the posterior distribution \\(f(\\hat{\\theta}|x)\\) can be used. This often goes hand in hand with simulation as one is able to generate Monte Carlo samples from the posterior distribtion.\n\n\n8.10.1 A Poisson example\nConsider an example where an insurance company models the number of weekly fires in a city using a Poisson distribution with parameter \\(\\lambda\\). Here, \\(\\lambda\\) is also the expected number of fires per week.\nAssume that the following data is collected over a period of \\(16\\) weeks:\n\\[\nx = (x_1, ..., x_{16}) = (2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0)\n\\]\nEach data point indicates the number of fires per week.\nIn this case, the MLE is \\(\\hat{\\lambda} = 1.8125\\) simply obtained by the sample mean. Hence, in a frequentist approach, after \\(16\\) weeks, the distribution of the number of fires per week is modeled by a Poisson distribution with \\(\\lambda = 1.8125\\). One can then obtain estimates for say, the probability of having more than \\(5\\) fires in a given week as follows:\n\\[\nP(\\text{fires per week} &gt; 5) = 1 - \\sum_{k=0}^{5} e^{-\\lambda \\frac{\\lambda^k}{k!}} \\approx 0.0107\n\\]\nHowever, the drawback of such an approach in estimating \\(\\lambda\\) is that it didn’t make use of previous information.\nSay that for example, further knowledge comes to light that the number of fires per week ranges between \\(0\\) and \\(10\\), and that the typical number is \\(3\\) fires per week. In this case, one can assign a prior distribution to \\(\\lambda\\) that captures this belief.\nIn this example, we can assume that we decide to use a triangular distribution which captures prior beliefs about the parameter \\(\\lambda\\) well because it has a defined range and a defined mode.\nWith the prior assigned and the data collected, we can use the machinery of Bayesian inference.\nIn this case, the prior distibution of the parameter \\(\\lambda\\) is the triangular distribution with the PDF:\n\\[\nf(\\lambda) = \\begin{cases}\n\\frac{1}{15}\\lambda, & \\lambda \\in [0,3] \\\\\n\\frac{1}{35}(10-\\lambda), & \\lambda \\in (3, 10]\n\\end{cases}\n\\]\nWith the \\(16\\) observations, \\(x_1, ..., x_{16}\\), the likelihood is\n\\[\nf(x|\\lambda) = \\prod_{k=1}^{16} e^{-\\lambda \\frac{\\lambda^k}{k!}}\n\\]\nHence, the posterior distribution \\(f(\\lambda|x) \\propto f(x|\\lambda)f(\\lambda)\\), then dividing by the evidence,\n\\[\n\\int_0^{10} f(x|\\lambda)f(\\lambda)d\\lambda\n\\]\n\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n\nComputational Bayes Estimate: 2.055555555555556\n\n\n\n\n\n\n\n8.10.2 Congugate prior example\nThe gamma distribution is said to be a conjugate prior to the Poisson distribution, which means that both the prior distribution and the posterior distribution are gamma distributions when the data is distributional as a Poisson distribution. In this case, the hyper-parameters typically have a simple update law from prior to posterior.\nIn the case of a gamma-Poisson conjugate pair, assume the hyper-parameters of the prior to have \\(\\alpha\\) (shape parameter) and \\(\\beta\\) (rate parameter). We have\n\\[\n\\begin{align}\n\\text{posterior} &\\propto \\text{likelihood}\\times \\text{prior} \\\\\n&\\propto \\left(\\prod_{k=1}^n e^{-\\lambda \\frac{\\lambda^k}{k!}}\\right) \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta\\lambda} \\\\\n&= \\lambda^{\\alpha+(\\sum_{k=1}^{n} x_k)-1}e^{-\\lambda(\\beta+n)} \\\\\n&\\propto \\text{gamma density with shape parameter } \\alpha + \\sum x_i \\text{ and rate parameter } \\beta+n\n\\end{align}\n\\]\nThe hyper-parameter \\(\\alpha\\) is updated to \\(\\alpha + \\sum x_i\\) and the hyper-parameter \\(\\beta\\) is updated to \\(\\beta + n\\).\n\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\ndelta = 10^-4\nlamRange = 0:delta:10\nK = sum([posteriorUpToK(lam) * delta for lam in lamRange])\nposterior(lam) = posteriorUpToK(lam) / K\n\nbayesEsitmate = sum([lam * posterior(lam) * delta for lam in lamRange])\n\nnewAlpha, newBeta = alpha + sum(d), beta + length(d)\nclosedFormBayesEstimate = mean(Gamma(newAlpha, 1 / newBeta))\n\nprintln(\"Computational Bayes Estimate: \", bayesEsitmate)\nprintln(\"Closed form Bayes Estimate: \", closedFormBayesEstimate)\n\nfig, ax = lines(lamRange, prior.(lamRange); color=:blue, label=\"Prior distribution\")\nlines!(ax, lamRange, posterior.(lamRange); color=:red, label=\"Posterior distribution\")\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n\nComputational Bayes Estimate: 2.055555555555556\nClosed form Bayes Estimate: 2.0555555555555554\n\n\n\n\n\n\n\n8.10.3 Markov Chain Monte Carlo example\nIn cases where the dimension of the parameter space is high, carrying out straighforward integration is not possible.\nHowever, there are other ways of carrying out Bayesian inference.\nOne such popular way is by using algorithms that fall under the category known as Markov Chain Monte Carlo (MCMC).\nThe Metropolis-Hastings algorithm is one such popular MCMC algorithm.\nIt produces a series of samples \\(\\theta(1), \\theta(2), \\theta(3), ...\\), where it is guaranteed that for large \\(t\\), \\(\\theta(t)\\) is distributed according to the posterior distribution.\nTechnically, the random sequence \\(\\{\\theta(t)\\}_{t=1}^\\infty\\) is a Markov chain and it is guaranteed that the stationary distribution of this Markov chain is the specified posterior distribution. That is, the posterior distribution is an input parameter to the algorithm.\nThe major bebefits of Metropolis-Hastings and similar MCMC algorithms is that they only use the ratios of the posterior on different parameter values. For example, for parameter \\(\\theta_1\\) and \\(\\theta_2\\), the algorithm only uses the posterior distribution via the ratio\n\\[\nL(\\theta_1, \\theta_2) = \\frac{f(\\theta_1|x)}{f(\\theta_2|x)}\n\\]\nThis means that the normalizing constant (evidence) is not needed as it is implicitly canceled out. Thus, using \\(\\text{psoterior} \\propto \\text{likelihood}\\times \\text{prior}\\) suffices.\nFurther to the posterior distribution, an additional input parameter to Metropolis-Hastings is the so-called proposal density, denoted by \\(q(\\cdot|\\cdot)\\). This is a family of distributions where given a certain value of \\(\\theta_1\\) taken as a parameter, the new value, say \\(\\theta_2\\) is distributed with PDF\n\\[\nq(\\theta_2|\\theta_1)\n\\]\nThe idea of Metropolis-Hastings is to walk around the parameter space by randomly generating new values using \\(q(\\cdot|\\cdot)\\).\nAt each step, some new values are accepted while others are not, all in a manner which ensures the desired limiting behavior. The algorithm specification is to accept with probability\n\\[\nH = \\min\\left\\{1,\\ L(\\theta^*, \\theta(t)) \\frac{q(\\theta(t)|\\theta^*)}{q(\\theta^*|\\theta(t))}\\right\\}\n\\]\nwhere \\(\\theta^*\\) is the new proposed value, generated via \\(q(\\cdot|\\theta(t))\\), and \\(\\theta(t)\\) is the current value.\nWith each such iteration, the new value is accepted with probability \\(H\\) and rejected otherwise. With certian technical requirements on the posterior and proposal densities, the theory of Markov chains then guarantees that the stationary distribution of the sequence \\(\\{\\theta(t)\\}\\) is the posterior distribution.\nDifferent variants of the Metropolis-Hastings algorithm employ different types of proposal densities. There are also generalizations and extensions that we don’t discuss here, such as Gibbs Sampling and Hamiltonian Monte Carlo.\nAs an example, we use the folded normal distribution as a proposal density. This distribution is achieved by taking a normal random variable \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and considering \\(Y=|X|\\). In this case, the PDF of \\(Y\\) is\n\\[\nf(y) = \\frac{1}{\\sigma 2\\pi} \\left(e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} + e^{-\\frac{(y+\\mu)^2}{2\\sigma^2}}\\right)\n\\]\nIt supports to generate the non-negative parameters in question.\n\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\n# prior with Gamma distribution which has two hyper-parameters α and β\nprior(lam) = pdf(Gamma(alpha, 1 / beta), lam)\nd = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\n# data with Poisson distribution which has a parameter λ\nlike(lam) = *([pdf(Poisson(lam), x) for x in d]...)\n# posterior ∝ likelihood * prior\nposteriorUpToK(lam) = like(lam) * prior(lam)\n\nsig = 0.5\n# use the folded normal distribution as the proposal density\n# with each parameter θ1, it produces a new parameter θ2\nfoldedNormalPDF(x, mu) = (1 / sqrt(2 * pi * sig^2)) * (exp(-(x - mu)^2 / 2sig^2) + exp(-(x + mu)^2 / 2sig^2))\nfoldedNormalRV(mu) = abs(rand(Normal(mu, sig)))\n\nfunction sampler(piProb, qProp, rvProp)\n    lam = 1\n    warmN, N = 10^5, 10^6\n    samples = zeros(N - warmN)\n\n    for t in 1:N\n        while true\n            lamTry = rvProp(lam)\n            Lo = piProb(lamTry) / piProb(lam)\n            H = min(1, Lo * qProp(lam, lamTry) / qProp(lamTry, lam))\n            if rand() &lt; H\n                lam = lamTry\n                if t &gt; warmN\n                    samples[t-warmN] = lam\n                end\n                break\n            end\n        end\n    end\n    return samples\nend\n\nmcmcSamples = sampler(posteriorUpToK, foldedNormalPDF, foldedNormalRV)\nprintln(\"MCMC Bayes Estimate: \", mean(mcmcSamples))\n\nfig, ax = stephist(mcmcSamples; bins=100, color=:black, normalization=:pdf, label=\"Histogram of MCMC samples\")\n\nlamRange = 0:0.01:10\nlines!(ax, prior.(lamRange); color=:blue, label=\"Prior distribution\")\n\nclosedFormPosterior(lam) = pdf(Gamma(alpha + sum(d), 1 / (beta + length(d))), lam)\nlines!(ax, lamRange, closedFormPosterior.(lamRange); color=:red, label=\"Posterior distribution\")\n\nax.limits = (0, 10, 0, 1.2)\nax.xlabel = L\"\\lambda\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n\nMCMC Bayes Estimate: 2.066064798662241"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#confidence-intervals",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#confidence-intervals",
    "title": "Probability and statistics with Julia",
    "section": "9 Confidence intervals",
    "text": "9 Confidence intervals\nIn the setting of symmetric sampling distributions, a typical formula for the confidence interval \\([L, U]\\) is of the form\n\\[\n\\hat{\\theta} \\pm K_\\alpha s_\\text{err}\n\\]\nHere, \\(\\hat{\\theta}\\) is typically the point estimate for the parameter in question, \\(s_\\text{err}\\) is some measure or estimate of the variability (e.g. standard error) of the parameter, and \\(K_\\alpha\\) is a constant which depends on the model at hand and on \\(\\alpha\\). Typically by decreasing \\(\\alpha \\rightarrow 0\\), we have that \\(K_\\alpha\\) increases, implying a wider confidence interval. In addition, the specific form of \\(K_\\alpha\\) often depends on conditions such as\n\nSample size: is the sample size small or large.\nVariance: is the variance \\(\\sigma^2\\) known or unknown.\nDistribution: is the population assumed normally distributed or not.\n\n\n9.1 Single sample confidence intervals for the mean\nAssume that we want to estimate the population mean \\(\\mu\\) using a random sample, \\(X_1, ..., X_n\\).\nAs mentioned before, an unbiased point estimate for the mean is the sample mean \\(\\overline{X}\\), so a typical formula for the confidence interval of the mean is\n\\[\n\\overline{X} \\pm K_\\alpha \\frac{S}{\\sqrt{n}}\n\\]\n\n9.1.1 Population variance known\nIf we assume that the population variance \\(\\sigma^2\\) is known and the data is normally distributed, then the sample mean \\(\\overline{X}\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\). This yields\n\\[\nP\\left( \\mu - z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\le \\overline{X} \\le \\mu + z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\right) = 1-\\alpha\n\\]\nwhere \\(z_{1-\\frac{\\alpha}{2}}\\) is the \\(1-\\frac{\\alpha}{2}\\) quantile of the standard normal distribution.\nBy rearranging the inequalities inside the probability statement above, we obtain the following confidence interval formula\n\\[\n\\bar{x} \\pm z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}\n\\]\nIn practice, \\(\\sigma^2\\) is rarely known; hence, it is tempting to replace \\(\\sigma\\) by \\(s\\) (sample standard deviation) in the formula above. Such a replacement is generally fine for large samples.\nIn addition, the validity of the normality assumption should also be considered. In cases where the data is not normally distributed, the probability statement above only approximately holds. However, as \\(n \\rightarrow \\infty\\), it quickly becomes precise due to the central limit theorem.\n\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\nsig = 1.2\nalpha = 0.1\nz = quantile(Normal(), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - z * sig / sqrt(n), xBar + z * sig / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleZTest(xBar, sig, n); level=1 - alpha))\n\nCalculating formula: (52.514845578531826, 53.39756666402797)\nUsing confint() function: (52.514845578531826, 53.39756666402797)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAssume \\(U \\sim N(0, 1)\\), and \\(X \\sim N(\\mu, \\sigma)\\), we have\n\\[\nU = \\frac{X-\\mu}{\\sigma}\n\\]\n\\[\nX = \\mu + U\\sigma\n\\]\n\\[\nP(-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}) = 1-\\alpha\n\\]\n\\[\nP(\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma) = 1-\\alpha\n\\]\n\n\n\n\n9.1.2 Population variance unknown\nIn cases where the population variance is unknown, if we replace \\(\\sigma\\) by the sample standard deviation \\(s\\), we can use the T-distribution instead of the normal distribution to obtain the confidence interval for the mean\n\\[\n\\bar{x} \\pm t_{1-\\frac{\\alpha}{2}, n-1} \\frac{s}{\\sqrt{n}}\n\\]\nwhere \\(t_{1-\\frac{\\alpha}{2}, n-1}\\) is the \\(1-\\frac{\\alpha}{2}\\) quantile of a T-distribution with \\(n-1\\) degrees of freedom.\nFor small samples, the replacement of \\(z_{1-\\frac{\\alpha}{2}}\\) by \\(t_{1-\\frac{\\alpha}{2}, n-1}\\) significantly affects the width of the confidence interval, as for the same value of \\(\\alpha\\), the T case is wider.\nHowever, as \\(n \\rightarrow \\infty\\), we have, \\(t_{1-\\frac{\\alpha}{2}, n-1} \\rightarrow z_{1-\\frac{\\alpha}{2}}\\).\n\nusing Distributions, HypothesisTests\n\nd = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nxBar, n = mean(d), length(d)\ns = std(d)\nalpha = 0.1\nt = quantile(TDist(n - 1), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar - t * s / sqrt(n), xBar + t * s / sqrt(n)))\nprintln(\"Using confint() function: \", confint(OneSampleTTest(xBar, s, n); level=1 - alpha))\n\nCalculating formula: (52.499893857795534, 53.41251838476426)\nUsing confint() function: (52.499893857795534, 53.41251838476426)\n\n\n\n\n\n9.2 Two sample confidence intervals for the difference in means\nIt is often of interest to estimate the difference between the population means, \\(\\mu_1 - \\mu_2\\).\nFirst we collect two random samples, \\(x_{i,1}, ..., x_{i,n}\\) for \\(i = 1,2\\), each with the sample mean \\(\\bar{x}_i\\) and sample standard deviation \\(s_i\\). In addition, the difference of sample means, \\(\\bar{x}_1 - \\bar{x}_2\\) serves as a point estimate for the difference in population means, \\(\\mu_1 - \\mu_2\\).\nA confidence interval for \\(\\mu_1 - \\mu_2\\) around the point estimate \\(\\bar{x}_1 - \\bar{x}_2\\) is then constructed via the same process seen previously\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm K_\\alpha s_\\text{err}\n\\]\n\n9.2.1 Population variances known\nIn cases where the population variances are known, we may explicitly compute\n\\[\nVar(\\overline{X}_1 - \\overline{X}_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\]\nHence, the standard error is given by\n\\[\ns_\\text{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\\]\nWhen combined with the assumption that the data is normally distributed, we can derive the following confidence interval\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\\]\n\n\n9.2.2 Population variances unknown and assumed equal\nIn cases where the population variances are unknown but assumed equal, denoted by \\(\\sigma^2\\). Based on this assumption, it is sensible to use both sample variances to estimate \\(\\sigma^2\\). This estimated variance using both samples is known as the pooled sample variance, and is given by\n\\[\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n\\]\nIn fact, it is a weighted average of the sample variances of the individual samples.\nIn this case, it can be shown that\n\\[\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{S_\\text{err}}\n\\]\nis distributed as a T-distribution with \\(n_1+n_2-2\\) degreees of freedom, where the standard error is\n\\[\nS_\\text{err} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nHence, we arrive at the following confidence interval\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, n_1+n_2-2} S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\n\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\nt = quantile(TDist(n1 + n2 - 2), 1 - alpha / 2)\n\ns1, s2 = std(d1), std(d2)\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sP * sqrt(1 / n1 + 1 / n2), xBar1 - xBar2 + t * sP * sqrt(1 / n1 + 1 / n2)))\nprintln(\"Using confint() function: \", confint(EqualVarianceTTest(d1, d2); level=1 - alpha))\n\nCalculating formula: (1.1127539566753217, 2.9048648558844814)\nUsing confint() function: (1.1127539566753217, 2.9048648558844814)\n\n\n\n\n9.2.3 Population variances unkown and not assumed equal\nIn cases where the population variances are unknown and not assumed equal, the estimate for \\(S_\\text{err}\\) is given by\n\\[\nS_\\text{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\\]\nHence, in this case the statistic \\(T = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{S_\\text{err}}\\) is adapted to\n\\[\nT = \\frac{\\overline{X}_1 - \\overline{X}_2 - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\\]\nIt turns out that the above formula is only T-distributed with \\(n_1+n_2-2\\) degrees of freedom if the variances are equal, otherwise, it isn’t.\nHowever, the Satterthwaite approximation suggests that \\(T\\ \\ \\widetilde{approx}\\ \\ t(v)\\), where the degrees of freedom \\(v\\) is\n\\[\nv = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(s_1^2/n_1\\right)^2}{n_1-1} + \\frac{\\left(s_2^2/n_2\\right)^2}{n_2-1}}\n\\]\nHence, we arrive at the confidence interval\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm t_{1-\\frac{\\alpha}{2}, v} \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\\]\n\nusing CSV, Distributions, HypothesisTests\n\nd1 = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\n\nd2 = [\n    48.23239011,\n    52.2735178,\n    52.07209917,\n    51.8813638,\n    50.89860065,\n    53.13910845,\n    50.88296219,\n    49.80725709,\n    49.04791179,\n    50.91491626,\n    50.73578183,\n    47.6154076,\n    50.89317122,\n    52.95593896,\n    51.90831274,\n    52.22159829,\n    51.60575821,\n    49.96704471,\n]\n\nxBar1, xBar2 = mean(d1), mean(d2)\ns1, s2 = std(d1), std(d2)\nn1, n2 = length(d1), length(d2)\nalpha = 0.05\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\nt = quantile(TDist(v), 1 - alpha / 2)\n\nprintln(\"Calculating formula: \", (xBar1 - xBar2 - t * sqrt((s1^2 / n1 + s2^2 / n2)), xBar1 - xBar2 + t * sqrt((s1^2 / n1 + s2^2 / n2))))\nprintln(\"Using confint() function: \", confint(UnequalVarianceTTest(d1, d2); level=1 - alpha))\n\nCalculating formula: (1.0960161140340268, 2.9216026985257764)\nUsing confint() function: (1.0960161140340265, 2.921602698525777)\n\n\n\n\n9.2.4 More on the Satterthwaite approximation\nObserve that both sides of the “distributed as” (\\(\\sim\\)) symbol are random variables which depdend on the same random experiment.\nHence, the statement can be presented generally, as a case of the following format:\n\\[\nX(\\omega) \\sim F_{h(\\omega)}\n\\]\nwhere \\(\\omega\\) is a point in the sample space.\nHere, \\(X(\\omega)\\) is a random variabe, and \\(F\\) is a distribution that depends on a parameter \\(h\\), which depends on \\(\\omega\\).\nIn our case of the Satterthwaite approximation, \\(h\\) is \\(v\\), defined above.\n\n\n\n\n\n\nWhy both sides of the \\(\\sim\\) symbol are random variables which depend on the same random experiment\n\n\n\nWhen we had finished a random experiment, we obtained a sample \\(\\omega\\) from the sample space.\nThen, we can derive some statistic (\\(X(\\omega)\\)) from the sample, which means that \\(X\\) is a random variable. In addition, under the same experiment, \\(X\\) should be distributed as some distribution \\(F\\), which may be defined by some parameter \\(h\\), saying the degrees of freedom. Obviously, \\(h\\) itself depends on the same sample obtained from the sample space before.\n\n\nBy using the inverse probability transformation, the above formula is equivalent to\n\\[\nF_{h(\\omega)}\\left(X(\\omega)\\right) \\sim \\text{Uniform}(0, 1)\n\\]\nThis means that the Satterthwaite approximation is a better approximative distribution close to the true distribution which \\(T\\) calculated in the case of population variances unknown and not assumed equal is distributed as in comparison with the alternative of treating \\(h\\) as simply dependent on the number of observations made (\\(n_1+n_2-2\\)).\nWe can make a simple validation using Q-Q plot: \\(t(v)\\) is more similar with the true distribution of \\(T\\) than \\(t(n_1+n_2-2)\\)\n\nusing Distributions, Statistics, Random, CairoMakie\n\nRandom.seed!(0)\n\nmu1, sig1, n1 = 0, 2, 8\nmu2, sig2, n2 = 0, 30, 15\n\ndist1 = Normal(mu1, sig1)\ndist2 = Normal(mu2, sig2)\n\nN = 10^6\ntdArray = Array{Tuple{Float64,Float64}}(undef, N)\n\nfunc(s1, s2, n1, n2) = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\n\nfor i in 1:N\n    x1Data = rand(dist1, n1)\n    x2Data = rand(dist2, n2)\n    x1Bar, x2Bar = mean(x1Data), mean(x2Data)\n    s1, s2 = std(x1Data), std(x2Data)\n    tStatistics = (x1Bar - x2Bar) / sqrt(s1^2 / n1 + s2^2 / n2)\n    tdArray[i] = (tStatistics, func(s1, s2, n1, n2))\nend\nsort!(tdArray, by=first)\n\ninvVal(v, i) = quantile(TDist(v), i / (N + 1))\n\nxCoords = Array{Float64}(undef, N)\nyCoords1 = Array{Float64}(undef, N)\nyCoords2 = Array{Float64}(undef, N)\n\nfor i in 1:N\n    xCoords[i] = first(tdArray[i])\n    yCoords1[i] = invVal(last(tdArray[i]), i)\n    yCoords2[i] = invVal(n1 + n2 - 2, i)\nend\n\nfig, ax = qqplot(xCoords, yCoords1; color=:blue, label=\"Calculated v\", qqline=:identity)\nqqplot!(ax, xCoords, yCoords2; color=:red, label=\"Fixed v\")\naxislegend(ax; position=:lt)\nfig\n\n\n\n\n\n\n\n9.3 Confidence intervals for proportions\nIn certain inference settings the parameter of interest is a population proportion.\nWhen carrying out inference for a proportion we assume that there exists some unknown population proportion \\(p \\in (0, 1)\\).\nWe then sample an i.i.d. sample of observations \\(I_1, ..., I_n\\), where for the \\(i\\)’th observation, \\(I_i = 0\\) if the event in question does not happen, and \\(I_i = 1\\) if the event occurs.\nA natural estimator for the proportion is then the sample mean of \\(I_1, ..., I_n\\), which we denote\n\\[\n\\hat{p} = \\frac{\\sum_{i=1}^{n}I_i}{n}\n\\]\nPlease note that \\(\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\) as \\(n \\rightarrow \\infty\\), so is \\(\\hat{p}\\).\nNow observe that each \\(I_i\\) is a Bernoulli random variable with success probability \\(p\\). Under the i.i.d assumption this means that the numerator of the above formula (i.e. \\(\\sum_{i=1}^{n}I_i\\)) is binomially distributed with parameters \\(n\\) and \\(p\\). Hence\n\\[\nE\\left[\\sum_{i=1}^{n}I_i\\right] = np\n\\]\nand\n\\[\n\\text{Var}(\\sum_{i=1}^{n}I_i) = np(1-p)\n\\]\nSo we have\n\\[\nE(\\hat{p}) = p\n\\]\nand\n\\[\n\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}\n\\]\nHence, \\(\\hat{p}\\) is an unbiased and consistent estimator of \\(p\\). That is, on average \\(\\hat{p}\\) estimates \\(p\\) perfectly (unbiased), and if more observations are collected the variance of the estimator vanishes and then \\(\\hat{p} \\rightarrow p\\) (consistent).\nFurthermore, we can use the central limit theorem to create a normal approximation for the distribution of \\(\\hat{p}\\) and yield an approximate condidence interval.\nDue to the fact \\(\\hat{p} \\sim N(p, \\frac{p(1-p)}{n})\\) as \\(n \\rightarrow \\infty\\), we have\n\\[\n\\tilde{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\n\\]\nwhich is a random variable that approximately follows a standard normal distribution. The approximation becomes exact as \\(n\\) grows. Due to the fact that \\(\\hat{p}\\) is an unbiased and consistent estimator of \\(p\\), it’s reasonable that we replace \\(p\\) with \\(\\hat{p}\\), and then we have\n\\[\n\\hat{Z}_n = \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}}\n\\]\nSo we have\n\\[\nP(z_\\frac{\\alpha}{2} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\approx 1-\\alpha\n\\]\nand then along with the fact that \\(z_\\frac{\\alpha}{2} = -z_{1-\\frac{\\alpha}{2}}\\) as follows\n\\[\n\\begin{align}\n1-\\alpha &\\approx P(-z_{1-\\frac{\\alpha}{2}} \\le \\hat{Z}_n \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(-z_{1-\\frac{\\alpha}{2}} \\le \\frac{\\hat{p}-p}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}} \\le z_{1-\\frac{\\alpha}{2}}) \\\\\n&= P(\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le p \\le \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}})\n\\end{align}\n\\]\nWe thus arrive the following approximate confidence interval for proportions\n\\[\n\\hat{p} \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nSimilarly, we have the confidence interval for the case of two populations\n\\[\n\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n}}\n\\]\n\nusing CSV, DataFrames, CategoricalArrays, Distributions\n\ndat = CSV.read(\"./data/purchaseData.csv\", DataFrame)\nprintln(\"Levels of Grade: \", levels(dat.Grade))\nprintln(\"Data points: \", nrow(dat))\n\nn = sum(.!(ismissing.(dat.Grade)))\nprintln(\"Non-missing data points: \", n)\ndat2 = dropmissing(dat[:, [:Grade]], :Grade)\n\ngradeInQuestion = \"E\"\nindicatorVector = dat2.Grade .== gradeInQuestion\nnumSuccess = sum(indicatorVector)\nphat = numSuccess / n\nserr = sqrt(phat * (1 - phat) / n)\n\nalpha = 0.05\nconfidencePercent = 100 * (1 - alpha)\nzVal = quantile(Normal(), 1 - alpha / 2)\nconfInt = (phat - zVal * serr, phat + zVal * serr)\n\nprintln(\"\\nOut of $n non-missing observations, $numSuccess are at level $gradeInQuestion.\")\nprintln(\"Hence a point estimate for the proportion of grades at level $gradeInQuestion is $phat.\")\nprintln(\"A $confidencePercent% confidence interval for the proportion of level $gradeInQuestion is:\\n$confInt\")\n\nLevels of Grade: String1[\"A\", \"B\", \"C\", \"D\", \"E\"]\nData points: 200\nNon-missing data points: 187\n\nOut of 187 non-missing observations, 61 are at level E.\nHence a point estimate for the proportion of grades at level E is 0.32620320855614976.\nA 95.0% confidence interval for the proportion of level E is:\n(0.2590083767381328, 0.3933980403741667)\n\n\n\n9.3.1 Sample size planing\nDenote the confidence interval of the proportion as \\(\\hat{p} \\pm E\\) where \\(E\\) is the margin of error or half the width of the confidence interval, denoted by\n\\[\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nWe may often want to plan an experiment or a sampling scheme such that \\(E\\) is not too wide.\nSay we want \\(E \\le \\epsilon\\) with the confidence probability \\(1-\\alpha\\), we have\n\\[\nE = z_{1-\\frac{\\alpha}{2}}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\le \\epsilon\n\\]\nGiven that \\(x(1-x)\\) is maximized at \\(x=1/2\\) with a maximal value of \\(1/4\\). Hence,\n\\[\nE \\le \\frac{z_{1-\\frac{\\alpha}{2}}}{2\\sqrt{n}} \\le \\epsilon\n\\]\nFinally, we have\n\\[\nn \\ge \\frac{z_{1-\\frac{\\alpha}{2}}^2}{4\\epsilon^2}\n\\]\n\n\n9.3.2 Validity of the approximation\nIn many cases, this confidence interval approximation for the proportion works well, however for small sample sizes \\(n\\) or values of \\(p\\) near \\(0\\) or \\(1\\), this is often too crude of an approximation. A consequence is that one may obtain a confidence interval that isn’t actually a \\(1-\\alpha\\) confidence interval, but rather has a different coverage probability.\nOne common rule of thumb used to decide if the confidence interval for the proportion is valid is to require that both the product \\(np\\) and the product \\(n(1-p)\\) be at least \\(10\\).\nHere, we expore some combinations of \\(n\\) and \\(p\\). For each combination, we use \\(N\\) Monte Carlo expriments and calculate the following\n\\[\n(1-\\alpha) - \\frac{1}{N} \\sum_{k=1}^{N}\\mathbf{1}\\left\\{p \\in \\left[\\hat{p} - z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\right]\\right\\}\n\\]\nThis estimated difference of the actual converage probability of the confidence interval and the desired confidence level \\(1-\\alpha\\) is a measure of the accuracy of the confidence level. We expect this difference to be almost \\(0\\) if the approximation is good. Otherwise, a higher absolute difference is oberved.\n\nusing Random, Distributions, CairoMakie\n\nN = 5e3\nalpha = 0.05\nconfLevel = 1 - alpha\nz = quantile(Normal(), 1 - alpha / 2)\n\nfunction randCI(n, p)\n    sample = rand(n) .&lt; p\n    pHat = sum(sample) / n\n    serr = sqrt(pHat * (1 - pHat) / n)\n    (pHat - z * serr, pHat + z * serr)\nend\ncover(p, ci) = ci[1] &lt;= p && p &lt;= ci[2]\n\npGrid = 0.1:0.01:0.9\nnGrid = 5:1:50\nerrs = zeros(length(nGrid), length(pGrid))\n\nfor i in 1:length(nGrid)\n    for j in 1:length(pGrid)\n        Random.seed!(1234)\n        n, p = nGrid[i], pGrid[j]\n        coverageRatio = sum([cover(p, randCI(n, p)) for _ in 1:N]) / N\n        errs[i, j] = confLevel - coverageRatio\n    end\nend\n\nfig, ax1, hm1 = heatmap(pGrid, nGrid, errs'; colormap=to_colormap([\"white\", \"black\"]))\nax2, hm2 = heatmap(fig[2, 1], pGrid, nGrid, errs' .&lt;= 0.04; colormap=to_colormap([\"black\", \"white\"]))\nColorbar(fig[1, 2], hm1)\nfig\n\n\n\n\n\n\n\n9.4 Confidence interval for the variance of a normal population\nFirst, a point estimator of the population variance is the sample variance\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2\n\\]\nWe have known that \\(\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi_{n-1}^2\\). So denoting the \\(\\gamma\\)-quantile of this distribution via \\(\\chi_{\\gamma,n-1}^2\\), we have\n\\[\nP(\\chi_{\\frac{\\alpha}{2},n-1}^2 \\le \\frac{(n-1)s^2}{\\sigma^2} \\le \\chi_{1-\\frac{\\alpha}{2},n-1}^2) = 1-\\alpha\n\\]\nThen we have\n\\[\nP(\\frac{(n-1)s^2}{\\chi_{1-\\frac{\\alpha}{2},n-1}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{\\frac{\\alpha}{2},n-1}^2}) = 1-\\alpha\n\\]\nNote: this equality holds only if the data is normally distributed.\n\nusing Distributions\n\ndat = rand(Normal(0, 1), 1000)\nn, s, alpha = length(dat), std(dat), 0.05\n\nci = ((n - 1) * s^2 / quantile(Chisq(n - 1), 1 - alpha / 2),\n    (n - 1) * s^2 / quantile(Chisq(n - 1), alpha / 2))\n\nprintln(\"Point estimate for the variance: \", s^2)\nprintln(\"Confidence interval for the variance: \", ci)\n\nPoint estimate for the variance: 0.9999905881991535\nConfidence interval for the variance: (0.9177790230416273, 1.0938240508530526)\n\n\n\n9.4.1 Sensitivity of the normal assumption\nAs mentioned before, this confidence interval for the variance of a normal population only holds for normally distributed population and is more sensitive about the normal assumption than the other confidence intervals constructed before.\nWe’ll find that the sample variance distributions of a normal distribution and a logistic distribution are quite different, though they have the same pupulation mean \\(\\mu\\) and population variance \\(\\sigma^2\\), and the PDF’s curve shape of the logistic distribution is somewhat similar with the normal.\nThe logistic distribution is defined by the location and scale parameters \\(\\mu\\) and \\(\\eta\\), and the PDF is\n\\[\nf(x) = \\frac{e^{-\\frac{x-\\mu}{\\eta}}}{\\eta(1+e^{-\\frac{x-\\mu}{\\eta}})^2}\n\\]\nwith mean \\(\\mu\\) and variance \\(\\sigma^2 = \\eta^2\\pi^2/3\\).\n\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\n\nn, N = 15, 10^7\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nxGrid = -8:0.1:12\n\nsNormal = [var(rand(dNormal, n)) for _ in 1:N]\nsLogistic = [var(rand(dLogistic, n)) for _ in 1:N]\n\nfig = Figure()\nax1 = Axis(fig[1, 1];\n    xlabel=\"x\",\n    ylabel=\"Density\")\nlines!(ax1, xGrid, pdf.(dNormal, xGrid); color=:blue, label=\"Normal\")\nlines!(ax1, xGrid, pdf.(dLogistic, xGrid); color=:red, label=\"Logistic\")\naxislegend(ax1)\nax2 = Axis(fig[2, 1],\n    xlabel=\"Sample Variance\",\n    ylabel=\"Density\",\n    limits=(0, 30, 0, 0.14))\nstephist!(ax2, sNormal; bins=200, color=:blue, normalization=:pdf, label=\"Normal\")\nstephist!(ax2, sLogistic; bins=200, color=:red, normalization=:pdf, label=\"Logistic\")\naxislegend(ax2)\nfig\n\n\n\n\nIn addition, we can check the actual confidence interval coverage under different model assumptions:\n\nusing Distributions, CairoMakie\n\nmu, sig = 2, 3\neta = sqrt(3) * sig / pi\nn, N = 15, 10^4\ndNormal = Normal(mu, sig)\ndLogistic = Logistic(mu, eta)\nalphaUsed = 0.001:0.001:0.1\n\nfunction alphaSimulator(dist, n, alpha)\n    popVar = var(dist)\n    coverageCount = 0\n    for _ in 1:N\n        sVar = var(rand(dist, n))\n        Lo = (n - 1) * sVar / quantile(Chisq(n - 1), 1 - alpha / 2)\n        Up = (n - 1) * sVar / quantile(Chisq(n - 1), alpha / 2)\n        coverageCount += Lo &lt; popVar && popVar &lt; Up\n    end\n    1 - coverageCount / N\nend\n\nfig, ax = ablines(0, 1; color=:green, label=\"y = x\")\nscatter!(ax, alphaUsed, alphaSimulator.(dNormal, n, alphaUsed); color=:blue, label=\"Normal\")\nscatter!(ax, alphaUsed, alphaSimulator.(dLogistic, n, alphaUsed); color=:red, label=\"Logistic\")\naxislegend(ax; position=:lt)\nfig\n\n\n\n\n\n\n\n9.5 Bootstrap confidence intervals\nBootstrap, also called empirical bootstrap, is a useful technique which relies on resampling from the observed data \\(x_1, ..., x_n\\) with replacement in order to empirically construct the distribution of the point estimator for some unknown population parameters.\nOne way in which this resampling can be conducted is to apply the inverse probability transform on the empirical cumulative distribution function.\n\n\n\n\n\n\nNote\n\n\n\n\nObtain a sample: \\(\\mathbf{X} = (x_1, ..., x_n)\\).\nObtain the empirical cumulative distribution function \\(F_X(\\mathbf{X})\\) based on \\(\\mathbf{X}\\).\nUse the inverse probability transformation: \\(\\mathbf{U} = F_X(\\mathbf{X}) \\Longrightarrow \\mathbf{X} = F_X^{-1}(\\mathbf{U})\\).\nResample \\(\\mathbf{X}\\) via resampling \\(\\mathbf{U} \\sim U(0, 1)\\) with replacement.\n\nIt seems that we sample a large number of “new” samples from the population, and then we can get a large number of point estimators, which then are used to construct the empirical distribution of the point estimator. Finally, to get the \\(1-\\alpha\\) confidence interval, we can just get the quantiles of \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\).\nBootstrap requires that the number of observations is not too small.\n\n\nHowever, from an implementation perspective, a simpler alternative is to consider the data points \\(x_1, ..., x_n\\), and then randomly sample \\(n\\) discrete uniform indexes, \\(j_1, ..., j_n\\) each in the range \\(\\{1, ..., n\\}\\). The resampled data denoted by \\(x^* = (x_1^*, ..., x_n^*)\\) is then\n\\[\nx^* = (x_{j_1}, ..., x_{j_n})\n\\]\nThat is, each point in the resampeld data is a random observation from the original data, where we allow to sample with replacement.\nIn Julia, \\(x^* = \\text{rand}(\\mathbf{X}, n)\\), where we use the rand() method to uniformally sample \\(n\\) random copies of elements in \\(\\mathbf{X}\\) with replacement.\nThe idea of empirical bootstrap is now to repeat the resampling a large number of times, say \\(N\\), and for each resampled data vector, \\(x^*(1), ..., x^*(N)\\) to compute the parameter estimate. If the parameter estimate is denoted by the function \\(h: R^n \\mapsto R\\), then we end up with values\n\\[\n\\begin{align}\nh^*(1) &= h(x_1^*(1), ..., x_n^*(1)) \\\\\nh^*(2) &= h(x_1^*(2), ..., x_n^*(2)) \\\\\n&\\vdots \\\\\nh^*(N) &= h(x_1^*(N), ..., x_n^*(N))\n\\end{align}\n\\]\nFinally, a bootstrap confidence interval is determined by computing the respective lower and upper \\((\\frac{\\alpha}{2}, 1-\\frac{\\alpha}{2})\\) quantiles of the sequence \\(h^*(1), ..., h^*(N)\\).\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nsampleData = [\n    53.35674558144255,\n    53.45887516134873,\n    52.282838627926125,\n    52.98746570643515,\n    51.993167774733486,\n    53.373333606198,\n    55.75410538860477,\n    50.279496381439365,\n    53.6359586914001,\n    53.517705831707495,\n    53.70044994508253,\n    54.15592592604583,\n    53.55054914606446,\n    52.37319589109419,\n    53.4900750059897,\n    52.939458524079605,\n    52.16761562743534,\n    50.87140009591033,\n    53.144919157924924,\n    52.09084035473537,\n]\nn, N = length(sampleData), 10^6\nalpha = 0.1\n\n# sampling with replacement\nbootstrapSampleMeans = [mean(rand(sampleData, n)) for _ in 1:N]\nLmean = quantile(bootstrapSampleMeans, alpha / 2)\nUmean = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n\nprintln(\"Bootstrap confidence interval for the mean: \", (Lmean, Umean))\n\nfig, ax = stephist(bootstrapSampleMeans; bins=1000, color=:blue, normalization=:pdf, label=\"Sample means\")\nvlines!(ax, [Lmean, Umean]; color=:red, label=\"90% CI\")\naxislegend(ax)\nfig\n\nBootstrap confidence interval for the mean: (52.52963545344743, 53.375601378187525)\n\n\n\n\n\nSimply, we can carry out a computational experiment to show that if the number of sample observations is not very large, then the coverage probability of bootstrapped confidence interval is only approximately \\(1-\\alpha\\), but not exactly. However, as the sample size \\(n\\) grows, the coverage probability converges to the desired \\(1-\\alpha\\).\n\nusing Random, Distributions\n\nRandom.seed!(0)\n\nlambda = 0.1\ndist = Exponential(1 / lambda)\nactualMedian = median(dist)\n\nM = 10^3\nN = 10^4\nnRange = 2:2:20\nalpha = 0.05\n\nfor n in nRange\n    coverageCount = 0\n    for _ in 1:M\n        sampleData = rand(dist, n)\n        bootstrapSampleMeans = [median(rand(sampleData, n)) for _ in 1:N]\n        Lo = quantile(bootstrapSampleMeans, alpha / 2)\n        Up = quantile(bootstrapSampleMeans, 1 - alpha / 2)\n        coverageCount += Lo &lt; actualMedian && actualMedian &lt; Up\n    end\n    println(\"n = \", n, \"\\t coverage = \", coverageCount / M)\nend\n\nn = 2    coverage = 0.499\nn = 4    coverage = 0.889\nn = 6    coverage = 0.942\nn = 8    coverage = 0.924\nn = 10   coverage = 0.936\nn = 12   coverage = 0.934\nn = 14   coverage = 0.943\nn = 16   coverage = 0.945\nn = 18   coverage = 0.954\nn = 20   coverage = 0.927\n\n\n\n\n9.6 Prediction intervals\nA prediction interval tells us a predicted range that a single next observation of data is expected to fall within with some level of confidence. This differs from a confidence interval which indicates how confident we are of a particular parameter that we are trying to estimate.\nIn brief, a prediction interval is constructed based on the distribution of a population itself from which we sample observations, while a confidence interval is constructed based on the distribution of a particular parameter which we are trying to estimate (e.g. \\(\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\)).\nSuppose we have a sequence of data points (observations) \\(x_1, x_2, x_3, ...\\), which come from a normal distribution and are assumed i.i.d. Further assume that we observed the first \\(n\\) data points \\(x_1, ..., x_n\\), but have not yet observed \\(X_{n+1}\\). Note that we use lower case \\(x\\) for values observed and upper case \\(X\\) for yet unobserved random variables.\nIn this case, a \\(1-\\alpha\\) prediction interval for the single future observation \\(X_{n+1}\\) is given by\n\\[\n\\bar{x} - t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}} \\le X_{n+1} \\le \\bar{x} + t_{1-\\frac{\\alpha}{2},n-1}s\\sqrt{1+\\frac{1}{n}}\n\\]\nwhere \\(\\bar{x}\\) and \\(s\\) are respectively the sample mean and sample standard deviation computed from \\(x_1, ..., x_n\\).\nNote that as the number of observations \\(n\\) grows, the bounds of the prediction interval decreases towards\n\\[\n\\bar{x} - z_{1-\\frac{\\alpha}{2}}s \\le X_{n+1} \\le \\bar{x} - z_{1-\\frac{\\alpha}{2}}s\n\\]\nand finally has an expected width close to \\(2z_{1-\\frac{\\alpha}{2}}\\sigma\\).\n\n\n\n\n\n\nNote\n\n\n\nSuppose \\(X\\) is a normally distributed variable (i.e. \\(X \\sim N(\\mu, \\sigma^2)\\)), and then we have \\(U = \\frac{X-\\mu}{\\sigma} \\sim N(0, 1)\\).\nFor \\(U\\), we have a \\(1-\\alpha\\) prediction interval \\(-z_{1-\\frac{\\alpha}{2}} \\le U \\le z_{1-\\frac{\\alpha}{2}}\\), and then we have \\(\\mu - z_{1-\\frac{\\alpha}{2}}\\sigma \\le X \\le \\mu + z_{1-\\frac{\\alpha}{2}}\\sigma\\), which is a \\(1-\\alpha\\) prediction interval of \\(X\\) in the case of population mean and population variance known.\nSimply, a prediction interval is such an interval constructed based on the distribution of observations and needs to cover a \\(1-\\alpha\\) proportion of this distribution.\n\n\n\nusing Random, Statistics, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nmu, sig = 50, 5\ndist = Normal(mu, sig)\nalpha = 0.01\nnMax = 40\n\nobservations = rand(dist, 1)\npiLarray, piUarray = [], []\n\nfor _ in 2:nMax\n    xNew = rand(dist)\n    push!(observations, xNew)\n\n    xBar, sd = mean(observations), std(observations)\n    n = length(observations)\n    tVal = quantile(TDist(n - 1), 1 - alpha / 2)\n    delta = tVal * sd * sqrt(1 + 1 / n)\n    piL, piU = xBar - delta, xBar + delta\n\n    push!(piLarray, piL)\n    push!(piUarray, piU)\nend\n\nfig, ax = scatter(1:nMax, observations; color=:blue, label=\"Observations\")\nscatterlines!(2:nMax, piUarray; marker=:cross, color=:red, markercolor=:black, label=\"Prediction Interval\")\n\nscatterlines!(2:nMax, piLarray; marker=:cross, color=:red, markercolor=:black)\naxislegend(ax)\nax.xlabel = \"Number of observations\"\nax.ylabel = \"Value\"\nfig\n\n\n\n\n\n\n9.7 Credible intervals\nThe concept of credible intervals comes from the field of Bayesian statistics.\nIn general, we often need to find an interval \\([l, u]\\) such that given some probability density \\(f(x)\\), the interval satisfies\n\\[\n\\int_l^u f(x)dx = 1-\\alpha\n\\]\nThis is needed for confidence intervals, prediction intervals, and credible intervals.\nHowever, as long as \\(\\alpha &lt; 1\\), there is more than one interval \\([l, u]\\) that satisfies the above formula.\nIn certain cases, there is a “natural” interval. For example, for a symmetric distribution, using equal quantiles is natural (i.e. \\(l = z_{\\frac{\\alpha}{2}}\\) and \\(u = z_{1-\\frac{\\alpha}{2}}\\)). In such cases, the mean is also the median and further if the density is unimodal (has a single maximum) the mean and median are also the mode.\nHowever, consider asymmetric distribution, there isn’t an immediate “natrual” choice for \\(l\\) and \\(u\\). There are three types of intervals we often use:\n\nClassic interval: this type of interval has the mode of the density (assuming the density is unimodal) at its center between \\(l\\) and \\(u\\). An alternative is to use mean or median at the center. That is, assuming the centrality measure (mode, mean, or median) is \\(m\\), we have \\([l, u] = [m-E, m+E]\\). One way to define \\(E\\) is via\n\n\\[\nE = \\max\\{\\epsilon \\ge 0: \\int_{m-\\epsilon}^{m+\\epsilon} f(x)dx \\le 1-\\alpha\\}\n\\]\n\nEqual tail interval: this type of interval simply sets \\(l\\) and \\(u\\) as the \\(\\frac{\\alpha}{2}\\) and \\(1-\\frac{\\alpha}{2}\\) quantiles, respectively. Namely,\n\n\\[\n\\frac{\\alpha}{2} = \\int_{-\\infty}^l f(x)dx\n\\]\nand\n\\[\n1 - \\frac{\\alpha}{2} = \\int_{u}^{\\infty} f(x)dx\n\\]\n\nHighest density interval: this type of interval seeks to cover the part of the support that is most probable. A consequence is that if the density is unimodal then this highest density interval is also the narrowest possible confidence interval. We can crudely does so by starting with a high-density value and decreasing it gradually while seeking for the associated interval \\([l, u]\\). An alternative would be to gradually increment \\(l\\) each time finding a corresponding \\(u\\) that satisfies the above formula and within this search to choose the interval that minimizes the width \\(u-l\\).\n\nFor a symmetric and unimodal distribution, all three of these confidence intervals agree.\nWe now explain the credible intervals. In the Bayesian setting, we treat the unknown parameter \\(\\theta\\) as a random variable, this is totally different from the classical case where we treat the unknown parameter \\(\\theta\\) as a fixed value. The process of inference is based on observing data, \\(x = (x_1, ..., x_n)\\) which has a distribution depending on the unknown parameter \\(\\theta\\), and fusing it with the prior distribution \\(f(\\theta)\\) that the unknown parameter \\(\\theta\\) is distributed as before observing data \\(x = (x_1, ..., x_n)\\) to obtain the posterior distribution \\(f(\\theta|x)\\) which the unknown parameter \\(\\theta\\) is distributed as after observing data \\(x = (x_1, ..., x_n\\).\nHere too, as in the frequentist case, we may wish to describe an interval where it is likely that our parameter lies. Then for a fixed confidence interval \\(1-\\alpha\\), seek \\([l, u]\\), such that\n\\[\n\\int_l^u f(\\theta|x)d\\theta = 1-\\alpha\n\\]\nThere is a basic difference between confidence intervals in the frequentist setting and credible intervals in the Bayesian setting:\nFor a given \\(1-\\alpha\\) interval \\([L, U]\\),\n\nIn the frequentist setting, the unknown parameter \\(\\theta\\) is a fixed value while \\(L\\) and \\(U\\) are random variables depending on observed data \\(X\\) which is a random variable. So this is why we often say the confidence interval \\([L, U]\\) will cover the unknown but fixed parameter \\(\\theta\\) under the confidence level \\(1-\\alpha\\).\nIn the Bayesian setting, the unknown parameter \\(\\theta\\) is a random variable while \\(L\\) and \\(U\\) are deterministic values. So we will see the unknown parameter \\(\\theta\\) will fall within the credible interval \\([L, U]\\) with the probability \\(1-\\alpha\\).\n\n\nusing Distributions, CairoMakie\n\nalpha, beta = 8, 2\ndat = [2, 1, 0, 0, 1, 0, 2, 2, 5, 2, 4, 0, 3, 2, 5, 0]\n\nnewAlpha, newBeta = alpha + sum(dat), beta + length(dat)\npost = Gamma(newAlpha, newBeta)\n\nxGrid = quantile(post, 0.01):0.001:quantile(post, 0.99)\nsignificance = 0.9\nhalfAlpha = (1 - significance) / 2\n\ncoverage(l, u) = cdf(post, u) - cdf(post, l)\n\nfunction classicCI(dist)\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) &lt; significance\n        l -= 0.00001\n        u += 0.00001\n    end\n    (l, u)\nend\nequalTailCI(dist) = (quantile(dist, halfAlpha), quantile(dist, 1 - halfAlpha))\nfunction highestDensityCI(dist)\n    height = 0.999 * maximum(pdf.(dist, xGrid))\n    l, u = mode(dist), mode(dist)\n    while coverage(l, u) &lt; significance\n        range = filter(theta -&gt; pdf(dist, theta) &gt; height, xGrid)\n        l, u = minimum(range), maximum(range)\n        height -= 0.00001\n    end\n    (l, u)\nend\n\nl1, u1 = classicCI(post)\nl2, u2 = equalTailCI(post)\nl3, u3 = highestDensityCI(post)\n\nprintln(\"Classical: \", (l1, u1), \"\\tWidth: \", u1 - l1, \"\\tCoverage: \", coverage(l1, u1))\nprintln(\"Equal tails: \", (l2, u2), \"\\tWidth: \", u2 - l2, \"\\tCoverage: \", coverage(l2, u2))\nprintln(\"Highest density: \", (l3, u3), \"\\tWidth: \", u3 - l3, \"\\tCoverage: \", coverage(l3, u3))\n\nfig, ax = lines(xGrid, pdf.(post, xGrid); color=:black, label=\"Gamma Posterior Distribution\")\nrangebars!(ax, [-0.00025], [l1], [u1]; direction=:x, whiskerwidth=10, color=:blue, label=\"Classic CI\")\nrangebars!(ax, [-0.0005], [l2], [u2]; direction=:x, whiskerwidth=10, color=:red, label=\"Equal Tail CI\")\nrangebars!(ax, [-0.00075], [l3], [u3]; direction=:x, whiskerwidth=10, color=:green, label=\"Highest Density CI\")\naxislegend(ax)\nfig\n\nClassical: (467.4288304558984, 828.5711695441016)   Width: 361.14233908820324   Coverage: 0.9000000065035687\nEqual tails: (496.70307973762834, 855.7332000231892)    Width: 359.0301202855608    Coverage: 0.9\nHighest density: (485.57452668179576, 843.1445266817958)    Width: 357.57000000000005   Coverage: 0.9006729463099745"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#hypothesis-testing",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#hypothesis-testing",
    "title": "Probability and statistics with Julia",
    "section": "10 Hypothesis testing",
    "text": "10 Hypothesis testing\nTo perform a hypothesis testing, first, we need to partition the parameter space \\(\\Theta\\) as two hypotheses:\n\nNull hypothesis: \\(H_0: \\theta \\in \\Theta_0\\)\nAlternative hypothesis: \\(H_1: \\theta \\in \\Theta_1\\)\n\nAnd then, we need to determine the test statistic used to perform the hypothesis testing. Once this is done, we can calculate the test statistic and then get the rejection region (e.g. \\((-\\infty, ICDF(\\frac{\\alpha}{2}))\\) and \\((ICDF(1-\\frac{\\alpha}{2}), +\\infty)\\) for a two-sided hypothesis test) or the p-value (\\(CDF(\\text{the value of test statistic})\\)) under certain confidence level \\(1-\\alpha\\) under \\(H_0\\) (the distribution of the test statistic is often known under \\(H_0\\) but it’s usually unknown under \\(H_1\\)).\nFinally, make a statement (rejecting or not rejecting \\(H_0\\)) under the assumption of null hypothesis based on some confidence level.\nSo there are several key concepts concerning a hypothesis testing: two alternative hypotheses, confidence level, test statistic, the distribution of the test statistic under the null hypothesis, rejection region or \\(p\\)-value.\n\n10.1 Single sample hypothesis tests for the mean\nAssume that the observations \\(X_1, ..., X_n\\) are normally distributed and we want to know whether this sample is from a population with \\(\\mu = \\mu_0\\) or not (\\(\\mu \\ne \\mu_0\\)).\nIn this case, we set up the hypothesis as two-sided (which means that \\(\\mu &lt; \\mu_0\\) and \\(\\mu &gt; \\mu_0\\) are both plausible) and the confidence level \\(1-\\alpha\\).\n\n10.1.1 Population variance known (Z-Test)\nAssume that \\(\\sigma\\) is known. Under \\(H_0\\), \\(\\overline{X}\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\frac{\\sigma^2}{n}\\). Hence, it holds that under \\(H_0\\) the Z-statistic\n\\[\nZ = \\frac{\\overline{X}-\\mu_0}{\\sigma/n}\n\\]\nfollows a standard normal distribution (\\(Z \\sim N(0, 1)\\)).\nIn this case, under the null hypothesis, \\(Z\\) is a standard normal random variable, and hence to carry out a hypothesis test we observe its position relative to a standard normal distribution. Specifically, we check if it lies within the rejection region or not, and if it does, we reject the null hypothesis, otherwise we don’t. In addition, we can also calculate the \\(p\\)-value (\\(p = 2P(Z &gt; |z|)\\)). If \\(p\\)-value is less than some pre-designated significance level \\(\\alpha\\), then we can reject \\(H_0\\) or we don’t.\n\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0 = 25\nmu1, sigma = 27, 2\nn = 100\nd = rand(Normal(mu1, sigma), n)\nxBar = mean(d)\n\ntestStatistic = (xBar - mu0) / (sigma / sqrt(n))\npVal = 2 * ccdf(Normal(), testStatistic)\nprintln(\"Manual hypothesis testing: \\nz-statistic: \", testStatistic, \"\\np-value: \", pVal, \"\\n\")\n\nOneSampleZTest(xBar, sigma, n, mu0)\n\nManual hypothesis testing: \nz-statistic: 10.322959784979435\np-value: 5.548583144318553e-25\n\n\n\nOne sample z-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         25\n    point estimate:          27.0646\n    95% confidence interval: (26.67, 27.46)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-24\n\nDetails:\n    number of observations:   100\n    z-statistic:              10.322959784979435\n    population standard error: 0.2\n\n\n\n\n10.1.2 Population variance unknown (T-Test)\nWhen the population variance is unknown, then \\(\\overline{X}\\) does not subject to \\(N(\\mu, \\frac{s}{\\sqrt{n}})\\). But we know that\n\\[\nT = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}} \\sim T(n-1)\n\\]\nunder the null hypothesis.\nThe observed test statistic from the data is then\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\n\\]\nand the corresponding \\(p\\)-value for a two-sided test is\n\\[\np = 2P(T_{n-1} &gt; |t|)\n\\]\nwhere \\(T_{n-1}\\) is a random variable distributed according to a T-distribution with \\(n-1\\) degrees of freedom.\n\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\n\nxBar, n, s = mean(d), length(d), std(d)\ntStatistic = (xBar - mu0) / (s / sqrt(n))\npVal = 2 * ccdf(TDist(n - 1), abs(tStatistic))\n\nprintln(\"Manually calculated t-statistic: \", tStatistic)\nprintln(\"Manually calculated p-value: \", pVal)\n\nOneSampleTTest(d, mu0)\n\nManually calculated t-statistic: 5.181941732141548\nManually calculated p-value: 5.303859808841965e-5\n\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         51\n    point estimate:          53.1999\n    95% confidence interval: (52.31, 54.09)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           &lt;1e-04\n\nDetails:\n    number of observations:   20\n    t-statistic:              5.181941732141548\n    degrees of freedom:       19\n    empirical standard error: 0.4245336299010766\n\n\n\n\n10.1.3 Non-parametric sign test\nThe validity of Z-Test or T-Test relies heavily on the assumption that the sample \\(X_1, ..., X_n\\) is comprised of independent normal random variables with variance known or unknown. This is because only under this assunmption does \\(\\overline{X} \\sim N(\\mu, \\frac{\\sigma^2}{n})\\) or \\(T \\sim T(n-1)\\).\nFor the non-parametric sign test, non-parametric implies that the distribution of the test statistic does not depend on any particular distributional assumption for the population.\nFor the sign test, we begin by denoting the random variables\n\\[\nX^+ = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i &gt; \\mu_0\\} \\quad\\text{and}\\quad X^- = \\sum_{i=1}^{n}\\mathbf{1}\\{X_i &lt; \\mu_0\\} = n - X^+\n\\]\nwhere \\(\\mathbf{1}\\) is the indicator function. The variable \\(X^+\\) is a count of the number of observations that exceed \\(\\mu_0\\), and similarly \\(X^-\\) is a count of the number of observations that are below \\(\\mu_0\\).\nObserve that under \\(H_0: \\mu = \\mu_0\\), it holds that \\(P(X_i &gt; \\mu_0) = P(X_i &lt; \\mu_0) = 1/2\\). Note that here we are actually taking \\(\\mu_0\\) as the median of the distribution and assuming that \\(P(X_i = \\mu_0) = 0\\) as is the case for a continuous distribution.\nHence, under \\(H_0\\), the random variables \\(X^+\\) and \\(X^-\\) both follow a binomial \\((n, 1/2)\\) distribution. Given the symmetry of this binomial distribution we define the test statistic to be\n\\[\nU = max\\{X^+, X^-\\}\n\\]\nHence, with observed data, and an observed test statistic \\(u\\), the \\(p\\)-value can be calculated via\n\\[\np = 2P(B &gt; u)\n\\]\nwhere \\(B \\sim B(n, 1/2)\\).\n\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu0, mu1, sigma = 51, 53, 2\ndist = Normal(mu1, sigma)\nd = rand(dist, 20)\nn = length(d)\n\nxPos = sum(d .&gt; mu0)\ntestStat = max(xPos, n - xPos)\n\nbinom = Binomial(n, 0.5)\npVal = 2 * ccdf(binom, testStat)\n\nprintln(\"mu0: \", mu0, \"\\nmu1: \", mu1)\nprintln(\"Binomial mean: \", mean(binom), \"\\nTest statistic: \", testStat)\nprintln(\"p-value: \", pVal)\n\nmu0: 51\nmu1: 53\nBinomial mean: 10.0\nTest statistic: 18\np-value: 4.005432128906249e-5\n\n\n\n\n10.1.4 Sign test vs. T-test\nWhen the normality assumption holds, the T-test is often more powerful than the sign test. That is, for a fixed \\(\\alpha\\), the probability of detecting \\(H_1\\) is higher for the T-test than for the sign test if \\(H_1 \\ne H_0\\). This makes it a more effective test to use, if the data can be assumed normally distributed.\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nActualMuRange = 51:0.02:55\nsigma = 1.2\nmu0 = 53\nn, N = 50, 10^4\npowerT, powerU = [], []\n\nfor ActualMu in ActualMuRange\n    dist = Normal(ActualMu, sigma)\n    rejectT, rejectU = 0, 0\n\n    for _ in 1:N\n        d = rand(dist, n)\n        xBar, stdDev = mean(d), std(d)\n\n        tStatistics = (xBar - mu0) / (stdDev / sqrt(n))\n        pValT = 2 * ccdf(TDist(n - 1), abs(tStatistics))\n\n        xPos = sum(d .&gt; mu0)\n        uStat = max(xPos, n - xPos)\n        pValSign = 2 * ccdf(Binomial(n, 0.5), uStat)\n\n        rejectT += pValT &lt; 0.05\n        rejectU += pValSign &lt; 0.05\n    end\n\n    push!(powerT, rejectT / N)\n    push!(powerU, rejectU / N)\nend\n\nfig, ax = lines(ActualMuRange, powerT; color=:blue, label=\"T test\")\nlines!(ax, ActualMuRange, powerU; color=:red, label=\"Sign test\")\nhlines!(ax, [0.05]; color=:green, label=\"Alpha\")\nvlines!(ax, [mu0 - sigma, mu0 + sigma]; color=:black, label=\"mu0 ± sigma\")\naxislegend(ax; position=:rb)\nax.xlabel = \"Different values of muActual\"\nax.ylabel = \"Proportion of times H0 rejected (power)\"\nfig\n\n\n\n\nFrom the result, under the normality assumption, we obsered that\n\nWhen \\(H_1 = H_0\\), then the power is \\(\\alpha\\) (i.e. the probability of rejecting \\(H_0\\)).\nT-test is more powerful than sign test when \\(H_1 \\ne H_0\\).\nWhen \\(H_1 \\to H_0\\), sign test will make higher \\(\\alpha\\) error than T-test.\nWhen \\(H_1\\) is away from \\(H_0\\) by one \\(\\sigma\\) or more, then the powers of T-test and sign test are really similar.\n\n\n\n\n10.2 Two sample hypothesis tests for comparing means\nWe now present some common hypothesis tests for the inference on the difference in means of two populations.\nCommonly, we wish to investigate if the population difference, \\(\\Delta_0\\), takes on a specific value.\nHence, we may wish to set up a two-sided hypothesis test as\n\\[\nH_0: \\mu_1 - \\mu_2 = \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 \\ne \\Delta_0\n\\]\nor one could formulate a one-sided hypothesis test, such as\n\\[\nH_0: \\mu_1 - \\mu_2 \\le \\Delta_0 \\quad\\text{and}\\quad H_1: \\mu_1 - \\mu_2 &gt; \\Delta_0\n\\]\nor the reverse if desired.\nIt is common to consider \\(\\Delta_0 = 0\\), in which case the hypothesis test can be stated as \\(H_0: \\mu_1 = \\mu_2\\), and \\(H_1: \\mu_1 \\ne \\mu_2\\).\nFor the tests introduced below, we assume that the observations \\(X_1^{(1)}, ..., X_n^{(1)}\\) from population \\(1\\) and \\(X_1^{(2)}, ..., X_n^{(2)}\\) from population \\(2\\) are all normally distributed, where \\(X_i^{(j)}\\) has mean \\(\\mu_j\\) and variance \\(\\sigma_j^2\\).\nThe testing methodology then differs based on the following three cases:\n\nThe population variances \\(\\sigma_1\\) and \\(\\sigma_2\\) are known.\nThe population variances \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown but assumed equal.\nThe population variances \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown and not assumed equal.\n\nIn each of these cases, the test statistic is given by\n\\[\n\\frac{(\\overline{X}_1 - \\overline{X}_2) - \\Delta_0}{S_{err}}\n\\]\nwhere \\(\\overline{X}_j\\) is the sample mean of \\(X_1^{(j)}, ..., x_n^{(j)}\\), and the standard error \\(S_{err}\\) varies according to the case (\\(1-3\\)).\n\n10.2.1 Population variances known\nWhen the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) are known, we have \\(\\overline{X}_1 \\sim N(\\mu_1, \\frac{\\sigma_1^2}{n_1})\\) and \\(\\overline{X}_2 \\sim N(\\mu_2, \\frac{\\sigma_2^2}{n_2})\\), and then \\(\\overline{X}_1 - \\overline{X}_2 \\sim N(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2})\\). Hence we set\n\\[\nS_{err} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\\]\nIn this case, the test statistic follows a standard normal distribution under \\(H_0\\), so we have\n\\[\nz = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\n\nusing Random, Distributions\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, n1 = mean(d1), length(d1)\nxBar2, n2 = mean(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(sigma1^2 / n1 + sigma2^2 / n2)\npVal = 2 * ccdf(Normal(), abs(testStat))\n\nprintln(\"μ1 = \", mu1, \"\\nμ2 = \", mu2)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n\nμ1 = 10\nμ2 = 12\nManually calculated test statistic: -3.1025769505420775\nManually calculated p-value: 0.001918436654992464\n\n\n\n\n10.2.2 Population variances unknown and assumed equal\nIn case the population variances are unknown and assumed equal (\\(\\sigma^2 := \\sigma_1^2 = \\sigma_2^2\\)), the pooled sample variance (weighted arithmetic mean), \\(S_p^2\\) is used to estimate \\(\\sigma^2\\) based on both samples. It is given by\n\\[\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n\\]\nwhere \\(S_j^2\\) is the sample variance of sample \\(j\\). It can be shown that under \\(H_0\\), if we set\n\\[\nS_{err} = \\sqrt{\\frac{S_p^2}{n_1} + \\frac{S_p^2}{n_2}} = S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\nthe test statistic is distributed as a T-distribution with \\(n_1+n_2-2\\) degrees of freedom.\nFor this case, the observed test statistic is\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\n\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 2\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nsP = sqrt(((n1 - 1) * s1^2 + (n2 - 1) * s2^2) / (n1 + n2 - 2))\ntestStat = (xBar1 - xBar2 - delta0) / (sP * sqrt(1 / n1 + 1 / n2))\npVal = 2 * ccdf(TDist(n1 + n2 - 2), abs(testStat))\n\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\n\nEqualVarianceTTest(d1, d2, delta0)\n\nManually calculated test statistic: -2.870462082198691\nManually calculated p-value: 0.006080203814276703\n\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.56931\n    95% confidence interval: (-2.669, -0.4701)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0061\n\nDetails:\n    number of observations:   [20,30]\n    t-statistic:              -2.870462082198691\n    degrees of freedom:       48\n    empirical standard error: 0.546709753154726\n\n\n\n\n10.2.3 Population variances unknown and not assumed equal\nWhere the population variances are unknown and not assumed equal (\\(\\sigma_1^2 \\ne \\sigma_2^2\\)), we set\n\\[\nS_{err} = \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\\]\nThen the observed test statistic is given by\n\\[\nt = \\frac{(\\bar{x}_1 - \\bar{x}_2) - \\Delta_0}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\\]\nThe distribution of the test statistic does not follow an exact T-distribution with \\(n_1+n_2-2\\) degrees of freedom. Instead, we use the Satterthwaite approxmation, and determine the degrees of freedom via\n\\[\nv = \\frac{(s_1^2n_1 + s_2^2n_2)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n2-1}}\n\\]\n\nusing Random, Distributions, HypothesisTests\n\nRandom.seed!(0)\n\nmu1, sigma1 = 10, 2\nmu2, sigma2 = 12, 1.5\n\nd1 = rand(Normal(mu1, sigma1), 20)\nd2 = rand(Normal(mu2, sigma2), 30)\n\nxBar1, s1, n1 = mean(d1), std(d1), length(d1)\nxBar2, s2, n2 = mean(d2), std(d2), length(d2)\n\n# to test whether μ1 = μ2 (two-sided)\ndelta0 = 0\n\nv = (s1^2 / n1 + s2^2 / n2)^2 / ((s1^2 / n1)^2 / (n1 - 1) + (s2^2 / n2)^2 / (n2 - 1))\ntestStat = (xBar1 - xBar2 - delta0) / sqrt(s1^2 / n1 + s2^2 / n2)\npVal = 2 * ccdf(TDist(v), abs(testStat))\n\nprintln(\"Manually calculated degrees of freedom, v: \", v)\nprintln(\"Manually calculated test statistic: \", testStat)\nprintln(\"Manually calculated p-value: \", pVal)\nUnequalVarianceTTest(d1, d2, delta0)\n\nManually calculated degrees of freedom, v: 32.78958178466954\nManually calculated test statistic: -3.2719914674400004\nManually calculated p-value: 0.002517632534475486\n\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          -1.62701\n    95% confidence interval: (-2.639, -0.6151)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0025\n\nDetails:\n    number of observations:   [20,30]\n    t-statistic:              -3.2719914674400004\n    degrees of freedom:       32.78958178466954\n    empirical standard error: 0.49725223770269017\n\n\n\n\n\n10.3 Analysis of variance (ANOVA or F-test)\nAs mentioned above, Z-test or T-test can handle the problems of comparing means of two populations. However, what if there are more than two populations that need to be compared? You may say that we can compare each pair of them, but in fact, this will reduce the hypothesis test power.\nWhen we have more than two populatons to be compared, we call each population a treatment or a group. It is of interest to see if vairous “treatments” have an effect on some mean value or not.\nMore formally, assume that there is some overall mean \\(\\mu\\) and there are \\(L \\ge 2\\) treatments, where each treatment may potentially alter the mean by \\(\\tau_i\\). In this case, the mean of the population under treatment \\(i\\) can be represented by \\(\\mu_i = \\mu + \\tau_i\\), with \\(\\mu\\) an overall mean, and\n\\[\n\\sum_{i=1}^{L} \\tau_i = 0\n\\]\nThis condition on the parameters \\(\\tau_1, ..., \\tau_L\\) ensures that given \\(\\mu_1, ..., \\mu_L\\), the overall mean \\(\\mu\\) and \\(\\tau_1, ..., \\tau_L\\) are well defined.\nGiven \\(\\mu_1, ..., \\mu_L\\), we have\n\\[\n\\mu = \\frac{1}{L} \\sum_{i=1}^{L} \\mu_i \\quad\\text{and}\\quad \\tau_i = \\mu_i - \\mu\n\\]\nThe question is then: Do the treatments have any effect or not?\nSuch a question is presented via the hypothesis formulation:\n\\[\nH_0: \\tau_1 = \\tau_2 = \\cdots = \\tau_L = 0 \\quad\\text{vs.}\\quad H_1: \\exists i\\ |\\ \\tau_i \\ne 0\n\\]\nNote that \\(H_0\\) is equivalent to the statement that \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_L\\), indicating that the treatments do not have an effect. Furthermore, \\(H_1\\) states that there exists an \\(i\\) with \\(\\tau_i \\ne 0\\) is equivalent to the case where there exist at least two treatments, \\(i\\) and \\(j\\) such that \\(\\mu_i \\ne \\mu_j\\).\nAssume that we collect observations as follows:\n\\[\n\\begin{align}\n\\text{Treatment 1: } &x_{11}, x_{12}, ..., x_{1n_1} \\\\\n\\text{Treatment 2: } &x_{21}, x_{22}, ..., x_{2n_2} \\\\\n&\\vdots \\\\\n\\text{Treatment L: } &x_{L1}, x_{L2}, ..., x_{Ln_L}\n\\end{align}\n\\]\nwhere \\(n_1, n_2, ..., n_L\\) are the sample sizes for each treatment.\nThen denote the total number of observations via\n\\[\nm = \\sum_{i=1}^{L} n_i\n\\]\nWe also consider the sample means for each treatment\n\\[\n\\bar{x}_i = \\frac{1}{n_i} \\sum_{i=1}^{n_i} x_{ij}\n\\]\nand the overall sample mean\n\\[\n\\bar{x} = \\frac{1}{m} \\sum_{i=1}^{L} \\sum_{i=1}^{n_i} x_{ij} = \\sum_{i=1}^{L} \\frac{n_i}{m} x_{ij}\n\\]\nIn ANOVA, the statistical model assumes that the observations of each treatment come from an underlying model of the following form:\n\\[\nX_i = \\mu_i + \\epsilon = \\mu + \\tau_i + \\epsilon \\quad\\text{where}\\quad \\epsilon \\sim N(0, \\sigma^2)\n\\]\nwhere \\(X_i\\) is the model for the \\(i\\)th treatment, and \\(\\epsilon\\) is some noise term with common unknown variance across all treatment groups, independent across measurements.\n\n10.3.1 Decomposing sum of squares\nA key idea of ANOVA is the decomposition of the total variability into two components: the variablity between the treatments, and the variability within the treatments.\nThe total sum of squares (\\(SS_\\text{Total}\\)) is a measure of the total variability of all observations, and is calculated as follows:\n\\[\nSS_\\text{Total} = \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2\n\\]\nwhere \\(\\bar{x}\\) is the overall mean.\nNow we decompose \\(SS_\\text{Total}\\) into two parts:\n\\[\n\\begin{align}\n\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x})^2 &= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i + \\bar{x}_i - \\bar{x})^2 \\\\\n&= \\sum_{i=1}^{L} \\sum_{j=1}^{n_i} \\left((x_{ij} - \\bar{x}_i)^2 + 2(x_{ij} - \\bar{x}_i)(\\bar{x}_i - \\bar{x}) + (\\bar{x}_i - \\bar{x})^2\\right) \\\\\n&= \\underbrace{\\sum_{i=1}^{L} \\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2}_{SS_\\text{Error}} + \\underbrace{\\sum_{i=1}^{L} n_i (\\bar{x}_i - \\bar{x})^2}_{SS_\\text{Treatment}}\n\\end{align}\n\\]\nNote \\(\\sum_{i=1}^{n_i} (x_{ij} - \\bar{x}_i) = 0\\). So we have\n\\[\nSS_\\text{Total} = SS_\\text{Error} + SS_\\text{Treatment}\n\\]\nNote that \\(SS_\\text{Error}\\) is also known as the sum of variability within the groups, and that \\(SS_\\text{Treatment}\\) is also known as the variability between the groups.\nThe decomposition holds under both \\(H_0\\) and \\(H_1\\), and hence allows us to construct a test statistic. Intuitively, under \\(H_0\\), both \\(SS_\\text{Error}\\) and \\(SS_\\text{Treatment}\\) should contribute to \\(SS_\\text{Total}\\) in the same manner (once properly normalized). Alternatively, under \\(H_1\\), it is expected that \\(SS_\\text{Treatment}\\) would contribute more heavily to the total variability.\nThe test statistic we constructed is called F-statistic, defined as the ratio of \\(SS_\\text{Treatment}\\) and \\(SS_\\text{Error}\\) normalized by their respective degrees of freedom \\(L-1\\) and \\(m-L\\):\n\\[\nF = \\frac{SS_\\text{Treatment}/(L-1)}{SS_\\text{Error}/(m-L)}\n\\]\nThese normalized quantities are, respectively, denoted by \\(MS_\\text{Treatment}\\) and \\(MS_\\text{Error}\\) standing for “Mean Squared”. Hence, \\(F = \\frac{MS_\\text{Treatment}}{MS_\\text{Error}}\\)\nUnder \\(H_0\\), the F-statistic follows an F-distribution with \\(L-1\\) degrees of freedom for the numerator and \\(m-L\\) degrees of freedom for the denominator. Intuitively, under \\(H_0\\), we expect the numerator and denominator to have similar values, hence expect \\(F\\) to be around \\(1\\) (indeed most of the mass of \\(F\\) distributions is concentrated around \\(1\\)). However, if \\(MS_\\text{Treatment}\\) is significantly larger, then it indicates that \\(H_0\\) may not hold. Hence, the approach of the F-test is to reject \\(H_0\\) if the F-statistic is geater than the \\(1-\\alpha\\) quantile of the respective F-distribution. Similarly, the \\(p\\)-value for an observed F-statistic \\(f_o\\) is given by\n\\[\np = P(F_{L-1, m-L} &gt; f_o)\n\\]\nwhere \\(F_{L-1, m-L}\\) is an F-distributed random variable with \\(L-1\\) numerator degrees of freedom and \\(m-L\\) denominator degrees of freedom.\n\nusing Random, Distributions, DataFrames, GLM, CategoricalArrays\n\nRandom.seed!(0)\n\nallData = [rand(Normal(10, 1), 20), rand(Normal(11, 2), 30), rand(Normal(11, 2), 25)]\n\n# manual ANOVA\n# the decomposition of the sum of squares\n# F-test\nnArray = length.(allData)\nd = length(nArray)\n\nxBarTotal = mean(vcat(allData...))\nxBarArray = mean.(allData)\n\nssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:d])\nssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:d])\n\ndfBetween = d - 1\ndfError = sum(nArray) - d\n\nmsBetween = ssBetween / dfBetween\nmsError = ssWithin / dfError\n\nfStat = msBetween / msError\npVal = ccdf(FDist(dfBetween, dfError), fStat)\n\nprintln(\"Maunal ANOVA: \\nF-Statistic: \", fStat, \"\\np-value: \", pVal)\n\n# ANOVA using GLM package which requires the DataFrames package\nnArray = length.(allData)\nd = length(nArray)\n\ntreatment = vcat([fill(k, nArray[k]) for k in 1:d]...)\nresponse = vcat(allData...)\ndf = DataFrame(Response=response, Treatment=categorical(treatment))\nmodelH0 = lm(@formula(Response ~ 1), df)\nmodelH1 = lm(@formula(Response ~ 1 + Treatment), df)\nres = ftest(modelH1.model, modelH0.model)\n\nprintln(\"GLM ANOVA: \\nF-Statistic: \", res.fstat[2], \"\\np-value: \", res.pval[2])\n\nMaunal ANOVA: \nF-Statistic: 3.3657643096267207\np-value: 0.040051359435068816\nGLM ANOVA: \nF-Statistic: 3.3657643096267127\np-value: 0.040051359435069135\n\n\n\n\n10.3.2 More on the distribution of the F-Statistic\nHere we use the Monte Carlo simulation to generate the distribution of the F-Statistic for two different cases (\\(H_0\\) and \\(H_1\\)).\nUnder \\(H_0\\), the distribution of F-Statistic obtained via Monte Carlo should be exactly the same as the analytical F-distribution with the same numerator and denominator degrees of freedom, but it’s not for the distribution of F-Statistic under \\(H_1\\).\n\nusing Random, Distributions, CairoMakie\n\nRandom.seed!(0)\n\nfunction anovaFStat(allData)\n    xBarArray = mean.(allData)\n    nArray = length.(allData)\n    xBarTotal = mean(vcat(allData...))\n    Le = length(nArray)\n\n    ssBetween = sum([nArray[i] * (xBarArray[i] - xBarTotal)^2 for i in 1:Le])\n    ssWithin = sum([sum([(ob - xBarArray[i])^2 for ob in allData[i]]) for i in 1:Le])\n\n    return ((ssBetween / (Le - 1)) / (ssWithin / (sum(nArray) - Le)))\nend\n\ncase1 = [13.4, 13.4, 13.4, 13.4, 13.4]\ncase2 = [12.7, 11.8, 13.4, 12.7, 12.9]\nstdDevs = [2, 2, 2, 2, 2]\nnumObs = [24, 15, 13, 23, 9]\nLe = length(case1)\n\nN = 10^5\n\nmcFstatsH0 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH0[i] = anovaFStat([rand(Normal(case1[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nmcFstatsH1 = Array{Float64}(undef, N)\nfor i in 1:N\n    mcFstatsH1[i] = anovaFStat([rand(Normal(case2[j], stdDevs[j]), numObs[j]) for j in 1:Le])\nend\n\nfig, ax = stephist(mcFstatsH0; bins=100, color=:blue, normalization=:pdf, label=\"H0\")\nstephist!(ax, mcFstatsH1; bins=100, color=:red, normalization=:pdf, label=\"H1\")\n\ndfBetween = Le - 1\ndfError = sum(numObs) - Le\nxGrid = 0:0.01:10\n\nlines!(ax, xGrid, pdf.(FDist(dfBetween, dfError), xGrid); color=:green, label=\"F-Statistic analytic\")\n\ncritVal = quantile(FDist(dfBetween, dfError), 0.95)\n\nvlines!(ax, [critVal]; color=:black, linestyle=:dash, label=\"Critical value boundary\")\n\naxislegend(ax)\nax.xlabel = \"F-value\"\nax.ylabel = \"Density\"\nfig\n\n\n\n\nAnalysis presented here is just the one-way ANOVA, which means that we have only one treatment category having different treatment levels. Often we may have two treatment categories, each of which has different treatment levels (two-way ANOVA). This can be extended to higher dimensional extensions, which are often considered in block factorial design. In addition, once we reject \\(H_0\\), we often want to known which specific treatments have an effect and in which way. This involves multiple comparisons.\n\n\n\n10.4 Independence and goodness of fit\nIn fact, checking for independence is a special type of checking goodness of fit. One question often posed is: Does the population follow a specific distributional form?\nFor checking goodness of fit, it means that we want to know whether a random variable \\(X\\) is distributed as some hypothesized distribution \\(F_0\\) or not (meaning that \\(X\\) may be distributed as any of other distributions). We can set up the hypothesis test as\n\\[\nH_0: X \\sim F_0 \\quad \\text{vs.} \\quad H_1: otherwise\n\\]\nThe hypothesis formulation then partitions this space into \\(\\{F_0\\}\\) for \\(H_0\\) and all other distributions in \\(H_1\\).\nFor checking independence, our data must have at least two dismentions, saying \\(Z = (X, Y)\\) (i.e. \\(Z\\) is a vector of two random variables). According to the independence of random variables, we know that \\(X\\) and \\(Y\\) are said to be independent if \\(f(z) = f(x, y) = f_X(x)f_Y(y)\\), where \\(f(x, y)\\) is the joint PDF of \\(X\\) and \\(Y\\), \\(f_X(x)\\) is the marginal PDF of \\(X\\), and \\(f_Y(y)\\) is the marginal PDF of \\(Y\\). Under \\(H_0\\), we know the theoretical marginal distributions of \\(X\\) and \\(Y\\), and then we can conclude their joint PDF via \\(f(x, y) = f_X(x)f_Y(y)\\). This means that we again back to the goodness of fit (i.e. checking whether the random variable \\(Z\\) has the PDF \\(f(x, y) = f_X(x)f_Y(y)\\)). In this case, we can set the hypothesis test as\n\\[\nH_0: X_1 \\text{ independent of } X_2 \\quad \\text{vs.} \\quad H_1: X_1 \\text{ not independent of } X_2\n\\]\nThis sets the space of \\(H_0\\) as the space of all distributions of independent random variable pairs, and \\(H_1\\) as the complement.\nTo test the two hypotheses, we have two different hypothesis test methods:\n\nchi-squared test: used for goodness of fit of discrete distributions with finite categories and for checking indepdendence.\nKolmogorov-Smirnov test: used for goodness of fit for arbitrary distributions.\n\nIn the chi-squared case, the approach involves looking at counts of observations that match disjoint categories \\(i = 1, ..., M\\). For each category \\(i\\), we denote \\(O_i\\) the number of the observations that match category \\(i\\). In addition, for each category, there is also an expected number of observations under \\(H_0\\), which we denote as \\(E_i\\). With these, one can express the test statistic as\n\\[\n\\chi^2 = \\sum_{i=1}^{M} \\frac{(O_i-E_i)^2}{E_i}\n\\]\nUnder \\(H_0\\), we expect that for each category \\(i\\), both \\(O_i\\) and \\(E_i\\) will be relatively close, and hence it is expected that the sum of relative squared differences, \\(\\chi^2\\), will not be too big. Conversely, a large value of \\(\\chi^2\\) may indicate that \\(H_0\\) is not plausible. Later, we’ll use the test statistic \\(\\chi^2\\) to test for both goodness of fit and independence since checking independence is a special type of checking goodness of fit.\nIn the case of Kolmogorov-Smirnov, a key concept is the Empirical Cumulative Distribution Function (ECDF). For a sample of observations, \\(x_1, ..., x_n\\), the ECDF is\n\\[\n\\hat{F}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{x_i \\le x\\}, \\quad \\text{where } \\mathbf{1}\\{\\cdot\\} \\text{ is the indicator function}\n\\]\nThe approach of Kolmogorov-Smirnov test is to check the closeness of the ECDF to the CDF hypothesized under \\(H_0\\). This is done via the Kolmogorov-Smirnov statistic\n\\[\n\\tilde{S} = \\sup_x |\\hat{F}(x) - F_0(x)|\n\\]\nwhere \\(F_0(\\cdot)\\) is the CDF under \\(H_0\\) and \\(\\sup\\) is the supremum over all possible \\(x\\)-values. Similar to the case of chi-squared, under \\(H_0\\) it is expected that \\(\\hat{F}(\\cdot)\\) does not deviate greatly from \\(F_0(\\cdot)\\), and hence it is expected that \\(\\tilde{S}\\) is not very large.\nThe key to both the chi-squared and Kolmogorov-Smirnov tests is that under \\(H_0\\) there are tractable known approximations to the distribution of the test statistics of both \\(\\chi^2\\) and \\(\\tilde{S}\\). Then these approximations allow us to obtain an approximate \\(p\\)-value in the standard way via\n\\[\np = P(W &gt; u)\n\\]\nwhere \\(W\\) denotes a random variable distributed based on the approximate distribution and \\(u\\) is the observed test statistic.\n\n10.4.1 Chi-squared test for goodness of fit\nAssume that the distribution \\(F_0\\) under \\(H_0\\) can be partitioned into categories \\(i = 1, ..., M\\). With such a partition, having \\(n\\) sample observations, we denote by \\(E_i\\) the expected number of observations satisfying category \\(i\\). These values are theoretically computed. Then, based on observations \\(x_1, ..., x_n\\), we denote by \\(O_i\\) as the number of observations that satisfy category \\(i\\). Note that\n\\[\n\\sum_{i=1}^{M} E_i = n \\quad \\text{and} \\quad \\sum_{i=1}^{M} O_i = n\n\\]\nNow, based on \\(\\{E_i\\}\\) and \\(\\{O_i\\}\\), we can calculate the \\(\\chi^2\\) test statistic.\nIt turns out that under \\(H_0\\), the \\(chi^2\\) test statistic approximately follows a chi-squared distribution with \\(M-1\\) degrees of freedom.\n\nusing Distributions, HypothesisTests\n\n# assume under H0 that a die is biased, with the probabilities for each side (1 to 6) given by the following vector p\np = [0.08, 0.12, 0.2, 0.2, 0.15, 0.25]\n# imagine that the die is rolled n = 60 times, and the following count of outcomes (1 to 6) is observed\nO = [3, 2, 9, 11, 8, 27]\n# the number of categories\nM = length(O)\n# the number of observations\nn = sum(O)\n# the expected count of outcomes (1 to 6) under H_0\nE = n .* p\n\ntestStatistic = sum((O .- E) .^ 2 ./ E)\npVal = ccdf(Chisq(M - 1), testStatistic)\n\nprintln(\"Manually calculated test statistic: \", testStatistic)\nprintln(\"Manually calculated p-value: \", pVal)\n\nprintln(ChisqTest(O, p))\n\nManually calculated test statistic: 14.974999999999998\nManually calculated p-value: 0.010469694843220361\nPearson's Chi-square Test\n-------------------------\nPopulation details:\n    parameter of interest:   Multinomial Probabilities\n    value under h_0:         [0.08, 0.12, 0.2, 0.2, 0.15, 0.25]\n    point estimate:          [0.05, 0.0333333, 0.15, 0.183333, 0.133333, 0.45]\n    95% confidence interval: [(0.0, 0.1828), (0.0, 0.1662), (0.03333, 0.2828), (0.06667, 0.3162), (0.01667, 0.2662), (0.3333, 0.5828)]\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    one-sided p-value:           0.0105\n\nDetails:\n    Sample size:        60\n    statistic:          14.975000000000001\n    degrees of freedom: 5\n    residuals:          [-0.821584, -1.93793, -0.866025, -0.288675, -0.333333, 3.09839]\n    std. residuals:     [-0.85656, -2.06584, -0.968246, -0.322749, -0.361551, 3.57771]\n\n\n\n\n\n10.4.2 Chi-squared test for checking independence\nConsidering the following contingency table:\n\n\n\n\n\nNow, under \\(H_0\\), we assume that the smoking or non-smoking behavior of the individual is independent of the gender (male or female). To check for this using a chi-squared statistic, we first set up \\(\\{E_i\\}\\) and \\(\\{O_i\\}\\) as in the following table:\n\n\n\n\n\nHere the observed marginal distribution over male versus female is based on the proportions \\(p = (0.402, 0.598)\\) and the distribution over smoking versus non-smoking is based on the proportions \\(q = (0.169, 0.831)\\). Then, since independence is assumed under \\(H_0\\), we multiply the marginal probabilities to obtain the expected observation counts\n\\[\nE_{ij} = np_iq_j\n\\]\nNow with these values at hand, the chi-squared test statistic can be set as follows:\n\\[\n\\chi^2 = \\sum_{i=1}^{m}\\sum_{i=1}^{l} \\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\n\\]\nwhere \\(m\\) and \\(l\\) are the respective dimensions of the contingency table (\\(m=l=2\\) in this case).\nIt turns out that under \\(H_0\\), the above test statistic is approximately chi-squred distributed with \\((m-1)\\times(l-1)\\) degrees of freedom.\n\nusing Distributions\n\nxObs = [18 132; 45 178]\nrowSums = [sum(xObs[i, :]) for i in 1:2]\ncolSums = [sum(xObs[:, j]) for j in 1:2]\nn = sum(xObs)\n\nrowProps = rowSums ./ n\ncolProps = colSums ./ n\n\nxExpect = [n * rowProps[i] * colProps[j] for i in 1:2, j in 1:2]\ntestStat = sum([(xObs[i, j] - xExpect[i, j])^2 / xExpect[i, j] for i in 1:2, j in 1:2])\npVal = ccdf(Chisq(1), testStat)\n\nprintln(\"Chi-squared value: \", testStat)\nprintln(\"p-value: \", pVal)\n\nChi-squared value: 4.274080056208799\np-value: 0.038697906065363365\n\n\n\n\n10.4.3 Kolmogorov-Smirnov test\nWe now consider the Kolmogorov-Smirnov test, which is based on the test statistic \\(\\tilde{S}\\).\nThe approach is based on the fact that under \\(H_0\\), the ECDF \\(\\hat{F}(\\cdot)\\) is close to the actual CDF \\(F_0(\\cdot)\\). Let us dive into more details about why the ECDF converges to the actual CDF as \\(n \\to \\infty\\):\nFor every value \\(x \\in R\\), the ECDF at that value, \\(\\hat{F}(x)\\), is the proportion of the number of observations less than or equal to \\(x\\). Under \\(H_0\\), multiplying the ECDF by \\(n\\) yields a binomial random variable with success probability \\(F_0(x)\\):\n\\[\nn\\hat{F}(x) \\sim Binomial(n, F_0(x))\n\\]\nHence,\n\\[\nE[\\hat{F}(x)] = F_0(x), \\quad Var(\\hat{F}(x)) = \\frac{F_0(x)(1-F_0(x))}{n}\n\\]\nHence, for non-small \\(n\\), the ECDF and CDF should be close since the variance for every value \\(x\\) is of the order \\(1/n\\) and diminishes as \\(n\\) grows.\nThe formal statement of this, taking \\(n \\to \\infty\\) and considering all values of \\(x\\) simutaneously, is known as the Glivenko Cantelli Theorem.\nFor finite \\(n\\), the ECDF will not exactly align with the CDF. However, the Kolmogorov-Smirnov test statistic \\(\\tilde{S}\\) is useful when it comes to measuring this deviation. This is due to the fact that under \\(H_0\\) the stochastic process in the variable \\(x\\)\n\\[\n\\sqrt{n}(\\hat{F}(x) - F_0(x))\n\\]\nis approximately identical in probability law to a standard Brownian Bridge, \\(B(\\cdot)\\), composed with \\(F_0(x)\\). That is, by denoting \\(\\hat{F}(\\cdot)\\) as the ECDF with \\(n\\) observations, we have that\n\\[\n\\sqrt{n}(\\hat{F}(x) - F_0(x)) \\stackrel{d}{\\approx} B(F_0(x))\n\\]\nwhich asymptotically converges to equality in distribution as \\(n \\to \\infty\\).\nNote that a Brownian Bridge, \\(B(t)\\), is a form of a variant of Brownian motion, constrained to equal \\(0\\) both at \\(t = 0\\) and \\(t = 1\\). It is a type of diffusion process.\nNow consider the supremum as in the Kolmogorov-Smirnov test statistic \\(\\tilde{S}\\). It can be shown that in case where \\(F_0(\\cdot)\\) is a continuous function (distribtuion), as \\(n \\to \\infty\\),\n\\[\n\\sqrt{n}\\tilde{S} \\stackrel{d}{=} \\sup_{t \\in [0,1]} |B(t)|\n\\]\nImportantly, notice that the right-hand side does not depend on \\(F_0(\\cdot)\\), but rather is the maximal value attained by the absolute value of the Brownian bridge process over the interval \\([0,1]\\). It then turns out that such a random variable denoted by \\(K\\), has CDF\n\\[\nF_K(x) = P\\left(\\sup_{t \\in [0,1]} |B(t)| \\le x\\right) = 1 - 2 \\sum_{k=1}^{\\infty} (-1)^{k-1} e^{-2k^2x^2} = \\frac{\\sqrt{2\\pi}}{x} \\sum_{k=1}^{\\infty} e^{-(2k-1)^2\\pi^2/(8x^2)}\n\\]\nThis is sometimes called the Kolmogorov distribution.\nIn brief, given \\(n\\) observations, under \\(H_0\\), the \\(\\sqrt{n}\\)-scaled supremum difference between ECDF and CDF \\(\\tilde{S} = \\sup_x |\\hat{F}(x) - F_0(x)|\\) is distributed as the Kolmogonov distribution, independent of \\(F_0(\\cdot)\\).\nThus, to obtain an approximate \\(p\\)-value for the Kolmogonov-Smirnov test, we calculate\n\\[\np = 1 - F_k(\\sqrt{n}\\tilde{S})\n\\]\nWe can make a simple validation to see that the distribution of \\(\\sqrt{n}\\tilde{S}\\) is independent of the actual CDF \\(F_0(\\cdot)\\) under \\(H_0\\):\n\nusing Distributions, StatsBase, Random, CairoMakie\n\nRandom.seed!(0)\n\nn = 25\nN = 10^4\nxGrid = -10:0.001:10\nkGrid = 0:0.01:5\ndist1, dist2 = Exponential(1), Normal()\n\nfunction ksStat(dist)\n    d = rand(dist, n)\n    Fhat = ecdf(d)\n    sqrt(n) * maximum(abs.(Fhat.(xGrid) .- cdf.(dist, xGrid)))\nend\n\nkStats1 = [ksStat(dist1) for _ in 1:N]\nkStats2 = [ksStat(dist2) for _ in 1:N]\n\nfig, ax = lines(kGrid, pdf.(Kolmogorov(), kGrid); color=:red, label=\"Kolmogorov PDF\")\nstephist!(ax, kStats1; bins=50, color=:blue, label=\"KS stat (Exponential)\", normalization=:pdf)\nstephist!(ax, kStats2; bins=50, color=:green, label=\"KS stat (Normal)\", normalization=:pdf)\naxislegend(ax)\nax.xlabel = \"K\"\nax.ylabel = \"Density\"\nfig\n\n\n\n\nIn addition, we show how to perform the Kolmogorov-Smirnov test:\n\nusing Random, Distributions, StatsBase, HypothesisTests, CairoMakie\n\nRandom.seed!(3)\n\n# the data is actually distributed as a gamma distribution with shape parameter 2 and mean 5\ndist = Gamma(2, 2.5)\n# assume F0 is an exponential distribution with mean 5 under H0\ndistH0 = Exponential(5)\n# the number of observations\nn = 100\n# sample n observations from the above gamma distribution\nd = rand(dist, n)\n\nFhat = ecdf(d)\ndiffF(dist, x) = sqrt(n) * (Fhat(x) - cdf(dist, x))\nxGrid = 0:0.001:30\nKSstat = maximum(abs.(diffF.(distH0, xGrid)))\n\nM = 10^5\nKScdf(x) = sqrt(2 * pi) / x * sum([exp(-(2k - 1)^2 * pi^2 / (8 * x^2)) for k in 1:M])\n\nprintln(\"Manually calculated KS-statistic: \", KSstat)\nprintln(\"p-value calculated via series: \", 1 - KScdf(KSstat))\nprintln(\"p-value calculated via Kolmogorov distribution: \", 1 - cdf(Kolmogorov(), KSstat))\n\nprintln(ApproximateOneSampleKSTest(d, distH0))\n\n# ECDF, actual and postulated CDFs\nfig = Figure(size=(600, 1000))\nax1 = Axis(fig[1, 1]; xlabel=\"x\", ylabel=\"Probability\")\nlines!(ax1, xGrid, cdf.(dist, xGrid); color=:green, label=\"CDF under actual distribution\")\nlines!(ax1, xGrid, cdf.(distH0, xGrid); color=:red, label=\"CDF under postulated H0\")\nlines!(ax1, xGrid, Fhat.(xGrid); color=:blue, label=\"ECDF from data\")\naxislegend(ax1; position=:rb)\n\nax2 = Axis(fig[2, 1]; xlabel=\"t\", ylabel=\"K-S process\")\nlines!(ax2, cdf.(dist, xGrid), diffF.(dist, xGrid); color=:blue, label=\"KS Process under actual distribution\")\nlines!(cdf.(distH0, xGrid), diffF.(distH0, xGrid); color=:red, label=\"KS Process under postualted H0\")\naxislegend(ax2; position=:rb)\nfig\n\nManually calculated KS-statistic: 1.7254271759200177\np-value calculated via series: 0.005189848797180319\np-value calculated via Kolmogorov distribution: 0.00518984879718043\nApproximate one sample Kolmogorov-Smirnov test\n----------------------------------------------\nPopulation details:\n    parameter of interest:   Supremum of CDF differences\n    value under h_0:         0.0\n    point estimate:          0.172608\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0052\n\nDetails:\n    number of observations:   100\n    KS-statistic:             1.7260771840779952\n\n\n\n\n\n\n\n\n10.4.4 Testing for an indenpendent sequence\nHere, we’ll use the Wald-Wolfowitz runs test to perform hypothesis testing for checking if a sequence of random variables is i.i.d. (indenpendent and identically distributed).\nConsider a sequence of data points \\(x_1, ..., x_n\\) with sample mean \\(\\bar{x}\\). For simplicity, assume that no point equals the sample mean. Now transform the sequence to \\(y_1, ..., y_n\\) via \\(y_i = x_i - \\bar{x}\\).\nWe now consider the signs of \\(y_i\\). For example, in a dataset with \\(20\\) observations, once considering the signs we may have a sequence such as\n\\[\n+-+-----++--+---++++\n\\]\nindicating that the first is positive (greater than the mean), the second is negative (less than the mean), and so on.\nNote that we assume no exact \\(0\\) for \\(y_i\\) and if such exist we can arbitrarily assign them to be either positive or negative. We then create a random variable \\(R\\) by counting the number of runs in this sequence, where a run is a consecutive sequence of points having the same sign. In our example, the runs (visually separated by white space) are\n\\[\n+\\ -\\ +\\ -----\\ ++\\ --\\ +\\ ---\\ ++++\n\\]\nHance \\(R = 9\\).\nThe essence of the Wald-Wolfowitz runs test is an approximation of the distribution of \\(R\\) under \\(H_0\\). The null hypothesis is that the data is i.i.d. Under \\(H_0\\), \\(R\\) can be shown to be approximately follow a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), where\n\\[\n\\mu = 2 \\frac{n_+n_-}{n} + 1,\\qquad \\sigma^2 = \\frac{(\\mu - 1)(\\mu - 2)}{n - 1}\n\\]\nHere \\(n_+\\) is the number of positive signs and \\(n_-\\) is the number of negative signs. Note that \\(n_+\\) and \\(n_-\\) are also random variables. Clearly, \\(n_+ + n_- = n\\), the total number of observations. With such values at hand the test creates the \\(p\\)-value via\n\\[\n2P(Z &gt; \\left|\\frac{R - \\mu}{\\sigma}\\right|)\n\\]\nwhere \\(Z\\) is a standard normal variable.\nIn the following code, we’ll validate that the random variable \\(R\\) (i.e. runs) is approximately distributed as a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma\\) defined above under \\(H_0\\) (i.e. the data points are i.i.d.). In other words, the distribution of the \\(p\\)-value is \\(U(0, 1)\\).\n\nusing Random, StatsBase, Distributions, CairoMakie\n\nRandom.seed!(0)\n\n# we'll use a Monte Carlo simulation to obtain the empirical distribution of the p-value\n# sample size: n\n# experimental numbers: N\nn, N = 10^3, 10^6\n\n# calculate the p-value for each sample\nfunction waldWolfowitz(data)\n    n = length(data)\n    sgns = data .&gt; mean(data)\n    nPlus, nMinus = sum(sgns), n - sum(sgns)\n    wwMu = 2 * nPlus * nMinus / n + 1\n    wwVar = (wwMu - 1) * (wwMu - 2) / (n - 1)\n\n    R = 1\n    for i in 1:(n-1)\n        R += sgns[i] != sgns[i+1]\n    end\n\n    zStat = abs((R - wwMu) / sqrt(wwVar))\n    2 * ccdf(Normal(), zStat)\nend\n\n# repeat the process for N times\npVals = [waldWolfowitz(rand(Normal(), n)) for _ in 1:N]\n# here, we calculate the hypothesis testing power\n# under H0, the power is equal to α\nfor alpha in [0.001, 0.005, 0.01, 0.05, 0.1]\n    pva = sum(pVals .&lt; alpha) / N\n    println(\"For alpha = $(alpha), p-value area = $(pva)\")\nend\n\nfig = Figure(size=(500, 800))\n# we can see that the distributio of the p-value is U(0, 1)\n# the reason why spikes of high density appear is that\n# we are approximating a discrete random variable R (can only have positive integers) with a normal random variable\nax1 = Axis(fig[1, 1]; xlabel=\"p-value\", ylabel=\"Frequency\")\nhist!(ax1, pVals; bins=5 * n)\n\npGrid = 0:0.001:1\n# get the ECDF using the ecdf function from StatsBase package\nFhat = ecdf(pVals)\n\n# the ECDF indicates that the distribution is almost uniform\nax2 = Axis(fig[2, 1]; xlabel=\"p-value\", ylabel=\"ECDF\")\nlines!(ax2, pGrid, Fhat.(pGrid))\nfig\n\nFor alpha = 0.001, p-value area = 0.000925\nFor alpha = 0.005, p-value area = 0.004769\nFor alpha = 0.01, p-value area = 0.010113\nFor alpha = 0.05, p-value area = 0.050592\nFor alpha = 0.1, p-value area = 0.100888\n\n\n\n\n\n\n\n\n10.5 More on power\n\n10.5.1 Parameters affecting power\nEstimate the hypothesis testing powers under different scenarios:\n\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\nmu0, mu1A, mu1B = 20, 22, 24\nsig, n = 7, 5\nN = 10^6\nalpha = 0.05\n\n# the underlying mean equals the mean under the null hypothesis\n# the power is α\ndataH0 = [tSat(mu0, mu0, sig, n) for _ in 1:N]\n# the underlying mean is increased\ndataH1A = [tSat(mu0, mu1A, sig, n) for _ in 1:N]\n# the underlying mean is increased further\ndataH1B = [tSat(mu0, mu1B, sig, n) for _ in 1:N]\n# increase the sample size\ndataH1C = [tSat(mu0, mu1B, sig, 2 * n) for _ in 1:N]\n# the underlying variance is decreased\ndataH1D = [tSat(mu0, mu1B, sig / 2, 2 * n) for _ in 1:N]\n\n# calculate the quantile of alpha\ntCrit = quantile(TDist(n - 1), 1 - alpha)\n# estimate the power\nestPwr(sample) = sum(sample .&gt; tCrit) / N\n\nprintln(\"Rejection boundary: \", tCrit)\nprintln(\"Power under H0 (equal α): \", estPwr(dataH0))\nprintln(\"Power under H1A (increase μ): \", estPwr(dataH1A))\nprintln(\"Power under H1B (increase μ further): \", estPwr(dataH1B))\nprintln(\"Power under H1C (increase sample size n to 2n): \", estPwr(dataH1C))\nprintln(\"Power under H1D (decrease σ to σ/2): \", estPwr(dataH1D))\n\nkH0 = kde(dataH0)\nkH1A = kde(dataH1A)\nkH1B = kde(dataH1B)\nkH1C = kde(dataH1C)\nkH1D = kde(dataH1D)\n\nxGrid = -10:0.1:15\n\nfig, ax = lines(xGrid, pdf(kH0, xGrid); color=:blue, label=\"Distribution under H0\")\nlines!(ax, xGrid, pdf(kH1A, xGrid); color=:red, label=\"Distribution under H1A\")\nlines!(ax, xGrid, pdf(kH1B, xGrid); color=:green, label=\"Distribution under H1B\")\nlines!(ax, xGrid, pdf(kH1C, xGrid); color=:orange, label=\"Distribution under H1C\")\nlines!(ax, xGrid, pdf(kH1D, xGrid); color=:purple, label=\"Distribution under H1D\")\nvlines!(ax, [tCrit]; color=:black, label=\"Critical value boundary\")\nax.xlabel = L\"\\Delta = \\mu - \\mu_0\"\nax.ylabel = \"Density\"\naxislegend(ax)\nfig\n\nRejection boundary: 2.1318467863266495\nPower under H0 (equal α): 0.050195\nPower under H1A (increase μ): 0.13399\nPower under H1B (increase μ further): 0.281885\nPower under H1C (increase sample size n to 2n): 0.407785\nPower under H1D (decrease σ to σ/2): 0.91574\n\n\n\n\n\nUnder a normal population, \\(\\mu\\), \\(\\sigma\\), \\(\\alpha\\), and sample size all have an effect on the power. But in practice, if keeping \\(\\alpha\\) constant, it is only the sample size can be controlled.\nAs a consequence, we need to know what is the sample size for our expected power.\n\n\n10.5.2 Power curves\nAs mentioned before, if keeeping \\(\\alpha\\) constant, it is only the sample size can be controlled. Therefore, underlying a given \\(\\alpha\\), we must know\n\nTo obtain a given power, how many observations (i.e. sample size \\(n\\)) do we need?\nFor a few of available sample sizes, what’s the largest power we can obtain?\n\nA power curve is a plot of the power as a function of certain parameters we are interested in.\nFor example, under the hypothesis test setup:\n\\[\nH_0: \\mu = \\mu_0\\ \\ \\ \\ \\text{and}\\ \\ \\ \\ H_1: \\mu &gt; \\mu_0\n\\]\nwe want to estimate the power of a one-sided T-test for different scenarios combining the mean \\(\\mu\\) and the sample size \\(n\\) under normality assumption.\n\nusing Random, Distributions, CairoMakie\n\n# calculate T-statistic by sampling n observations from a given normal distribution\nfunction tSat(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    (xBar - mu0) / (s / sqrt(n))\nend\n\n# estimate the statistical power under a given scenario (μ, n)\nfunction powerEstimate(mu0, mu1, sig, n, alpha, N)\n    Random.seed!(0)\n    sampleH1 = [tSat(mu0, mu1, sig, n) for _ in 1:N]\n    critVal = quantile(TDist(n - 1), 1 - alpha)\n    sum(sampleH1 .&gt; critVal) / N\nend\n\nmu0 = 20\nsig = 5\nalpha = 0.05\nN = 10^4\nrangeMu1 = 16:0.1:30\nnList = [5, 10, 20, 30]\n\npowerCurves = [powerEstimate.(mu0, rangeMu1, sig, n, alpha, N) for n in nList]\n\nfig, ax = lines(rangeMu1, powerCurves[1]; color=:blue, label=\"n = $(nList[1])\")\nlines!(ax, rangeMu1, powerCurves[2]; color=:red, label=\"n = $(nList[2])\")\nlines!(ax, rangeMu1, powerCurves[3]; color=:green, label=\"n = $(nList[3])\")\nlines!(ax, rangeMu1, powerCurves[4]; color=:purple, label=\"n = $(nList[4])\")\nax.xlabel = L\"\\mu\"\nax.ylabel = \"Power\"\naxislegend(ax; position=:lt)\nfig\n\n\n\n\nAs seen in the above figure, under a given power, if \\(\\mu\\) is away from \\(\\mu_0\\) further, then we can take less observations. On the other hand, for a given \\(\\mu\\), if we hope to increase the power, we need to increase the sample size.\nAnother point to note is that the x-axis could be adjusted to represent the difference \\(\\Delta = \\mu - \\mu_0\\). Furthermore, one could make the axis scale invariant by dividing \\(\\Delta\\) by the standard deviation.\n\n\n10.5.3 Distribution of the \\(p\\)-value\nFor a uniform \\(U(0, 1)\\) random variable, we have its CDF \\(F(x) = x, x \\in [0, 1]\\), and CCDF \\(F_c(x) = 1 - x, x \\in [0, 1]\\).\n\\(p\\)-value is defined as \\(p = P(S &gt; u)\\), where \\(S\\) is a random variable representing the test statistic, \\(u\\) is the observed test statistic, and \\(p\\) is the \\(p\\)-value of the observed test statistic.\nTo discuss the distribution of the \\(p\\)-value, denote the random variable of the \\(p\\)-value by \\(P\\). Hence \\(P = 1 - F(S)\\), where \\(F(\\cdot)\\) is the CDF of the test statistic under \\(H_0\\). Note that \\(P\\) is just a transformation of the test statistic random variable \\(S\\). Assume that \\(S\\) is continuous and assume that \\(H_0\\) holds, hence \\(P(S &lt; u) = F(u)\\). we now have\n\\[\n\\begin{align}\nP(P &gt; x) &= P(1 - F(S) &gt; x) \\\\\n&= P(F(S) &lt; 1 - x) \\\\\n&= P(S &lt; F^{-1}(1-x)) \\\\\n&= F(F^{-1}(1-x)) \\\\\n&= 1 - x\n\\end{align}\n\\]\nObviously, the CDF of the random variable \\(P\\) is \\(F(x) = P(P &lt; x) = x, x \\in [0, 1]\\), so the distribution of the \\(p\\)-value is \\(U(0, 1)\\) under \\(H_0\\).\nIf \\(H_0\\) does not hold, then \\(P(S &lt; u) \\ne F(u)\\) and the derivation above fails. In such a case, the distribution of the \\(p\\)-value is no longer uniform. In fact, in such a case, if the setting is such that the power of the test statistic increases, then we expect the distribution of the \\(p\\)-value to be more concentrated around \\(0\\) than a uniform distribution.\n\nusing Random, Distributions, KernelDensity, CairoMakie\n\nRandom.seed!(1)\n\nfunction pval(mu0, mu, sig, n)\n    sample = rand(Normal(mu, sig), n)\n    xBar = mean(sample)\n    s = std(sample)\n    tStatistics = (xBar - mu0) / (s / sqrt(n))\n    # under H0, t ∼ T(n - 1)\n    ccdf(TDist(n - 1), tStatistics)\nend\n\nmu0, mu1A, mu1B = 20, 23, 26\nsig, n, N = 7, 5, 10^6\n\npValsH0 = [pval(mu0, mu0, sig, n) for _ in 1:N]\npValsH1A = [pval(mu0, mu1A, sig, n) for _ in 1:N]\npValsH1B = [pval(mu0, mu1B, sig, n) for _ in 1:N]\n\nalpha = 0.05\nestPwr(pVals) = sum(pVals .&lt; alpha) / N\n\nprintln(\"Power under H0: \", estPwr(pValsH0))\nprintln(\"Power under H1A: \", estPwr(pValsH1A))\nprintln(\"Power under H1B: \", estPwr(pValsH1B))\n\nfig, ax = stephist(pValsH0; bins=100, normalization=:pdf, color=:blue, label=\"Under H0\")\nstephist!(ax, pValsH1A; bins=100, normalization=:pdf, color=:red, label=\"Under H1A\")\nstephist!(ax, pValsH1B; bins=100, normalization=:pdf, color=:green, label=\"Under H1B\")\nvlines!(ax, [alpha]; color=:black, label=\"α\", linestyle=:dash)\naxislegend(ax)\nax.xlabel = \"p-value\"\nax.ylabel = \"Density\"\nfig\n\nPower under H0: 0.050195\nPower under H1A: 0.199417\nPower under H1B: 0.477365"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#appendices",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/probability_and_statistics_with_julia/index.html#appendices",
    "title": "Probability and statistics with Julia",
    "section": "11 Appendices",
    "text": "11 Appendices\n\n11.1 Base conversions\n\nBase \\(10\\) to base \\(k\\):\n\n\n整数部分：除 \\(k\\) 取余，逆序写出，直到商为 \\(0\\)。\n小数部分：乘 \\(k\\) 取整，顺序写出，直到小数部分为 \\(0\\) 或达到指定的精度为止。\n\nNote: \\(\\frac{1}{2} \\frac{1}{1} \\frac{0}{0} \\frac{.}{} \\frac{1}{-1} \\frac{0}{-2} \\frac{1}{-3}\\).\n\n\n11.2 Misc\n\nThe largest integer which can be represented by Int32 is \\(2^{31} - 1\\).\n\n对于 Int32 来说，最高位为符号位，因此最大的二进制数为 \\(\\underbrace{1...1}_{\\text{31 1's}}\\)，即所能表示的最大整数为 \\(1\\cdot 2^0 + 1\\cdot 2^1 + ... + 1\\cdot 2^{30}\\)，为了表示方便，我们将其加 \\(1\\)，变为 \\(1\\underbrace{0...0}_{\\text{31 0's}}\\)，再减 \\(1\\)，即 \\(2^{31} - 1\\)。"
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "",
    "text": "In an experiment, there are only two outcomes: success \\(A\\) (\\(X = 1\\)) and failure \\(\\overline{A}\\) (\\(X = 0\\)), and the probability of success is \\(p\\). Such an experiment is called a Bernoulli experiment.\nIn a Bernoulli experiment, the number of success, denoted by \\(X\\), is a random variable and its distribution is called a Bernoulli distribution or a two-point distribution.\nClearly, the PMF of a Bernoulli distribution is\n\\[\np(x) = P(X = x) = \\begin{cases}\np &\\text{if } x = 1 \\\\\n1 - p &\\text{if } x = 0\n\\end{cases}\n\\]\nThen \\(E(X) = \\sum_{i=1}^{\\infty} x_i p(x_i) = 1 \\times p + 0 \\times (1 - p) = p\\), and \\(Var(X) = E[(X - E(X))^2] = \\sum_{i=1}^{\\infty} (x_i - E(X))^2 p(x_i) = (1 - p)^2 \\times p + (0 - p)^2 \\times (1 - p) = p(1 - p)\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#bernoulli-distribution",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#bernoulli-distribution",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "",
    "text": "In an experiment, there are only two outcomes: success \\(A\\) (\\(X = 1\\)) and failure \\(\\overline{A}\\) (\\(X = 0\\)), and the probability of success is \\(p\\). Such an experiment is called a Bernoulli experiment.\nIn a Bernoulli experiment, the number of success, denoted by \\(X\\), is a random variable and its distribution is called a Bernoulli distribution or a two-point distribution.\nClearly, the PMF of a Bernoulli distribution is\n\\[\np(x) = P(X = x) = \\begin{cases}\np &\\text{if } x = 1 \\\\\n1 - p &\\text{if } x = 0\n\\end{cases}\n\\]\nThen \\(E(X) = \\sum_{i=1}^{\\infty} x_i p(x_i) = 1 \\times p + 0 \\times (1 - p) = p\\), and \\(Var(X) = E[(X - E(X))^2] = \\sum_{i=1}^{\\infty} (x_i - E(X))^2 p(x_i) = (1 - p)^2 \\times p + (0 - p)^2 \\times (1 - p) = p(1 - p)\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#binomial-distribution",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#binomial-distribution",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "2 Binomial distribution",
    "text": "2 Binomial distribution\nIn \\(n\\) independent Bernoulli trials, each with the probability of success \\(p\\), the number of success \\(Y\\) is distributed as a binomial distribution.\nThe PMF of a binomial distribution is\n\\[\np(k) = P(Y = k) = \\binom{n}{k} p^k (1-p)^{n - k}, k = 0, 1, ..., n\n\\]\nClearly, a binomial distribution can be regarded as the sum of \\(n\\) independent Bernoulli distributions, i.e., \\(Y = X_1 + \\cdots + X_n\\). Then, by using the arithmetic properties of expectation and variance, we have \\(E(Y) = np\\) and \\(Var(Y) = np(1 - p)\\).\nDue to \\(Y\\) is the sum of \\(n\\) independent Bernoulli random variables, we have \\(Y\\ \\ \\widetilde{\\text{approx}}\\ \\ N(np, np(1 - p))\\) as \\(n \\to \\infty\\) based on the central limit theorem. We also have \\(\\frac{Y}{n}\\ \\ \\widetilde{\\text{approx}}\\ \\ N(p, \\frac{p(1 - p)}{n})\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#introduction",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#introduction",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "3 Introduction",
    "text": "3 Introduction\nAssume that we repeat a Bernoulli trial \\(n\\) times, each of which has the same success probability \\(p\\), and that the number of successes is \\(n_s\\). Then, the random variable (denoted by \\(X_i\\), with value \\(1\\) (success) or \\(0\\) (failure)) in the \\(i\\)th Bernoulli trial is distributed as a Bernoulli distribution with the probability of success \\(p\\). According to the central limit theorem, the random variable \\(Y_n = \\sum_{i=1}^{n} X_i\\) distributed as a Binomial distribution with parameters \\(n\\) and \\(p\\) can be approximately distributed as a normal distribution (i.e. \\(Y_n \\sim N(np, np(1-p))\\)). And similarly, we have \\(\\hat{p} = \\frac{Y_n}{n} \\sim N(p, \\frac{p(1-p)}{n})\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#point-estimates-of-p",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#point-estimates-of-p",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "4 Point estimates of \\(p\\)",
    "text": "4 Point estimates of \\(p\\)\n\n4.1 Method of Moments (MM)\n\\[\n\\begin{align}\n\\hat{m} &= \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\frac{k}{n} \\\\\nm &= p \\\\\n\\hat{m} &= m\n\\end{align}\n\\]\nHence\n\\[\np = \\frac{k}{n}\n\\]\n\n\n4.2 Maximum likelihood estimate (MLE)\nFor given \\(n\\) and \\(k\\), we need to find a \\(p\\) to maximize the likelihood function (to make the likelihood of observing \\(k\\) successes among \\(n\\) experiments in total maximized):\n\\[\n\\begin{align}\nL(p) &= \\binom{n}{k} p^k (1 - p)^{n - k} \\\\\n\\log{L(p)} &= \\log{\\binom{n}{k} + k\\log{p} + (n - k)\\log{(1 - p)}} \\\\\n\\frac{d\\log{L(p)}}{dp} &= \\frac{k}{p} - \\frac{n - k}{1 - p} = 0 \\\\\np &= \\frac{k}{n}\n\\end{align}\n\\]\nThere is a point \\(p = \\frac{k}{n}\\) that maximizes \\(L(p)\\)."
  },
  {
    "objectID": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#confidence-intervals-of-p",
    "href": "Blogs/Mathematics/posts/Probability and Statistics/statistical_inference_concepts_of_binomial_distribution/index.html#confidence-intervals-of-p",
    "title": "Statistical inference concepts of binomial distribution",
    "section": "5 Confidence intervals of \\(p\\)",
    "text": "5 Confidence intervals of \\(p\\)\n\n  It appears you don't have a PDF plugin for this browser. No biggie. You can click here to download the PDF file.\n\nWe can use the standard logistic curve to check the overshoot and zero-width of the normal confidence interval:\n\nusing CairoMakie, Statistics, Distributions\n\n# the standard logistic curve can be regareded as a CDF\n# for each p-value, given specific n, we can calculate its normal confidence interval\n# we can easily see that there is a overshoot or zero-width situation\n# when p → 0 or 1\nfunction standard_logistic(x)\n    1 / (1 + exp(-x))\nend\n\nxGrid = -10:0.1:10\npVals = standard_logistic.(xGrid)\n\nalpha = 0.05\nz = quantile(Normal(), 1 - alpha / 2)\n\nns = [10, 100]\ncolors = [:blue, :green]\n\nfig, ax = lines(xGrid, pVals; color=:red, label=\"Standard logistic (true p-value)\")\nfor i in 1:length(ns)\n    pValLowerCIs = similar(pVals)\n    pValUpperCIs = similar(pVals)\n\n    @. pValLowerCIs = pVals - z * sqrt(pVals * (1 - pVals) / ns[i])\n    @. pValUpperCIs = pVals + z * sqrt(pVals * (1 - pVals) / ns[i])\n\n    lines!(ax, xGrid, pValLowerCIs; color=colors[i], linestyle=:dash, label=string(\"CI (n: \", ns[i], \")\"))\n    lines!(ax, xGrid, pValUpperCIs; color=colors[i], linestyle=:dash)\nend\naxislegend(ax; position=:rb)\nfig"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "NeuroBorder",
    "section": "",
    "text": "If you have any question or suggestion, please let me know.\nEmail: 2413667864@qq.com"
  }
]